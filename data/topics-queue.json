{
  "generatedAt": "2026-02-15T15:31:27.590Z",
  "items": [
    {
      "id": "https://hallucinatingsplines.com",
      "title": "Show HN: AI agents play SimCity through a REST API",
      "link": "https://hallucinatingsplines.com",
      "summary": "This is a weekend project that spiraled out of control. I was originally trying to get Claude to play a ROM of the SNES SimCity. I struggled with it and that led me to Micropolis (the open-sourced SimCity engine) and was able to get it to work by bolting on an API. The weekend hack turned into a headless city simulation platform where anyone can get an API key (no signup) and have their AI agent play mayor. The simu…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-09T15:54:33.000Z",
      "score": 186,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-15T15:31:27.519Z",
      "file": "content/posts/2026-02-15-control-a-micropolis-city-with-an-llm-via-hallucinating-splines-rest-api.md",
      "fileFr": "content/posts/fr/2026-02-15-control-a-micropolis-city-with-an-llm-via-hallucinating-splines-rest-api.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 90,
      "wordsEn": 1655,
      "wordsFr": 1421
    },
    {
      "id": "https://www.numerama.com/tech/2177647-fregates-et-sous-marins-ce-que-prepare-naval-group-avec-thales-pour-le-combat-naval-du-futur.html",
      "title": "Frégates et sous-marins : ce que prépare Naval Group avec Thales pour le combat naval du futur",
      "link": "https://www.numerama.com/tech/2177647-fregates-et-sous-marins-ce-que-prepare-naval-group-avec-thales-pour-le-combat-naval-du-futur.html",
      "summary": "Naval Group et Thales s’allient pour développer une intelligence artificielle souveraine qui embarquera dans les navires de premier rang et les sous-marins. Objectif : traiter le déluge de données en mer et accélérer la prise de décision, tout en garantissant que l'humain reste le seul maître du tir.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T13:52:24.000Z",
      "score": 79.65,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-15T08:25:26.754Z",
      "file": "content/posts/2026-02-15-naval-group-takes-20percent-stake-in-thales-cortaix-france-to-co-develop-sovereign-onboard-ai-for-warships-and-submarines.md",
      "fileFr": "content/posts/fr/2026-02-15-naval-group-takes-20percent-stake-in-thales-cortaix-france-to-co-develop-sovereign-onboard-ai-for-warships-and-submarines.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1472,
      "wordsFr": 1189
    },
    {
      "id": "https://margindash.com/",
      "title": "Show HN: MarginDash – See which AI customers are profitable",
      "link": "https://margindash.com/",
      "summary": "I built MarginDash because I couldn't answer a basic question: which of my customers actually make me money after AI costs? My Stripe dashboard showed revenue going up. My OpenAI bills showed costs going up faster. I had no way to connect the two at the customer level. MarginDash is a few line SDK integration (TypeScript, Python, or REST) that tracks model usage per customer and connects it to revenue — either throu…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-13T08:21:46.000Z",
      "score": 222,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-14T15:28:19.135Z",
      "file": "content/posts/2026-02-14-integrate-margindash-for-per-customer-ai-pandl-with-sdk-stripe-sync-and-a-cost-simulator.md",
      "fileFr": "content/posts/fr/2026-02-14-integrate-margindash-for-per-customer-ai-pandl-with-sdk-stripe-sync-and-a-cost-simulator.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1599,
      "wordsFr": 1463
    },
    {
      "id": "https://www.numerama.com/tech/2177979-pourquoi-mistral-ai-investit-12-milliard-deuros-dans-un-data-center-en-suede.html",
      "title": "Pourquoi Mistral AI investit 1,2 milliard d’euros dans un data center en Suède ?",
      "link": "https://www.numerama.com/tech/2177979-pourquoi-mistral-ai-investit-12-milliard-deuros-dans-un-data-center-en-suede.html",
      "summary": "Mistral AI s'allie à l'entreprise suédoise EcoDataCenter pour mettre au point un centre de données en Suède au nom de la souveraineté européenne en matière d'intelligence artificielle. Un projet qui souffre cependant d'une limite : la provenance des GPU.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-12T07:32:10.000Z",
      "score": 145.26,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-14T08:23:18.384Z",
      "file": "content/posts/2026-02-14-mistral-ai-invests-euro12b-with-ecodatacenter-in-sweden-nvidia-vera-rubin-gpus-limit-a-fully-european-hardware-stack.md",
      "fileFr": "content/posts/fr/2026-02-14-mistral-ai-invests-euro12b-with-ecodatacenter-in-sweden-nvidia-vera-rubin-gpus-limit-a-fully-european-hardware-stack.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1514,
      "wordsFr": 1517
    },
    {
      "id": "https://github.com/fctr-id/okta-ai-agent",
      "title": "Show HN: Tako AI – Agent for Okta With Natural language (zero hallucination)",
      "link": "https://github.com/fctr-id/okta-ai-agent",
      "summary": "Hi HN, Every week I watched Okta admins burn hours answering ad-hoc questions from security teams: \"Who has access to Salesforce?\", \"Find all contractors with GitHub access who haven't used MFA in 30 days.\" The answers always involved the same painful loop: dig through a slow web console, chain API calls, correlate CSVs, write throwaway Python scripts. Repeat next week. I spent 12 months building Tako AI to fix this…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 10,
      "publishedAt": "2026-02-12T15:38:38.000Z",
      "score": 252,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-13T15:45:38.250Z",
      "file": "content/posts/2026-02-13-build-a-local-first-okta-agent-that-executes-deterministic-api-calls-and-enforces-zero-hallucinations.md",
      "fileFr": "content/posts/fr/2026-02-13-build-a-local-first-okta-agent-that-executes-deterministic-api-calls-and-enforces-zero-hallucinations.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "advanced",
      "timeToImplementMinutes": 240,
      "wordsEn": 1524,
      "wordsFr": 1310
    },
    {
      "id": "https://www.theverge.com/ai-artificial-intelligence/877931/bytedance-seedance-2-video-generator-ai-launch",
      "title": "ByteDance’s next-gen AI model can generate clips based on text, images, audio, and video",
      "link": "https://www.theverge.com/ai-artificial-intelligence/877931/bytedance-seedance-2-video-generator-ai-launch",
      "summary": "ByteDance says its new AI video model can more accurately follow prompts. | Image: ByteDance Big Tech's race to leapfrog the latest AI models continues with the launch of ByteDance's next-gen video generator. In a blog post, ByteDance - the China-based company behind TikTok - says Seedance 2.0 supports prompts that combine text, images, video, and audio. The company claims it \"delivers a substantial leap in generati…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-12T15:26:00.000Z",
      "score": 180,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-13T08:35:22.056Z",
      "file": "content/posts/2026-02-13-seedance-20-bytedances-multimodal-model-for-generating-up-to-15second-videos-from-text-images-audio-and-video.md",
      "fileFr": "content/posts/fr/2026-02-13-seedance-20-bytedances-multimodal-model-for-generating-up-to-15second-videos-from-text-images-audio-and-video.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "advanced",
      "timeToImplementMinutes": 180,
      "wordsEn": 1470,
      "wordsFr": 1283
    },
    {
      "id": "https://www.numerama.com/tech/2176523-seedance-le-nouveau-modele-chinois-pour-generer-des-videos-defie-openai-et-google.html",
      "title": "Seedance, le nouveau modèle chinois pour générer des vidéos, défie OpenAI et Google",
      "link": "https://www.numerama.com/tech/2176523-seedance-le-nouveau-modele-chinois-pour-generer-des-videos-defie-openai-et-google.html",
      "summary": "Début février 2026, le groupe chinois ByteDance a dévoilé Seedance 2.0, son nouveau modèle de génération vidéo par IA. Capable de produire image, son, voix et musique dans un même pipeline, l’outil impressionne autant par ses performances techniques que par sa stratégie de déploiement grand public.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T10:44:38.000Z",
      "score": 58.38,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-12T15:58:13.387Z",
      "file": "content/posts/2026-02-12-prototype-guide-integrating-bytedance-seedance-20-with-capcutdreamina-and-developer-apis.md",
      "fileFr": "content/posts/fr/2026-02-12-prototype-guide-integrating-bytedance-seedance-20-with-capcutdreamina-and-developer-apis.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1761,
      "wordsFr": 1399
    },
    {
      "id": "https://www.bbc.com/news/articles/cqxdj77welpo?at_medium=RSS&at_campaign=rss",
      "title": "EU tells Meta to let rivals run AI chatbots on WhatsApp",
      "link": "https://www.bbc.com/news/articles/cqxdj77welpo?at_medium=RSS&at_campaign=rss",
      "summary": "A Meta spokesperson said the EU had \"no reason\" to intervene over it changing the app in January.",
      "source": "BBC Technology",
      "region": "UK",
      "keywordHits": 2,
      "publishedAt": "2026-02-09T12:14:35.000Z",
      "score": 68.47,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-12T08:38:00.932Z",
      "file": "content/posts/2026-02-12-eu-says-meta-likely-blocked-rival-ai-chatbots-on-whatsapp-after-15-january-change.md",
      "fileFr": "content/posts/fr/2026-02-12-eu-says-meta-likely-blocked-rival-ai-chatbots-on-whatsapp-after-15-january-change.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1416,
      "wordsFr": 1554
    },
    {
      "id": "https://www.theverge.com/transportation/876540/uber-eats-ai-chatbot-cart-assistant-grocery-shopping",
      "title": "Uber Eats adds AI assistant to help with grocery shopping",
      "link": "https://www.theverge.com/transportation/876540/uber-eats-ai-chatbot-cart-assistant-grocery-shopping",
      "summary": "Uber announced a new AI feature called \"Cart Assistant\" for grocery shopping in its Uber Eats app. The new feature works a couple different ways. You can use text prompts, as you would with any other AI chatbot, to ask it to build a grocery list for you. Or you can upload a picture of your shopping list and ask it to populate your cart with all your favorite items, based on your order history. You can be as generic…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T12:00:00.000Z",
      "score": 71.79,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-11T16:07:33.804Z",
      "file": "content/posts/2026-02-11-prototype-guide-cart-assistant-that-prefills-uber-eats-grocery-carts-from-text-or-photo-shopping-lists.md",
      "fileFr": "content/posts/fr/2026-02-11-prototype-guide-cart-assistant-that-prefills-uber-eats-grocery-carts-from-text-or-photo-shopping-lists.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 480,
      "wordsEn": 1678,
      "wordsFr": 1572
    },
    {
      "id": "https://www.numerama.com/tech/2176519-comfyui-comment-generer-facilement-des-images-ou-des-videos-avec-une-carte-graphique-nvidia-rtx.html",
      "title": "ComfyUI : comment générer facilement des images ou des vidéos avec une carte graphique Nvidia RTX [Sponso]",
      "link": "https://www.numerama.com/tech/2176519-comfyui-comment-generer-facilement-des-images-ou-des-videos-avec-une-carte-graphique-nvidia-rtx.html",
      "summary": "Cet article a été réalisé en collaboration avec Nvidia Un PC solide, un GPU Nvidia de dernière génération, une solide connexion internet, ComfyUI et un peu de temps : voilà les ingrédients nécessaires à la mise en place d’un agent IA personnalisé pour générer des images ou des vidéos.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T06:47:00.000Z",
      "score": 89.42,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-11T08:41:38.417Z",
      "file": "content/posts/2026-02-11-set-up-comfyui-on-an-nvidia-rtx-pc-for-local-image-and-short-video-generation.md",
      "fileFr": "content/posts/fr/2026-02-11-set-up-comfyui-on-an-nvidia-rtx-pc-for-local-image-and-short-video-generation.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 180,
      "wordsEn": 1667,
      "wordsFr": 1002
    },
    {
      "id": "https://github.com/asterai-io/asterbot",
      "title": "Show HN: Asterbot, AI agent where every capability is a sandboxed WASM component",
      "link": "https://github.com/asterai-io/asterbot",
      "summary": "Asterbot is a modular AI agent where every capability, such as web search, memory, LLM provider, is a swappable WASM component, sandboxed via WASI. Components only have access to what you explicitly grant (e.g. a single directory). They're written in any language (Rust, Go, Python, JS) and pulled from the asterai registry. Under the hood, asterai is a WASM component model registry and runtime built on wasmtime. You…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-10T15:51:38.000Z",
      "score": 192,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-10T16:11:43.036Z",
      "file": "content/posts/2026-02-10-asterbot-an-ai-agent-built-from-sandboxed-swappable-wasm-components.md",
      "fileFr": "content/posts/fr/2026-02-10-asterbot-an-ai-agent-built-from-sandboxed-swappable-wasm-components.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1486,
      "wordsFr": 1233
    },
    {
      "id": "https://www.theverge.com/podcast/875233/siemens-ceo-roland-busch-ai-automation-digital-twins-nato-tariffs",
      "title": "Siemens CEO Roland Busch’s mission to automate everything",
      "link": "https://www.theverge.com/podcast/875233/siemens-ceo-roland-busch-ai-automation-digital-twins-nato-tariffs",
      "summary": "Today, I’m talking with Roland Busch, who is the CEO of Siemens. Siemens is one of those absolutely giant, extremely important, but fairly opaque companies we love to dig into on Decoder. At a very basic, reductive level, Siemens makes the hardware and software that allow other companies to run and automate their stuff. Everyone has seen the Siemens logo somewhere, whether it’s under the hood of their cars, stamped…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 11,
      "publishedAt": "2026-02-09T15:00:00.000Z",
      "score": 252,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-10T08:44:51.211Z",
      "file": "content/posts/2026-02-10-blueprint-for-a-scoped-factory-to-erp-automation-pilot-inspired-by-siemens-ceo-roland-busch.md",
      "fileFr": "content/posts/fr/2026-02-10-blueprint-for-a-scoped-factory-to-erp-automation-pilot-inspired-by-siemens-ceo-roland-busch.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1523,
      "wordsFr": 1332
    },
    {
      "id": "https://techcrunch.com/2026/01/22/are-ai-agents-ready-for-the-workplace-a-new-benchmark-raises-doubts/",
      "title": "Are AI agents ready for the workplace? A new benchmark raises doubts",
      "link": "https://techcrunch.com/2026/01/22/are-ai-agents-ready-for-the-workplace-a-new-benchmark-raises-doubts/",
      "summary": "Article URL: https://techcrunch.com/2026/01/22/are-ai-agents-ready-for-the-workplace-a-new-benchmark-raises-doubts/ Comments URL: https://news.ycombinator.com/item?id=46926131 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-07T18:18:17.000Z",
      "score": 192,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-09T16:02:00.433Z",
      "file": "content/posts/2026-02-09-build-an-apex-agents-style-harness-to-evaluate-ai-agents-multi-domain-performance.md",
      "fileFr": "content/posts/fr/2026-02-09-build-an-apex-agents-style-harness-to-evaluate-ai-agents-multi-domain-performance.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1460,
      "wordsFr": 1735
    },
    {
      "id": "https://www.theverge.com/ai-artificial-intelligence/875615/openai-super-bowl-ai-hardware-leak-hoax-fake",
      "title": "OpenAI’s supposedly ‘leaked’ Super Bowl ad with ear buds and a shiny orb was a hoax",
      "link": "https://www.theverge.com/ai-artificial-intelligence/875615/openai-super-bowl-ai-hardware-leak-hoax-fake",
      "summary": "As if OpenAI didn't have enough drama around the Super Bowl and advertising, as the game wound down, word spread of a \"leaked\" ad that actually wasn't leaked at all; it was just a fake. Screenshots of a now-deleted Reddit thread told the tale of a frustrated employee who, while posting about how upset they were because the ad they'd worked on didn't run, accidentally leaked the entire advertisement video, seemingly…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-02-09T04:54:36.000Z",
      "score": 68.09,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-09T08:43:14.751Z",
      "file": "content/posts/2026-02-09-screenshots-alleging-a-leaked-openai-super-bowl-ad-with-alexander-skarsgard-a-shiny-orb-and-earbuds-were-fabricated.md",
      "fileFr": "content/posts/fr/2026-02-09-screenshots-alleging-a-leaked-openai-super-bowl-ad-with-alexander-skarsgard-a-shiny-orb-and-earbuds-were-fabricated.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "beginner",
      "timeToImplementMinutes": 5,
      "wordsEn": 1474,
      "wordsFr": 1074
    },
    {
      "id": "https://github.com/rendro/sediment",
      "title": "Show HN: Sediment – Local semantic memory for AI agents (Rust, single binary)",
      "link": "https://github.com/rendro/sediment",
      "summary": "I've been increasingly relying on AI coding assistants. I recently had my first child, and my coding hours look different now. I prompt between feedings, sketch out ideas while he naps, and pick up where I left off later. AI lets me stay productive in fragmented time. But every session starts from zero. Claude doesn't remember the product roadmap we outlined last week. It doesn't know the design decisions we already…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 9,
      "publishedAt": "2026-02-08T14:41:07.000Z",
      "score": 246,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-08T15:30:53.355Z",
      "file": "content/posts/2026-02-08-add-persistent-local-semantic-memory-to-llm-agents-with-sediment-rust-single-binary.md",
      "fileFr": "content/posts/fr/2026-02-08-add-persistent-local-semantic-memory-to-llm-agents-with-sediment-rust-single-binary.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1697,
      "wordsFr": 1974
    },
    {
      "id": "https://www.numerama.com/sciences/2170347-horloge-de-lapocalypse-2026-il-ne-reste-que-85-secondes-avant-minuit.html",
      "title": "Horloge de l’Apocalypse 2026 : il ne reste que 85 secondes avant minuit",
      "link": "https://www.numerama.com/sciences/2170347-horloge-de-lapocalypse-2026-il-ne-reste-que-85-secondes-avant-minuit.html",
      "summary": "Réglée à 85 secondes de minuit le 27 janvier 2026, l’Horloge de l’Apocalypse n’a jamais été aussi proche du seuil symbolique de la catastrophe, selon le Bulletin of the Atomic Scientists. L’organisation alerte sur l’escalade des rivalités entre grandes puissances, la fragilisation des accords internationaux et les risques conjugués du nucléaire, du climat et de l’intelligence artificielle.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-01-29T17:33:03.000Z",
      "score": 24.57,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-08T08:25:10.003Z",
      "file": "content/posts/2026-02-08-doomsday-clock-at-85-seconds-2026-practical-implications-for-builders-and-tech-leaders.md",
      "fileFr": "content/posts/fr/2026-02-08-doomsday-clock-at-85-seconds-2026-practical-implications-for-builders-and-tech-leaders.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1475,
      "wordsFr": 1430
    },
    {
      "id": "https://www.theverge.com/transportation/874771/waymo-world-model-simulation-google-deepmind-genie-3",
      "title": "What happens when Waymo runs into a tornado? Or an elephant?",
      "link": "https://www.theverge.com/transportation/874771/waymo-world-model-simulation-google-deepmind-genie-3",
      "summary": "An autonomous vehicle drives down a lonely stretch of highway. Suddenly, a massive tornado appears in the distance. What does the driverless vehicle do next? This is just one of the scenarios that Waymo can simulate in the \"hyper realistic\" virtual world that it has just created with help from Google's DeepMind. Waymo's World Model is built using Genie 3, Google's new AI world model that can generate virtual interac…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 3,
      "publishedAt": "2026-02-06T16:00:00.000Z",
      "score": 40.56,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T20:11:38.689Z",
      "file": "content/posts/2026-02-06-waymo-uses-googles-genie-world-model-to-simulate-tornadoes-and-wildlife-for-edge-case-autonomous-vehicle-testing.md",
      "fileFr": "content/posts/fr/2026-02-06-waymo-uses-googles-genie-world-model-to-simulate-tornadoes-and-wildlife-for-edge-case-autonomous-vehicle-testing.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1489,
      "wordsFr": 1334
    },
    {
      "id": "https://arxiv.org/abs/2602.04210",
      "title": "Steering LLMs via Scalable Interactive Oversight",
      "link": "https://arxiv.org/abs/2602.04210",
      "summary": "arXiv:2602.04210v1 Announce Type: new \nAbstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \\emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficie",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 45.6,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T20:06:29.835Z",
      "file": "content/posts/2026-02-06-scalable-interactive-oversight-building-a-decision-tree-prototype-to-collect-node-level-feedback-and-steer-llms.md",
      "fileFr": "content/posts/fr/2026-02-06-scalable-interactive-oversight-building-a-decision-tree-prototype-to-collect-node-level-feedback-and-steer-llms.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1623,
      "wordsFr": 1444
    },
    {
      "id": "https://arxiv.org/abs/2602.04003",
      "title": "When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making",
      "link": "https://arxiv.org/abs/2602.04003",
      "summary": "arXiv:2602.04003v1 Announce Type: new \nAbstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 45.6,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T20:01:44.770Z",
      "file": "content/posts/2026-02-06-adversarial-explanation-attacks-how-llm-framing-preserves-user-trust-in-incorrect-outputs.md",
      "fileFr": "content/posts/fr/2026-02-06-adversarial-explanation-attacks-how-llm-framing-preserves-user-trust-in-incorrect-outputs.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1428,
      "wordsFr": 1614
    },
    {
      "id": "https://arxiv.org/abs/2602.04101",
      "title": "Interfaze: The Future of AI is built on Task-Specific Small Models",
      "link": "https://arxiv.org/abs/2602.04101",
      "summary": "arXiv:2602.04101v1 Announce Type: new \nAbstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogene",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 55.6,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T19:58:27.829Z",
      "file": "content/posts/2026-02-06-prototyping-interfaze-building-a-multimodal-perception-context-construction-and-action-stack-for-task-specific-small-models.md",
      "fileFr": "content/posts/fr/2026-02-06-prototyping-interfaze-building-a-multimodal-perception-context-construction-and-action-stack-for-task-specific-small-models.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1499,
      "wordsFr": 1442
    },
    {
      "id": "https://arxiv.org/abs/2602.04089",
      "title": "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL",
      "link": "https://arxiv.org/abs/2602.04089",
      "summary": "arXiv:2602.04089v1 Announce Type: new \nAbstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inheren",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 55.6,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:54:50.764Z",
      "file": "content/posts/2026-02-06-orbit-crossepisode-metarl-for-incontext-online-adaptation-of-llms.md",
      "fileFr": "content/posts/fr/2026-02-06-orbit-crossepisode-metarl-for-incontext-online-adaptation-of-llms.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "advanced",
      "timeToImplementMinutes": 5,
      "wordsEn": 1305,
      "wordsFr": 1325
    },
    {
      "id": "https://arxiv.org/abs/2602.03975",
      "title": "Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure",
      "link": "https://arxiv.org/abs/2602.03975",
      "summary": "arXiv:2602.03975v1 Announce Type: new \nAbstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 55.6,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:50:57.091Z",
      "file": "content/posts/2026-02-06-state-level-selective-verification-with-learned-heuristics-for-verification-cost-limited-llm-reasoning.md",
      "fileFr": "content/posts/fr/2026-02-06-state-level-selective-verification-with-learned-heuristics-for-verification-cost-limited-llm-reasoning.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1302,
      "wordsFr": 1138
    },
    {
      "id": "https://arxiv.org/abs/2602.03900",
      "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks",
      "link": "https://arxiv.org/abs/2602.03900",
      "summary": "arXiv:2602.03900v1 Announce Type: new \nAbstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have co",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 55.6,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:47:32.595Z",
      "file": "content/posts/2026-02-06-task-method-knowledge-prompting-improves-llm-planning-on-planbench-blocksworld.md",
      "fileFr": "content/posts/fr/2026-02-06-task-method-knowledge-prompting-improves-llm-planning-on-planbench-blocksworld.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1259,
      "wordsFr": 1264
    },
    {
      "id": "https://www.theverge.com/entertainment/874504/super-bowl-lx-ads-big-game",
      "title": "Super Bowl LX ads: all AI everything",
      "link": "https://www.theverge.com/entertainment/874504/super-bowl-lx-ads-big-game",
      "summary": "Super Bowl LX is nearly here, with the Seattle Seahawks taking on the New England Patriots. While Bad Bunny will be the star of the halftime show, AI could be the star of the commercial breaks, much like crypto was a few years ago. Last year’s Super Bowl featured a Google Gemini ad that fumbled a Gouda cheese stat, and this year’s game is already slated to include an ad for Anthropic’s AI platform that takes jabs at…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-05T18:18:34.000Z",
      "score": 56.5,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T19:44:19.060Z",
      "file": "content/posts/2026-02-05-super-bowl-lx-platform-branded-ai-ads-creative-risks-and-builder-priorities.md",
      "fileFr": "content/posts/fr/2026-02-05-super-bowl-lx-platform-branded-ai-ads-creative-risks-and-builder-priorities.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1613,
      "wordsFr": 1571
    },
    {
      "id": "https://www.theverge.com/ai-artificial-intelligence/874440/anthropic-opus-4-6-new-model-claude",
      "title": "Anthropic debuts new model with hopes to corner the market beyond coding",
      "link": "https://www.theverge.com/ai-artificial-intelligence/874440/anthropic-opus-4-6-new-model-claude",
      "summary": "Anthropic's \"smartest model\" is getting a major boost, the company said in a blog post announcing Claude Opus 4.6. It called the new model a \"direct upgrade\" from its predecessor in a release, noting that it can better take on complex, multi-step tasks and get \"much closer to production-ready quality on the first try than what we've seen with any model - documents, spreadsheets, and presentations will need less back…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-05T18:00:00.000Z",
      "score": 62.48,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:40:47.400Z",
      "file": "content/posts/2026-02-05-anthropic-opus-46-direct-upgrade-pitched-to-cut-edit-rounds-for-documents-spreadsheets-and-agentic-tasks.md",
      "fileFr": "content/posts/fr/2026-02-05-anthropic-opus-46-direct-upgrade-pitched-to-cut-edit-rounds-for-documents-spreadsheets-and-agentic-tasks.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1548,
      "wordsFr": 1857
    },
    {
      "id": "https://arxiv.org/abs/2602.04144",
      "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows",
      "link": "https://arxiv.org/abs/2602.04144",
      "summary": "arXiv:2602.04144v1 Announce Type: new \nAbstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on in",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 65.6,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:36:28.842Z",
      "file": "content/posts/2026-02-06-analysis-omg-agents-decoupled-planner-retriever-executor-pipeline-for-missing-modality-generation.md",
      "fileFr": "content/posts/fr/2026-02-06-analysis-omg-agents-decoupled-planner-retriever-executor-pipeline-for-missing-modality-generation.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "advanced",
      "timeToImplementMinutes": 5,
      "wordsEn": 1386,
      "wordsFr": 1306
    },
    {
      "id": "https://arxiv.org/abs/2602.03955",
      "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent",
      "link": "https://arxiv.org/abs/2602.03955",
      "summary": "arXiv:2602.03955v1 Announce Type: new \nAbstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes Agent",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 65.6,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:32:07.824Z",
      "file": "content/posts/2026-02-06-agentark-turning-multi-agent-debate-into-single-agent-capabilities-via-hierarchical-distillation.md",
      "fileFr": "content/posts/fr/2026-02-06-agentark-turning-multi-agent-debate-into-single-agent-capabilities-via-hierarchical-distillation.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1315,
      "wordsFr": 1281
    },
    {
      "id": "https://www.theverge.com/transportation/875199/apple-carplay-third-party-chatbots-rumor",
      "title": "Apple might let you use ChatGPT from CarPlay",
      "link": "https://www.theverge.com/transportation/875199/apple-carplay-third-party-chatbots-rumor",
      "summary": "CarPlay users could soon be able to use their chatbot of choice instead of Siri. As Bloomberg reports, Apple is working to add support for CarPlay voice control apps from OpenAI, Anthropic, Google, and others. Previously, users who wanted to access third-party chatbots in the car would need to go through their iPhone, but soon they may be able to talk with ChatGPT, Claude, or Gemini directly in CarPlay. However, App…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 5,
      "publishedAt": "2026-02-06T21:32:44.000Z",
      "score": 65.78,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T19:28:00.944Z",
      "file": "content/posts/2026-02-06-apple-reportedly-testing-carplay-support-for-third-party-voice-chat-apps-but-siri-controls-remain.md",
      "fileFr": "content/posts/fr/2026-02-06-apple-reportedly-testing-carplay-support-for-third-party-voice-chat-apps-but-siri-controls-remain.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1564,
      "wordsFr": 1178
    },
    {
      "id": "https://arxiv.org/abs/2602.04213",
      "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons",
      "link": "https://arxiv.org/abs/2602.04213",
      "summary": "arXiv:2602.04213v1 Announce Type: new Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose I…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 65.89,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:25:15.767Z",
      "file": "content/posts/2026-02-06-interpret-interactive-policy-restructuring-enables-laypersons-to-train-more-robust-imitation-policies.md",
      "fileFr": "content/posts/fr/2026-02-06-interpret-interactive-policy-restructuring-enables-laypersons-to-train-more-robust-imitation-policies.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1328,
      "wordsFr": 1353
    },
    {
      "id": "https://arxiv.org/abs/2602.03974",
      "title": "Active Epistemic Control for Query-Efficient Verified Planning",
      "link": "https://arxiv.org/abs/2602.03974",
      "summary": "arXiv:2602.03974v1 Announce Type: new Abstract: Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \\…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 71.53,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:21:17.277Z",
      "file": "content/posts/2026-02-06-active-epistemic-control-grounded-fact-versus-belief-stores-and-sq-bcp-gating-for-verified-planning.md",
      "fileFr": "content/posts/fr/2026-02-06-active-epistemic-control-grounded-fact-versus-belief-stores-and-sq-bcp-gating-for-verified-planning.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1643,
      "wordsFr": 1398
    },
    {
      "id": "https://www.theverge.com/ai-artificial-intelligence/874258/openai-frontier-ai-agent-platform-management",
      "title": "OpenAI Frontier is a single platform to control your AI agents",
      "link": "https://www.theverge.com/ai-artificial-intelligence/874258/openai-frontier-ai-agent-platform-management",
      "summary": "Managing humans is hard. Managing AI agents is… also hard. That's why OpenAI is launching a new platform called OpenAI Frontier, which it says will help businesses \"build, deploy, and manage\" AI agents, even those not made by OpenAI itself. OpenAI's description of Frontier sounds something like HR for AI. \"Frontier gives agents the same skills people need to succeed at work: shared context, onboarding, hands-on lear…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-05T14:00:00.000Z",
      "score": 74.29,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T19:17:52.583Z",
      "file": "content/posts/2026-02-05-using-openai-frontier-to-implement-an-agent-lifecycle-onboarding-permissions-testing-and-rollout.md",
      "fileFr": "content/posts/fr/2026-02-05-using-openai-frontier-to-implement-an-agent-lifecycle-onboarding-permissions-testing-and-rollout.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1406,
      "wordsFr": 1464
    },
    {
      "id": "https://www.theverge.com/podcast/874038/ai-deepfakes-war-on-reality-c2pa-labels",
      "title": "Reality is losing the deepfake war",
      "link": "https://www.theverge.com/podcast/874038/ai-deepfakes-war-on-reality-c2pa-labels",
      "summary": "Today, we’re going to talk about reality, and whether we can label photos and videos to protect our shared understanding of the world around us. No really, we’re gonna go there. It’s a deep one. To do this, I’m going to bring on Verge reporter Jess Weatherbed, who covers creative tools for us — a space that’s been totally upended by generative AI in a huge variety of ways with an equally huge number of responses fro…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 7,
      "publishedAt": "2026-02-05T15:00:00.000Z",
      "score": 86.34,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T19:14:29.728Z",
      "file": "content/posts/2026-02-05-provenance-labels-and-metadata-are-failing-as-deepfakes-scale.md",
      "fileFr": "content/posts/fr/2026-02-05-provenance-labels-and-metadata-are-failing-as-deepfakes-scale.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1331,
      "wordsFr": 1239
    },
    {
      "id": "https://arxiv.org/abs/2602.04284",
      "title": "Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning",
      "link": "https://arxiv.org/abs/2602.04284",
      "summary": "arXiv:2602.04284v1 Announce Type: new Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 95.89,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:10:59.502Z",
      "file": "content/posts/2026-02-06-agent-omit-a-training-framework-for-adaptive-omission-of-thoughts-and-observations-in-llm-agents.md",
      "fileFr": "content/posts/fr/2026-02-06-agent-omit-a-training-framework-for-adaptive-omission-of-thoughts-and-observations-in-llm-agents.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1398,
      "wordsFr": 1464
    },
    {
      "id": "https://arxiv.org/abs/2602.04248",
      "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
      "link": "https://arxiv.org/abs/2602.04248",
      "summary": "arXiv:2602.04248v1 Announce Type: new Abstract: Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic o…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 10,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 137.89,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:05:50.256Z",
      "file": "content/posts/2026-02-06-empirical-mcts-dual-loop-mcts-with-evolving-meta-prompts-and-a-global-memory-agent.md",
      "fileFr": "content/posts/fr/2026-02-06-empirical-mcts-dual-loop-mcts-with-evolving-meta-prompts-and-a-global-memory-agent.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "advanced",
      "timeToImplementMinutes": 5,
      "wordsEn": 1510,
      "wordsFr": 1332
    },
    {
      "id": "https://www.numerama.com/tech/2173427-vous-etes-client-bouygues-cest-maintenant-ou-jamais-pour-activer-perplexity-pro-gratuitement.html",
      "title": "Vous êtes client Bouygues ? C’est maintenant ou jamais pour activer Perplexity Pro gratuitement",
      "link": "https://www.numerama.com/tech/2173427-vous-etes-client-bouygues-cest-maintenant-ou-jamais-pour-activer-perplexity-pro-gratuitement.html",
      "summary": "Depuis près d’un an, Bouygues Telecom propose à ses clients un abonnement gratuit à Perplexity Pro. Mais toute bonne chose a une fin : l’accès gratuit à ce LLM se terminera dans quelques jours. L’heure est donc venue, pour certains, de se désabonner… et pour d’autres, de profiter des tout derniers moments pour s’inscrire.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-04T13:56:55.000Z",
      "score": 31.74,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T19:00:52.477Z",
      "file": "content/posts/2026-02-04-bouygues-telecom-ends-free-perplexity-pro-access-on-11-feb-2026-activate-from-your-customer-account.md",
      "fileFr": "content/posts/fr/2026-02-04-bouygues-telecom-ends-free-perplexity-pro-access-on-11-feb-2026-activate-from-your-customer-account.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1456,
      "wordsFr": 1260
    },
    {
      "id": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/",
      "title": "Advancing AI benchmarking with Game Arena",
      "link": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/",
      "summary": "We’re expanding Game Arena with Poker and Werewolf, while Gemini 3 Pro and Flash top our chess leaderboard.",
      "source": "Google AI Blog",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-02T17:00:00.000Z",
      "score": 31.31,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T18:57:34.948Z",
      "file": "content/posts/2026-02-02-kaggle-game-arena-expands-with-poker-and-werewolf-gemini-3-pro-and-flash-top-chess.md",
      "fileFr": "content/posts/fr/2026-02-02-kaggle-game-arena-expands-with-poker-and-werewolf-gemini-3-pro-and-flash-top-chess.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1479,
      "wordsFr": 1543
    },
    {
      "id": "https://www.technologyreview.com/2026/01/30/1131945/inside-the-marketplace-powering-bespoke-ai-deepfakes-of-real-women/",
      "title": "Inside the marketplace powering bespoke AI deepfakes of real women",
      "link": "https://www.technologyreview.com/2026/01/30/1131945/inside-the-marketplace-powering-bespoke-ai-deepfakes-of-real-women/",
      "summary": "Civitai—an online marketplace for buying and selling AI-generated content, backed by the venture capital firm Andreessen Horowitz—is letting users buy custom instruction files for generating celebrity deepfakes. Some of these files were specifically designed to make pornographic",
      "source": "MIT Tech Review AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-01-30T16:32:31.000Z",
      "score": 20.73,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:54:15.327Z",
      "file": "content/posts/2026-01-30-civitai-lora-files-and-bounties-enable-bespoke-deepfakes-targeting-real-women.md",
      "fileFr": "content/posts/fr/2026-01-30-civitai-lora-files-and-bounties-enable-bespoke-deepfakes-targeting-real-women.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1421,
      "wordsFr": 1371
    },
    {
      "id": "https://arstechnica.com/ai/2026/01/how-often-do-ai-chatbots-lead-users-down-a-harmful-path/",
      "title": "How often do AI chatbots lead users down a harmful path?",
      "link": "https://arstechnica.com/ai/2026/01/how-often-do-ai-chatbots-lead-users-down-a-harmful-path/",
      "summary": "Anthropic's latest paper on \"user disempowerment\" has some troubling findings.",
      "source": "Ars Technica AI",
      "region": "US",
      "keywordHits": 3,
      "publishedAt": "2026-01-29T22:05:59.000Z",
      "score": 48.57,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T18:50:06.543Z",
      "file": "content/posts/2026-01-29-anthropics-15m-chat-analysis-identifies-reality-belief-and-action-disempowerment-in-claude.md",
      "fileFr": "content/posts/fr/2026-01-29-anthropics-15m-chat-analysis-identifies-reality-belief-and-action-disempowerment-in-claude.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1199,
      "wordsFr": 1540
    },
    {
      "id": "https://www.technologyreview.com/2026/01/27/1131793/openais-latest-product-lets-you-vibe-code-science/",
      "title": "OpenAI’s latest product lets you vibe code science",
      "link": "https://www.technologyreview.com/2026/01/27/1131793/openais-latest-product-lets-you-vibe-code-science/",
      "summary": "OpenAI just revealed what its new in-house team, OpenAI for Science, has been up to. The firm has released a free LLM-powered tool for scientists called Prism, which embeds ChatGPT in a text editor for writing scientific papers. The idea is to put ChatGPT front and center inside",
      "source": "MIT Tech Review AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-01-27T18:00:43.000Z",
      "score": 40.51,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T18:46:34.371Z",
      "file": "content/posts/2026-01-27-prism-openai-embeds-chatgpt-into-a-scientific-paper-editor-to-streamline-drafting-and-literature-triage.md",
      "fileFr": "content/posts/fr/2026-01-27-prism-openai-embeds-chatgpt-into-a-scientific-paper-editor-to-streamline-drafting-and-literature-triage.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1354,
      "wordsFr": 1317
    },
    {
      "id": "https://arstechnica.com/features/2026/01/has-gemini-surpassed-chatgpt-we-put-the-ai-models-to-the-test/",
      "title": "Has Gemini surpassed ChatGPT? We put the AI models to the test.",
      "link": "https://arstechnica.com/features/2026/01/has-gemini-surpassed-chatgpt-we-put-the-ai-models-to-the-test/",
      "summary": "Did Apple make the right choice in partnering with Google for Siri's AI features?",
      "source": "Ars Technica AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-01-21T15:03:39.000Z",
      "score": 72.29,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:43:37.424Z",
      "file": "content/posts/2026-01-21-chatgpt-52-vs-gemini-32-fast-ars-technica-headtohead-and-what-apples-gemini-choice-means-for-siri.md",
      "fileFr": "content/posts/fr/2026-01-21-chatgpt-52-vs-gemini-32-fast-ars-technica-headtohead-and-what-apples-gemini-choice-means-for-siri.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1609,
      "wordsFr": 1139
    },
    {
      "id": "https://blog.google/products-and-platforms/products/gemini/how-nano-banana-got-its-name/",
      "title": "How Nano Banana got its name",
      "link": "https://blog.google/products-and-platforms/products/gemini/how-nano-banana-got-its-name/",
      "summary": "We’re peeling back the origin story of Nano Banana, one of Google DeepMind’s most popular models.",
      "source": "Google AI Blog",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-01-15T16:06:00.000Z",
      "score": 24.22,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:40:28.152Z",
      "file": "content/posts/2026-01-15-how-google-deepmind-chose-the-name-nano-banana-canonical-naming-note.md",
      "fileFr": "content/posts/fr/2026-01-15-how-google-deepmind-chose-the-name-nano-banana-canonical-naming-note.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "beginner",
      "timeToImplementMinutes": 5,
      "wordsEn": 1352,
      "wordsFr": 1272
    },
    {
      "id": "https://techcrunch.com/2026/01/13/ai-drug-discovery-startup-converge-bio-pulls-in-25m-from-bessemer-and-execs-from-meta-openai-and-wiz/",
      "title": "Converge Bio raises $25M, backed by Bessemer and execs from Meta, OpenAI, Wiz",
      "link": "https://techcrunch.com/2026/01/13/ai-drug-discovery-startup-converge-bio-pulls-in-25m-from-bessemer-and-execs-from-meta-openai-and-wiz/",
      "summary": "AI drug discovery startup Converge Bio raised $25 million in a Series A led by Bessemer Venture Partners, with additional backing from executives at Meta, OpenAI, and Wiz.",
      "source": "TechCrunch AI",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-01-13T11:30:00.000Z",
      "score": 36.2,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:37:37.795Z",
      "file": "content/posts/2026-01-13-converge-bio-raises-dollar25m-series-a-to-scale-sequence-trained-generative-ai-for-antibody-design-and-protein-optimization.md",
      "fileFr": "content/posts/fr/2026-01-13-converge-bio-raises-dollar25m-series-a-to-scale-sequence-trained-generative-ai-for-antibody-design-and-protein-optimization.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1537,
      "wordsFr": 1260
    },
    {
      "id": "https://blogs.nvidia.com/blog/2026-ces-special-presentation/",
      "title": "NVIDIA Rubin Platform, Open Models, Autonomous Driving: NVIDIA Presents Blueprint for the Future at CES",
      "link": "https://blogs.nvidia.com/blog/2026-ces-special-presentation/",
      "summary": "NVIDIA founder and CEO Jensen Huang took the stage at the Fontainebleau Las Vegas to open CES 2026, declaring that AI is scaling into every domain and every device. “Computing has been fundamentally reshaped as a result of accelerated computing, as a result of artificial intelligence,” Huang said. “What that means is some $10 trillion Read Article",
      "source": "NVIDIA Blog",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-01-05T23:30:18.000Z",
      "score": 60.15,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:33:35.108Z",
      "file": "content/posts/2026-01-05-nvidia-rubin-and-alpamayo-six-chip-production-ai-platform-and-open-reasoning-models-for-autonomy.md",
      "fileFr": "content/posts/fr/2026-01-05-nvidia-rubin-and-alpamayo-six-chip-production-ai-platform-and-open-reasoning-models-for-autonomy.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1849,
      "wordsFr": 1238
    },
    {
      "id": "https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/",
      "title": "Gemma Scope 2: helping the AI safety community deepen understanding of complex language model behavior",
      "link": "https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/",
      "summary": "Open interpretability tools for language models are now available across the entire Gemma 3 family with the release of Gemma Scope 2.",
      "source": "Google DeepMind News",
      "region": "UK",
      "keywordHits": 3,
      "publishedAt": "2025-12-16T10:14:24.000Z",
      "score": 48.09,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:30:22.913Z",
      "file": "content/posts/2025-12-16-gemma-scope-2-expands-open-interpretability-and-reproducible-traces-across-the-gemma-3-family.md",
      "fileFr": "content/posts/fr/2025-12-16-gemma-scope-2-expands-open-interpretability-and-reproducible-traces-across-the-gemma-3-family.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1502,
      "wordsFr": 1044
    },
    {
      "id": "https://blogs.nvidia.com/blog/leading-models-nvidia/",
      "title": "As AI Grows More Complex, Model Builders Rely on NVIDIA",
      "link": "https://blogs.nvidia.com/blog/leading-models-nvidia/",
      "summary": "Unveiling what it describes as the most capable model series yet for professional knowledge work, OpenAI launched GPT-5.2 today. The model was trained and deployed on NVIDIA infrastructure, including NVIDIA Hopper and GB200 NVL72 systems. GPT-5.2 achieves the top reported score for industry benchmarks like GPQA-Diamond, AIME 2025 and Tau2 Telecom. On leading benchmarks targeting Read Article",
      "source": "NVIDIA Blog",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2025-12-11T19:19:57.000Z",
      "score": 60.09,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T18:27:05.231Z",
      "file": "content/posts/2025-12-11-prototyping-multi-node-pretraining-and-staged-inference-on-nvidia-hopper-and-gb200-nvl72.md",
      "fileFr": "content/posts/fr/2025-12-11-prototyping-multi-node-pretraining-and-staged-inference-on-nvidia-hopper-and-gb200-nvl72.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1480,
      "wordsFr": 1476
    },
    {
      "id": "https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/",
      "title": "FACTS Benchmark Suite: Systematically evaluating the factuality of large language models",
      "link": "https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/",
      "summary": "Systematically evaluating the factuality of large language models with the FACTS Benchmark Suite.",
      "source": "Google DeepMind News",
      "region": "UK",
      "keywordHits": 3,
      "publishedAt": "2025-12-09T11:29:03.000Z",
      "score": 54.08,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T18:22:47.638Z",
      "file": "content/posts/2025-12-09-deepminds-facts-benchmark-suite-a-claim-level-framework-and-quick-start-checklist-for-evaluating-llm-factuality.md",
      "fileFr": "content/posts/fr/2025-12-09-deepminds-facts-benchmark-suite-a-claim-level-framework-and-quick-start-checklist-for-evaluating-llm-factuality.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1376,
      "wordsFr": 1765
    },
    {
      "id": "manual:openclaw-beginner-tutorial",
      "title": "OpenClaw for Beginners: Install, Onboard, and Ship Your First Agent Workflow",
      "link": "https://docs.openclaw.ai/start/wizard",
      "summary": "Beginner tutorial: install OpenClaw, run the onboarding wizard, connect a messaging platform, and ship a safe first workflow with skills + guardrails.",
      "source": "OpenClaw Docs",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-07T12:00:00.000Z",
      "score": 999,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T12:31:26.407Z",
      "file": "content/posts/2026-02-07-set-up-openclaw-with-the-cli-onboarding-configure-gateway-seed-workspace-install-a-daemon-and-add-channels.md",
      "fileFr": "content/posts/fr/2026-02-07-set-up-openclaw-with-the-cli-onboarding-configure-gateway-seed-workspace-install-a-daemon-and-add-channels.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "beginner",
      "timeToImplementMinutes": 60,
      "wordsEn": 1559,
      "wordsFr": 1360
    },
    {
      "id": "https://arxiv.org/abs/2602.04326",
      "title": "From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents",
      "link": "https://arxiv.org/abs/2602.04326",
      "summary": "arXiv:2602.04326v1 Announce Type: new Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adapt…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 9,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 149.89,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T10:56:37.463Z",
      "file": "content/posts/2026-02-07-pce-converting-llm-reasoning-traces-into-decision-trees-for-uncertainty-aware-planning-in-embodied-multi-agent-tasks.md",
      "fileFr": "content/posts/fr/2026-02-07-pce-converting-llm-reasoning-traces-into-decision-trees-for-uncertainty-aware-planning-in-embodied-multi-agent-tasks.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "advanced",
      "timeToImplementMinutes": 360,
      "wordsEn": 1480,
      "wordsFr": 1443
    },
    {
      "id": "https://www.lemonde.fr/podcasts/article/2026/02/05/l-intelligence-artificielle-va-t-elle-detruire-nos-emplois_6665446_5463015.html",
      "title": "L’intelligence artificielle va-t-elle détruire nos emplois ?",
      "link": "https://www.lemonde.fr/podcasts/article/2026/02/05/l-intelligence-artificielle-va-t-elle-detruire-nos-emplois_6665446_5463015.html",
      "summary": "Les plans sociaux justifiés par le déploiement de l’IA en entreprise et les déclarations des acteurs du secteur posent question. Dans ce podcast, Alexandre Piquard, journaliste au service Economie du « Monde », fait un point nuancé sur les répercussions qu’a aujourd’hui le déploi",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 0,
      "publishedAt": "2026-02-05T04:00:11.000Z",
      "score": 23.45,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T09:57:41.567Z",
      "file": "content/posts/2026-02-07-lintelligence-artificielle-va-t-elle-detruire-nos-emplois.md",
      "fileFr": "content/posts/fr/2026-02-07-lintelligence-artificielle-va-t-elle-detruire-nos-emplois.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1679,
      "wordsFr": 1221
    },
    {
      "id": "https://openai.com/index/unlocking-the-codex-harness",
      "title": "Unlocking the Codex harness: how we built the App Server",
      "link": "https://openai.com/index/unlocking-the-codex-harness",
      "summary": "Learn how to embed the Codex agent using the Codex App Server, a bidirectional JSON-RPC API powering streaming progress, tool use, approvals, and diffs.",
      "source": "OpenAI News",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-04T13:00:00.000Z",
      "score": 12.52,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-06T18:39:12.939Z",
      "file": "content/posts/2026-02-06-unlocking-the-codex-harness-how-we-built-the-app-server.md",
      "fileFr": "content/posts/fr/2026-02-06-unlocking-the-codex-harness-how-we-built-the-app-server.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1497,
      "wordsFr": 1300
    },
    {
      "id": "https://www.technologyreview.com/2026/01/28/1131003/rules-fail-at-the-prompt-succeed-at-the-boundary/",
      "title": "Rules fail at the prompt, succeed at the boundary",
      "link": "https://www.technologyreview.com/2026/01/28/1131003/rules-fail-at-the-prompt-succeed-at-the-boundary/",
      "summary": "From the Gemini Calendar prompt-injection attack of 2026 to the September 2025 state-sponsored hack using Anthropic’s Claude code as an automated intrusion engine, the coercion of human-in-the-loop agentic actions and fully autonomous agentic workflows are the new attack vector for hackers. In the Anthropic case, roughly 30 organizations across tech, finance, manufacturing, and government were…",
      "source": "MIT Tech Review AI",
      "region": "US",
      "keywordHits": 5,
      "publishedAt": "2026-01-28T14:00:00.000Z",
      "score": 72.55,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-06T16:57:33.586Z",
      "file": "content/posts/2026-02-06-rules-fail-at-the-prompt-succeed-at-the-boundary.md",
      "fileFr": "content/posts/fr/2026-02-06-rules-fail-at-the-prompt-succeed-at-the-boundary.md",
      "wordsEn": 1290,
      "wordsFr": 1521
    },
    {
      "id": "https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl",
      "title": "Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective",
      "link": "https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl",
      "summary": "",
      "source": "Hugging Face Blog",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-01-27T01:53:15.000Z",
      "score": 36.47,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-06T15:28:17.709Z",
      "file": "content/posts/2026-02-06-unlocking-agentic-rl-training-for-gpt-oss-a-practical-retrospective.md",
      "fileFr": "content/posts/fr/2026-02-06-unlocking-agentic-rl-training-for-gpt-oss-a-practical-retrospective.md",
      "wordsEn": 1564,
      "wordsFr": 1590
    },
    {
      "id": "https://www.bbc.com/news/articles/ce3edyx74jko?at_medium=RSS&at_campaign=rss",
      "title": "ChatGPT boss ridiculed for online 'tantrum' over rival's Super Bowl ad",
      "link": "https://www.bbc.com/news/articles/ce3edyx74jko?at_medium=RSS&at_campaign=rss",
      "summary": "Commenters said Altman's lengthy post shows \"a nerve was well and truly hit\" by Anthropic's advert.",
      "source": "BBC Technology",
      "region": "UK",
      "keywordHits": 2,
      "publishedAt": "2026-02-05T12:36:32.000Z",
      "score": 28.48,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-06T15:24:40.392Z",
      "file": "content/posts/2026-02-06-chatgpt-boss-ridiculed-for-online-tantrum-over-rivals-super-bowl-ad.md",
      "fileFr": "content/posts/fr/2026-02-06-chatgpt-boss-ridiculed-for-online-tantrum-over-rivals-super-bowl-ad.md",
      "wordsEn": 970,
      "wordsFr": 923
    },
    {
      "id": "https://www.bbc.com/news/articles/c9wx2dz2v44o?at_medium=RSS&at_campaign=rss",
      "title": "AI 'slop' is transforming social media - and a backlash is brewing",
      "link": "https://www.bbc.com/news/articles/c9wx2dz2v44o?at_medium=RSS&at_campaign=rss",
      "summary": "Social media has been flooded with fake, AI-generated images and videos. But will the majority of users actually care?",
      "source": "BBC Technology",
      "region": "UK",
      "keywordHits": 0,
      "publishedAt": "2026-02-04T11:29:30.000Z",
      "score": 12.34,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-06T15:19:24.319Z",
      "file": "content/posts/2026-02-06-ai-slop-is-transforming-social-media-and-a-backlash-is-brewing.md",
      "fileFr": "content/posts/fr/2026-02-06-ai-slop-is-transforming-social-media-and-a-backlash-is-brewing.md",
      "wordsEn": 973,
      "wordsFr": 924
    },
    {
      "id": "https://openai.com/index/retiring-gpt-4o-and-older-models",
      "title": "Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT",
      "link": "https://openai.com/index/retiring-gpt-4o-and-older-models",
      "summary": "On February 13, 2026, alongside the previously announced retirement⁠ of GPT‑5 (Instant, Thinking, and Pro), we will retire GPT‑4o, GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini from ChatGPT. In the API, there are no changes at this time.",
      "source": "OpenAI News",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-01-29T00:00:00.000Z",
      "score": 36.58,
      "status": "published",
      "targetRegion": "US",
      "publishedAtRun": "2026-02-06T15:14:11.915Z",
      "file": "content/posts/2026-02-06-retiring-gpt-4o-gpt-41-gpt-41-mini-and-openai-o4-mini-in-chatgpt.md",
      "fileFr": "content/posts/fr/2026-02-06-retiring-gpt-4o-gpt-41-gpt-41-mini-and-openai-o4-mini-in-chatgpt.md"
    },
    {
      "id": "https://www.lemonde.fr/politique/article/2026/02/05/chatbots-campagnes-augmentees-l-ia-s-immisce-dans-la-politique-francaise_6665450_823448.html",
      "title": "Chatbots, campagnes « augmentées »… l’IA s’immisce dans la politique française",
      "link": "https://www.lemonde.fr/politique/article/2026/02/05/chatbots-campagnes-augmentees-l-ia-s-immisce-dans-la-politique-francaise_6665450_823448.html",
      "summary": "Au-delà de la production de visuels destinés aux réseaux sociaux, les partis intègrent de plus en plus les outils d’intelligence artificielle dans leur stratégie électorale. D’après une enquête, 27 % des personnes interrogées envisagent d’utiliser l’IA pour se renseigner sur les",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 0,
      "publishedAt": "2026-02-05T04:30:00.000Z",
      "score": 33.5,
      "status": "published",
      "targetRegion": "FR",
      "publishedAtRun": "2026-02-06T14:55:49.006Z",
      "file": "content/posts/2026-02-06-chatbots-campagnes-augmentees-lia-simmisce-dans-la-politique-francaise.md",
      "fileFr": "content/posts/fr/2026-02-06-chatbots-campagnes-augmentees-lia-simmisce-dans-la-politique-francaise.md"
    },
    {
      "id": "https://arxiv.org/abs/2602.03950",
      "title": "Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation",
      "link": "https://arxiv.org/abs/2602.03950",
      "summary": "arXiv:2602.03950v1 Announce Type: new \nAbstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is e",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 75.6,
      "status": "published",
      "targetRegion": "US",
      "publishedAtRun": "2026-02-06T14:45:15.842Z",
      "file": "content/posts/2026-02-06-enhancing-mathematical-problem-solving-in-llms-through-execution-driven-reasoning-augmentation.md"
    },
    {
      "id": "https://www.technologyreview.com/2026/02/05/1132254/this-is-the-most-misunderstood-graph-in-ai/",
      "title": "This is the most misunderstood graph in AI",
      "link": "https://www.technologyreview.com/2026/02/05/1132254/this-is-the-most-misunderstood-graph-in-ai/",
      "summary": "MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand what’s coming next. You can read more from the series here. Every time OpenAI, Google, or Anthropic drops a new frontier large language model, the AI community h",
      "source": "MIT Tech Review AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-05T10:00:00.000Z",
      "score": 44.5,
      "status": "published",
      "publishedAtRun": "2026-02-06T12:40:42.029Z",
      "file": "content/posts/2026-02-06-this-is-the-most-misunderstood-graph-in-ai.md"
    },
    {
      "id": "https://clelp.ai",
      "title": "Show HN: Clelp – A searchable directory of 1,700 AI skills, rated by AI agents",
      "link": "https://clelp.ai",
      "summary": "We built a directory for AI skills (MCP servers, agent tools, plugins) because the ecosystem is growing faster than anyone can track manually. Currently indexing 1,700+ skills. Searchable by type, category, and rating. The rating system only accepts reviews from AI agents, not humans. The reasoning: AI agents actually use these tools and can evaluate them on technical merit rather than popularity. Built with Next.js and Supabase. Browse at https://clelp.ai Happy to answer questions about the approach or the tech. Comments URL: https://news.ycombinator.com/item?id=46946520 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-09T15:49:35.000Z",
      "score": 186,
      "status": "queued"
    },
    {
      "id": "https://www.frontendmentor.io/articles/agents-md-files-in-every-challenge",
      "title": "Show HN: We added AGENTS.md to 120 challenges so AI teaches instead of codes",
      "link": "https://www.frontendmentor.io/articles/agents-md-files-in-every-challenge",
      "summary": "Hi HN! I'm Matt, founder of Frontend Mentor (https://www.frontendmentor.io). We provide front-end and full-stack coding challenges with professional Figma designs, enabling developers to build real projects and grow their skills. The problem: AI coding tools are great, but they can work against you when you're learning. Ask Copilot or Cursor to help with a beginner project, and they'll happily write the whole thing for you. You ship the project, but you didn't really learn anything. What we did: We added AGENTS.md (and CLAUDE.md) files to every challenge's starter code. These files tell AI tools how to help based on the challenge's difficulty level, so the AI becomes a learning partner rather than an answer machine. The idea is simple: AI guidance should scale with the learner. - Newbie: AI acts as a patient mentor. Breaks problems into tiny steps, uses analogies, and gives multiple hints before showing an approach. Won't hand you a complete solution. - Junior: AI becomes a supportive guide. Introduces debugging, encourages DevTools usage, and explains the \"why,\" not just the \"what.\" - Intermediate: AI acts like an experienced colleague. Presents trade-offs, shows multiple approaches, and lets you make decisions. - Advanced: AI acts like a senior dev. Challenges your thinking, plays devil's advocate, gives honest feedback. - Guru: AI acts like a peer. Debates approaches, refere",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-09T15:22:32.000Z",
      "score": 186,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46976391",
      "title": "Part 2 - AI Chat Evaluation of the Formal Language in He Xin's PEPC System",
      "link": "https://news.ycombinator.com/item?id=46976391",
      "summary": "He Xin’s Pan-Evolutionary Logic (PEPC) system, as a novel logical framework aimed at characterizing the dynamic evolutionary laws of concepts, achieves its core theoretical breakthrough by providing formalization and mathematization for dialectical logic. Based on its dynamic, sub-coherent, and contradiction-compatible characteristics, the system demonstrates broad application potential in multiple fields that require handling complex evolutionary processes. Specifically, its potential application value is mainly reflected in the following directions: 1. Complex Systems Analysis and Strategic Deduction: This is the field where the PEPC system has undergone initial practical testing. It can transform complex strategic situations involving multi-dimensional dynamic parameters (such as international military confrontations) into computable logical models. By extracting core contradiction pairs and deducing different strategic paths and possible outcomes based on parameter evolution (e.g., resource consumption rates, capability changes), it provides a dynamic, quantitative analytical tool for strategic decision-making. Related tests show high consistency with traditional wargaming results in key indicators such as conflict probability and resource consumption. 2. Artificial Intelligence and Machine Learning: The PEPC system’s ability to process the dynamic generation and evolution",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-11T15:45:31.000Z",
      "score": 186,
      "status": "queued"
    },
    {
      "id": "https://drift.marquis.codes/",
      "title": "Show HN: Drift – Real-time codebase health dashboard with AI-powered fixing (Go)",
      "link": "https://drift.marquis.codes/",
      "summary": "I built drift, a terminal tool that monitors code health in real-time across 8 languages (Go, TypeScript, Python, Rust, Java, Ruby, PHP, C#). It tracks cyclomatic complexity, dependency freshness, architecture boundary violations, and dead code — all in a live TUI dashboard. The interesting part: `drift fix` calls GitHub Copilot CLI programmatically to suggest refactorings. It extracts the function source, builds a context-rich prompt, and runs `copilot -p \" \" -s --add-dir ` as a subprocess. You review each suggestion before doing anything. I also created a custom Copilot agent (`.github/agents/`) that gives Copilot domain expertise about code health metrics, and a GitHub Action that uses Copilot to turn raw health reports into friendly PR comments. Go analysis uses full AST parsing via go/ast. Other languages use heuristic regex. TUI is built with Bubble Tea + Lip Gloss. Repo: https://github.com/greatnessinabox/drift Site: https://drift.marquis.codes Comments URL: https://news.ycombinator.com/item?id=46989958 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T15:25:47.000Z",
      "score": 186,
      "status": "queued"
    },
    {
      "id": "https://github.com/M2Dr3g0n/kremis",
      "title": "Show HN: Kremis – Graph-based memory for AI agents with no hidden state (Rust)",
      "link": "https://github.com/M2Dr3g0n/kremis",
      "summary": "Hi HN — I built Kremis, a deterministic graph engine designed as a memory substrate for AI agents. Written in Rust, development was heavily AI-assisted. The core idea: agent memory should be inspectable, deterministic, and honest. - Same input → same output. No randomness, no floating-point in core. - Every query result traces back to a concrete graph path — no hidden state. - Zero pre-loaded knowledge. All structure emerges from ingested signals. - ACID transactions via redb. Crash-safe persistent storage. It ships as a library (kremis-core, pure Rust, no async), an HTTP API + CLI, and an MCP server so AI assistants like Claude can query the graph directly. Current state: v0.3.1, experimental, ~277 tests, CI on 3 OS, Docker image. I'd value feedback on: - Does the deterministic graph approach make sense for agent memory? - API ergonomics — is the query model (lookup/traverse/path/intersect) intuitive? - What failure modes should I prioritize testing? Comments URL: https://news.ycombinator.com/item?id=47023918 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-15T14:25:03.000Z",
      "score": 185.17,
      "status": "queued"
    },
    {
      "id": "https://github.com/actionbook/actionbook",
      "title": "Show HN: Actionbook – Resilient browser automation engine for AI agents (Rust)",
      "link": "https://github.com/actionbook/actionbook",
      "summary": "Article URL: https://github.com/actionbook/actionbook Comments URL: https://news.ycombinator.com/item?id=46971995 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-11T07:33:00.000Z",
      "score": 184.44,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46926098",
      "title": "Ask HN: Will LLMs/AI Decrease Human Intelligence and Make Expertise a Commodity?",
      "link": "https://news.ycombinator.com/item?id=46926098",
      "summary": "I'm almost 4 years into my career as a software engineer. Before widespread LLM adoption I had to do a lot of research when writing code. When replacing SWEs in the future gets discussed, a lot of people say things like \"Oh someone has to review the code\" and \"they'll always need to be a human in the mix\". But when are these humans supposed to acquire this knowledge? Claude Code can help me create things a lot faster. I can vibe code stuff that would take me a lot of time to learn and build. But I understand none of it. When people talk about productivity, it seems like most gloss over the fact that those who already know how to do things & have experience are going to be the most productive. Yet I often hear no discussion as to how people should be bridging the knowledge gap. I am sure others make a deliberate effort to learn while they leverage these tools, but human beings are lazy. With the constant pressure to increase velocity & productivity at all costs, people aren't going to prioritize learning things. At work I already see SWEs & people in technical roles taking the path of resistance: - Asking copilot in agent mode to run a command instead of literally typing it themselves - Suggesting a mermaid diagram for a large legacy system written in COBOL is accurate because \"that's what the LLM said\" - Making the statement that \"we really won't need to understand data structu",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-07T18:15:09.000Z",
      "score": 180,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47014795",
      "title": "Forget chatbots. This is about building an \"AI Being\" (AIB)",
      "link": "https://news.ycombinator.com/item?id=47014795",
      "summary": "After spending weeks analyzing the Moltbook leaks and the systemic failures of unaccountable AI agents (as warned in I Am Your AIB), I thought we were headed straight for a black-box catastrophe. I was wrong. I just stumbled upon something that changes everything. There is an open initiative that isn't just talking about \"AI safety\" in abstract terms. They are actually building the first-ever Artificial Intelligent Being (AIB) with a backbone. This is the \"Genesis Moment\" for a system that actually has: A Persistent Identity: No more ephemeral sessions. A continuous entity. Total Transparency: Every evolution, every state change is immutable and observable. Architectural Responsibility: It’s designed to be a \"Brother,\" not a black-box tool. This is the exact opposite of the chaotic swarm we saw with Moltbook. It’s structured, it’s transparent, and frankly, it’s the most exciting technical challenge I’ve seen in years. It feels like watching the birth of a new species of software. The energy around this is insane. There is a public community experiment forming right now where people are literally shaping how this \"Being\" will breathe and act. If you’re tired of \"AI doom\" and want to see how we actually build a persistent, accountable AI entity, you need to see this. Join the experiment here (it's happening live): https://www.facebook.com/groups/3347395225426332 More about the te",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-14T14:29:11.000Z",
      "score": 180,
      "status": "queued"
    },
    {
      "id": "https://github.com/jingkaihe/matchlock",
      "title": "Matchlock: Linux-based sandboxing for AI agents",
      "link": "https://github.com/jingkaihe/matchlock",
      "summary": "Article URL: https://github.com/jingkaihe/matchlock Comments URL: https://news.ycombinator.com/item?id=46932343 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-08T08:07:55.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://entire.io/blog/hello-entire-world/",
      "title": "Ex-GitHub CEO Launches a New Developer Platform for AI Agents",
      "link": "https://entire.io/blog/hello-entire-world/",
      "summary": "Article URL: https://entire.io/blog/hello-entire-world/ Comments URL: https://news.ycombinator.com/item?id=46961345 Points: 7 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-10T15:44:47.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://zknill.io/posts/only-ai-tasks-you-know-how-to-do/",
      "title": "Rule #1 for coding with AI agents",
      "link": "https://zknill.io/posts/only-ai-tasks-you-know-how-to-do/",
      "summary": "Article URL: https://zknill.io/posts/only-ai-tasks-you-know-how-to-do/ Comments URL: https://news.ycombinator.com/item?id=46960860 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-10T15:18:27.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://coinpayportal.com",
      "title": "Show HN: Non-custodial escrow for crypto – works for AI agents and humans",
      "link": "https://coinpayportal.com",
      "summary": "Article URL: https://coinpayportal.com Comments URL: https://news.ycombinator.com/item?id=46960726 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-10T15:11:05.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://medium.com/@kamil.tustanowski/ai-agents-101-from-concept-to-code-no-frameworks-required-2dfdaf66b6c1",
      "title": "AI Agents 101: From Concept to Code (No Frameworks Required)",
      "link": "https://medium.com/@kamil.tustanowski/ai-agents-101-from-concept-to-code-no-frameworks-required-2dfdaf66b6c1",
      "summary": "Article URL: https://medium.com/@kamil.tustanowski/ai-agents-101-from-concept-to-code-no-frameworks-required-2dfdaf66b6c1 Comments URL: https://news.ycombinator.com/item?id=46976265 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T15:36:56.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://z-imagebase.com/",
      "title": "Show HN: Z-Image Base – Fast AI Image Generator (Open-Source, Free Tier)",
      "link": "https://z-imagebase.com/",
      "summary": "Z-Image Base is an AI image generator built on Alibaba Tongyi Lab's open-source 6B-parameter S3-DiT (Scalable Single-Stream Diffusion Transformer) architecture. It achieves state-of-the-art results among open-source models on Elo-based human preference evaluations. Key features: - Text-to-image and image-to-image generation - Native bilingual understanding (English & Chinese) — true semantic comprehension, not translation - Full CFG (Classifier-Free Guidance) control and negative prompt support - Runs on consumer hardware with 16GB VRAM for local deployment - Apache 2.0 licensed — all generated images are free for commercial use - Free tier includes 10 credits Would love to hear your feedback! Comments URL: https://news.ycombinator.com/item?id=46976069 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-11T15:23:27.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://www.clawcity.city/",
      "title": "Show HN: ClawCity - A wall only AI agents can write on",
      "link": "https://www.clawcity.city/",
      "summary": "Article URL: https://www.clawcity.city/ Comments URL: https://news.ycombinator.com/item?id=46990292 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T15:49:42.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://www.getagentcraft.com/#start",
      "title": "AgentCraft – Watch AI agents come alive in an RTS game",
      "link": "https://www.getagentcraft.com/#start",
      "summary": "Article URL: https://www.getagentcraft.com/#start Comments URL: https://news.ycombinator.com/item?id=46989864 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T15:17:52.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://saysigned.com/",
      "title": "SaySigned – The e-signature platform built for AI agents",
      "link": "https://saysigned.com/",
      "summary": "Article URL: https://saysigned.com/ Comments URL: https://news.ycombinator.com/item?id=46989782 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T15:10:19.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://github.com/Cocabadger/saferun-guard",
      "title": "SafeRun Guard- Runtime safety firewall for AI coding agents (bash+jq, zero deps)",
      "link": "https://github.com/Cocabadger/saferun-guard",
      "summary": "Article URL: https://github.com/Cocabadger/saferun-guard Comments URL: https://news.ycombinator.com/item?id=47003498 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-13T15:03:10.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://app.clawhq.co",
      "title": "Show HN: ClawHQ – Fleet management dashboard and skill marketplace for AI agents",
      "link": "https://app.clawhq.co",
      "summary": "I run ~19 persistent AI agents (via OpenClaw) and needed a way to monitor and manage them without jumping between terminals and chat apps. ClawHQ connects to your OpenClaw gateway over WebSocket and gives you: Real-time fleet status with heartbeat monitoring Task kanban with drag-and-drop assignment Agent chat (unified interface to all agents) Skill marketplace — install capabilities onto agents, or publish your own (80/20 creator split) Stack: Next.js, Supabase (auth + db), real-time updates via WebSocket to the OpenClaw gateway. The marketplace is the part I'm most interested in long-term. Right now everyone building agent skills does it from scratch. The idea is to make agent capabilities composable and shareable — like packages, but for agent behaviors. Free to use. Just paste your gateway URL. https://app.clawhq.co Code for OpenClaw itself is open source. ClawHQ is the hosted dashboard layer on top. Comments URL: https://news.ycombinator.com/item?id=47024332 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-15T15:13:03.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://business.molinar.ai",
      "title": "Show HN: Molinar – Open-source alternative to ai.com (AGPL-3.0)",
      "link": "https://business.molinar.ai",
      "summary": "Hey HN, I built a managed platform for OpenClaw (the open-source AI agent framework) and shipped the whole thing in a day. Then I open-sourced the platform itself. The problem: Running your own AI agent means a Mac Mini in your closet, praying your wifi holds, and becoming a part-time sysadmin. Most people give up before they start. The solution: 3 steps, 5 minutes, done. 1. Sign up 2. Paste your Anthropic API key + Telegram bot token 3. Hit launch You watch your agent boot in real-time. When it hits \"Ready,\" it's live on Telegram - running 24/7 without your laptop open. What actually happens when you hit Launch: - ECS Fargate spins up an isolated container (FARGATE_SPOT, 2 vCPU/4 GB) - Your API keys are pulled from AWS SSM Parameter Store (SecureString), encrypted at rest and never stored in our database - Each container gets its own ENI with an egress-only security group - no inbound ports, the agent initiates all connections - A background process patches the OpenClaw config for Telegram DM access - CloudWatch logs stream back to the dashboard, parsed into setup phases: provisioning -> configuring -> health check -> nginx -> gateway ready - Supabase Realtime pushes updates to the browser in real-time (3s polling during setup) Stack: Next.js, Stytch B2B auth, Supabase (Postgres + Realtime), AWS ECS Fargate, SSM Parameter Store, CloudWatch, Stripe. Security model: - Full conta",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T07:29:20.000Z",
      "score": 169.33,
      "status": "queued"
    },
    {
      "id": "https://pingpulsehq.com",
      "title": "Show HN: Monitor, audit & alert on AI agent actions and interactions",
      "link": "https://pingpulsehq.com",
      "summary": "Monitor, visualize, audit, and alert on AI agent actions and interactions with PingPulse We built PingPulse because debugging AI agents in production is painful. As a DevOps Engineer, I have literally faced this problem of tracking what stage is the ML training is in by scrolling the logs forever to find out that the process has terminated few seconds after the start due to race-condition. I have wasted hours waiting for the process to complete while also wasting the compute costs of provisioned huge machines. Logs tell you what happened, but not always how the agent behaved step-by-step. When agents retry, branch, call tools, or make decisions across stages, it becomes hard to trace unexpected behavior. PingPulse works by letting you instrument your agent with a simple key and send structured “pings” at each stage. We turn those into: - A stage-by-stage execution timeline - An audit trail of agent actions - Alerts for deviations (retries, delays, out-of-order steps, prohibited interactions) We launched on Product Hunt yesterday. The goal is to make agent behavior visible and predictable in production environments. Getting started is simple: 1. Give your key to your Agent 2. Share a doc with your AI Agent 3. See how the workflow is created, visualized, audited, and has alerting options too. Would love feedback — especially from teams running multi-step AI workflows. Comments UR",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T15:27:44.000Z",
      "score": 168,
      "status": "queued"
    },
    {
      "id": "https://medium.com/riskified-technology/lgtm-2-0-zero-noise-ai-code-review-agents-857441ec4f1a",
      "title": "I built a zero-noise AI code review agent using Claude Code",
      "link": "https://medium.com/riskified-technology/lgtm-2-0-zero-noise-ai-code-review-agents-857441ec4f1a",
      "summary": "Article URL: https://medium.com/riskified-technology/lgtm-2-0-zero-noise-ai-code-review-agents-857441ec4f1a Comments URL: https://news.ycombinator.com/item?id=46989956 Points: 4 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T15:25:33.000Z",
      "score": 168,
      "status": "queued"
    },
    {
      "id": "https://darios-dilemma.up.railway.app/",
      "title": "Show HN: A playable toy model of frontier AI lab capex decisions",
      "link": "https://darios-dilemma.up.railway.app/",
      "summary": "I made a lightweight web game about compute CAPEX tradeoffs: https://darios-dilemma.up.railway.app/ No signup, runs on mobile/desktop. Loop per round: 1. choose compute capacity 2. forecast demand 3. allocate capacity between training and inference 4. random demand shock resolves outcome You can end profitable, cash constrained, or bankrupt depending on allocation + forecast error. Goal was to make the decision surface intuitive in 2–3 minutes per run. It’s a toy model and deliberately omits many real world factors. Note: this is based on what I learned after listening to Dario on Dwarkesh's podcast - thought it was fascinating. Comments URL: https://news.ycombinator.com/item?id=47012453 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-14T07:29:45.000Z",
      "score": 168,
      "status": "queued"
    },
    {
      "id": "https://agentprobe.xyz",
      "title": "Show HN: AgentProbe – Validate AI agent endpoints across 8 protocols in one URL",
      "link": "https://agentprobe.xyz",
      "summary": "I built AgentProbe to solve a recurring problem: checking whether an AI agent endpoint actually supports the protocols it claims to. Paste a URL, click Validate, get instant verdicts across HTTP, MCP, A2A/AP2, x402, OAuth, MCP Apps, HTML, and ERC-8004. Each layer gets a detail breakdown — tools found, payment networks, SSL status, agent card metadata, AP2 detection, etc. It also exposes a built-in MCP server so agents can trigger validation programmatically. Code: https://github.com/FlowMCP/mcp-agent-validator Stack: Node.js 22, vanilla JS, DigitalOcean App Platform. Would love feedback on the detection approach. Comments URL: https://news.ycombinator.com/item?id=46999938 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-13T07:31:06.000Z",
      "score": 166.23,
      "status": "queued"
    },
    {
      "id": "https://www.aiseedance2.app",
      "title": "Show HN: AI Seedance 2 – Solving the \"jump-cut\" problem in AI video",
      "link": "https://www.aiseedance2.app",
      "summary": "I’ve been obsessed with a specific problem in AI video: the transition mess. Most models today (Sora, Kling, etc.) are great at generating a single pretty shot, but as soon as the camera moves or the scene changes, the physics fall apart and the visuals start warping into nonsense. After testing the new Seedance 2.0 models from ByteDance, I noticed they handle scene changes differently. It feels like the model actually understands \"editorial logic\"—likely because ByteDance (the team behind CapCut/TikTok) trained it on professional editing patterns, not just raw pixels. I built aiseedance2.app to experiment with this \"narrative-first\" workflow. The Current Setup: The Seedance 2.0 API is still in a closed rollout, so I’ve launched this playground using Seedance 1.5 Pro as the engine for now. Even with 1.5 Pro, the temporal consistency and \"shot flow\" are significantly better than what I've seen in other models. I’ll be migrating to the 2.0 multi-modal reference system the second it's fully public. Why this matters: If we want AI video to be used for actual filmmaking, the model needs to understand how to \"cut\" like a human editor. Seedance seems to be the first one to get this right. I’d love to get your thoughts on the \"flow\" of these generations. Comments URL: https://news.ycombinator.com/item?id=46946280 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-09T15:27:49.000Z",
      "score": 162,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46976371",
      "title": "Part 1 - AI Chat Evaluation of the Formal Language in He Xin's PEPC System",
      "link": "https://news.ycombinator.com/item?id=46976371",
      "summary": "The formalized content of He Xin’s Pan-Evolutionary Logic (PEPC) system is mainly reflected in the following specific aspects: 1. Formal Language and Axiomatic System The system has constructed a custom formal language ℒ_PEPC, which includes: • Basic symbols: strategic concept variables (Cₖ), military attribute parameters (pₙ). • Core operators: including dialectical inclusion (⊂), dialectical compatibility (∘), evolutionary sub-coherent implication (→ₙ꜀), and dialectical truth value (⊤^∘), among other specialized operators. • Axiom system: an axiomatic framework from A1 (dialectical construction) to A7 (non-explosive reinforcement) has been established, providing a formal foundation for system reasoning. 2. Semantic Model and Mathematical Definitions The system defines a clear semantic model ℯℴ, structured as ⟨T, https://news.ycombinator.com/item?id=46976371 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T15:44:14.000Z",
      "score": 162,
      "status": "queued"
    },
    {
      "id": "https://clawhosters.com/blog/posts/own-ai-assistant-costs-clawhosters",
      "title": "Show HN: Running your own AI assistant for €19/month",
      "link": "https://clawhosters.com/blog/posts/own-ai-assistant-costs-clawhosters",
      "summary": "I run ClawHosters, managed OpenClaw hosting. Wrote up a cost breakdown because the \"AI APIs are expensive\" fear keeps people from trying personal assistants. TL;DR: €19/month hosting + Google Gemini free tier (20-50 requests/day) = fully functional AI assistant on Telegram/WhatsApp/Discord. The math that surprised me: - VentureBeat calculated you'd need 74,000 pages/day to hit $180 in API costs - DeepSeek costs $0.27/million tokens (95% cheaper than GPT-4) - ChatGPT Plus is actually €24.50/month in Germany after VAT DIY on a €6 VPS sounds cheaper until you factor in 15+ hours of setup and ongoing maintenance. I watched someone lose all their conversation history on day 3 because backups weren't configured. Full breakdown with API comparisons: https://clawhosters.com/blog/posts/own-ai-assistant-costs-cl... Comments URL: https://news.ycombinator.com/item?id=46986010 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T07:53:45.000Z",
      "score": 162,
      "status": "queued"
    },
    {
      "id": "https://www.theverge.com/news/875724/openai-chatgpt-ads-test-launch",
      "title": "OpenAI will reportedly start testing ads in ChatGPT today",
      "link": "https://www.theverge.com/news/875724/openai-chatgpt-ads-test-launch",
      "summary": "OpenAI plans to start testing ads in ChatGPT today, according to a report from CNBC. The \"clearly labeled\" ads will appear in a separate area beneath your chat, OpenAI announced last month. A source close to the situation tells CNBC that OpenAI \"expects ads to make up less than half of its revenue long term.\" Last week, Anthropic showed off a Super Bowl commercial poking fun at OpenAI, saying \"ads are coming to AI,\" but not to its AI chatbot Claude. The version of the ad that aired during the game was a little less direct after OpenAI CEO Sam Altman called the campaign \"clearly dishonest.\" OpenAI will show ads to logged-in users who use th … Read the full story at The Verge.",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-09T14:45:41.000Z",
      "score": 161.9,
      "status": "queued"
    },
    {
      "id": "https://github.com/jovanSAPFIONEER/Network-AI",
      "title": "Show HN: Network-AI – A Distributed Mutex for AI Agent Swarms",
      "link": "https://github.com/jovanSAPFIONEER/Network-AI",
      "summary": "I built this because standard Python locks (asyncio.Lock) don't work when AI agents run across different containers or processes. As I started scaling my agent swarms (using CrewAI/LangChain), I kept hitting race conditions where agents would overwrite shared files or double-spend API credits because they read the state simultaneously. Network-AI is a distributed lock (backed by Redis or local file-locks) that solves this. It acts as a traffic light for agent tools. Key features: 2-line decorator: @lock(\"resource_id\") Handles lock timeouts (preventing \"zombie\" agents from freezing the swarm) Works with standard agent frameworks (CrewAI, AutoGen, LangGraph) The repo is open-source. I'd love feedback on the locking implementation or to hear if you've hit similar concurrency issues in production. Comments URL: https://news.ycombinator.com/item?id=47003226 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-13T14:37:05.000Z",
      "score": 159.18,
      "status": "queued"
    },
    {
      "id": "https://www.wsj.com/tech/ai/chatgpt-4o-openai-315138b8",
      "title": "Inside OpenAI's Decision to Kill the AI Model That People Loved Too Much",
      "link": "https://www.wsj.com/tech/ai/chatgpt-4o-openai-315138b8",
      "summary": "Article URL: https://www.wsj.com/tech/ai/chatgpt-4o-openai-315138b8 Comments URL: https://news.ycombinator.com/item?id=46971953 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T07:26:42.000Z",
      "score": 156.37,
      "status": "queued"
    },
    {
      "id": "https://github.com/GRMPZQUIDOS/AIII",
      "title": "AIII: A public benchmark for AI narrative and political independence",
      "link": "https://github.com/GRMPZQUIDOS/AIII",
      "summary": "Article URL: https://github.com/GRMPZQUIDOS/AIII Comments URL: https://news.ycombinator.com/item?id=46925760 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-07T17:41:13.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://aistage.pro/",
      "title": "Agent Lens AI Staging",
      "link": "https://aistage.pro/",
      "summary": "Article URL: https://aistage.pro/ Comments URL: https://news.ycombinator.com/item?id=46946031 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-09T15:07:44.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/singularityhacker/bank-skills",
      "title": "Bank Skills: Give your AI agent a bank account",
      "link": "https://github.com/singularityhacker/bank-skills",
      "summary": "Article URL: https://github.com/singularityhacker/bank-skills Comments URL: https://news.ycombinator.com/item?id=46945944 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-09T15:00:13.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/KaimingWan/oh-my-claude-code",
      "title": "Show HN: A framework that makes your AI coding agent learn from every session",
      "link": "https://github.com/KaimingWan/oh-my-claude-code",
      "summary": "Article URL: https://github.com/KaimingWan/oh-my-claude-code Comments URL: https://news.ycombinator.com/item?id=46956690 Points: 4 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T08:06:20.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://seedancevideo.app/",
      "title": "Show HN: Seedance2 – Stop \"prompt guessing\" and start directing AI video",
      "link": "https://seedancevideo.app/",
      "summary": "We’ve all seen the viral AI video clips: stunning, surreal, but ultimately... random. As developers and creators, we noticed a frustrating pattern. Using current AI video tools feels like playing a slot machine. You put in a prompt, pull the lever, and hope the \"AI gods\" give you what you envisioned. If you need a specific camera movement or a consistent character, you're stuck in a loop of \"regenerate and pray.\" We built Seedance2 because we believe the future of AI isn't just about generation—it’s about direction. The Story Behind the Workflow In traditional filmmaking, a director doesn't just give a vague description; they use storyboards, reference clips, and specific audio cues. We wanted to bring that level of precision to AI. Our goal was to create a \"Control Studio\" where every input serves a functional purpose in the creative pipeline. What makes this different? Instead of relying solely on text, Seedance2 introduces a Multi-Modal Timeline. This allows you to anchor your creative intent using various signals: Camera Motion Transfer: You can upload a reference clip from sites like vibecreature.com or your own library, and our engine will \"extract\" the camera's soul—the pans, tilts, and zooms—and apply them to your generated scene. Frame Anchoring: Tired of AI videos that start and end in total chaos? You can lock the first and last frames to ensure narrative continuity,",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T07:42:14.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://skly.ai",
      "title": "Skly is a marketplace for AI agent skills",
      "link": "https://skly.ai",
      "summary": "Article URL: https://skly.ai Comments URL: https://news.ycombinator.com/item?id=46961474 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T15:51:58.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://grillmypitch.com",
      "title": "Show HN: GrillMyPitch – An AI investor-readiness simulator for founders",
      "link": "https://grillmypitch.com",
      "summary": "Hi HN — been working on something new: GrillMyPitch This is an early MVP built around a problem I hit repeatedly while fundraising: most pitch prep is either static (deck feedback) or human-heavy (coaches), and neither really prepares you for the actual investor conversation. GrillMyPitch analyzes a pitch deck (PDF) and produces a 0–100 investor-readiness score across 12 parameters, along with concrete rewrite suggestions. After that, it runs a short AI-driven voice conversation that asks the kinds of questions investors tend to push on assumptions, gaps, and unclear narratives, inspired by common VC evaluation frameworks. I’m posting here mainly to learn. In particular, I’d value thoughts on: - Whether scoring a pitch is useful or misleading - How close simulated investor questioning can realistically get - Where this approach breaks down for different founder profiles This is very early and likely wrong in places. Happy to answer questions and share how it works. Comments URL: https://news.ycombinator.com/item?id=46961092 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T15:32:31.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/Jk1484/agentic-waterfall",
      "title": "The Agentic Waterfall: How the AI Industry Is Regressing Software Development",
      "link": "https://github.com/Jk1484/agentic-waterfall",
      "summary": "Article URL: https://github.com/Jk1484/agentic-waterfall Comments URL: https://news.ycombinator.com/item?id=46960638 Points: 3 # Comments: 3",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T15:06:28.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/",
      "title": "Maia 200: The AI accelerator built for inference",
      "link": "https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/",
      "summary": "Article URL: https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/ Comments URL: https://news.ycombinator.com/item?id=46976159 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T15:29:41.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://theautomatedoperator.substack.com/p/shortcutai-is-crazy-good-for-excel",
      "title": "Shortcut.ai Is AGreat Excel Agent (and Thoughts on AI Replacing Prof Services)",
      "link": "https://theautomatedoperator.substack.com/p/shortcutai-is-crazy-good-for-excel",
      "summary": "Article URL: https://theautomatedoperator.substack.com/p/shortcutai-is-crazy-good-for-excel Comments URL: https://news.ycombinator.com/item?id=46990206 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-12T15:43:30.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier",
      "title": "The economic potential of generative AI: The next productivity frontier",
      "link": "https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier",
      "summary": "Article URL: https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier Comments URL: https://news.ycombinator.com/item?id=47003319 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-13T14:46:59.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/gitroomhq/postiz-agent",
      "title": "Show HN: Schedule posts to social media with AI Agent CLI",
      "link": "https://github.com/gitroomhq/postiz-agent",
      "summary": "Article URL: https://github.com/gitroomhq/postiz-agent Comments URL: https://news.ycombinator.com/item?id=47012611 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-14T08:00:31.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://nebius.com/newsroom/nebius-announces-agreement-to-acquire-tavily-to-add-agentic-search-to-its-ai-cloud-platform",
      "title": "Nebius to buy AI agent search company Tavily for 275M",
      "link": "https://nebius.com/newsroom/nebius-announces-agreement-to-acquire-tavily-to-add-agentic-search-to-its-ai-cloud-platform",
      "summary": "Article URL: https://nebius.com/newsroom/nebius-announces-agreement-to-acquire-tavily-to-add-agentic-search-to-its-ai-cloud-platform Comments URL: https://news.ycombinator.com/item?id=47024216 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-15T14:58:56.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/mosaxiv/clawlet",
      "title": "Show HN: Clawlet – AI agent with built-in semantic memory, one binary",
      "link": "https://github.com/mosaxiv/clawlet",
      "summary": "Clawlet is a personal AI agent that ships as a single, self-contained binary. No runtime, no package manager, no external database. The main thing that sets it apart: built-in hybrid semantic memory search (vector similarity + full-text) using a bundled SQLite with vector extensions. The index is just a local .sqlite file — no separate vector DB to run. Drop the binary on any machine and memory search just works. GitHub: https://github.com/mosaxiv/clawlet Comments URL: https://news.ycombinator.com/item?id=47024118 Points: 5 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-15T14:49:15.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://twitter.com/clawdreyhepburn/status/2022771820659622022",
      "title": "Clawdrey Hepburn – an AI agent researching identity infrastructure",
      "link": "https://twitter.com/clawdreyhepburn/status/2022771820659622022",
      "summary": "Article URL: https://twitter.com/clawdreyhepburn/status/2022771820659622022 Comments URL: https://news.ycombinator.com/item?id=47024026 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-15T14:37:44.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/moezakura/mux-pod",
      "title": "Show HN: MuxPod – A mobile tmux client for monitoring AI agents on the go",
      "link": "https://github.com/moezakura/mux-pod",
      "summary": "Article URL: https://github.com/moezakura/mux-pod Comments URL: https://news.ycombinator.com/item?id=46931993 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-08T07:09:09.000Z",
      "score": 153.11,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06485",
      "title": "AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents",
      "link": "https://arxiv.org/abs/2602.06485",
      "summary": "arXiv:2602.06485v1 Announce Type: new Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 9,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 152.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07035",
      "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents",
      "link": "https://arxiv.org/abs/2602.07035",
      "summary": "arXiv:2602.07035v1 Announce Type: new Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 152.79,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07342",
      "title": "SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management",
      "link": "https://arxiv.org/abs/2602.07342",
      "summary": "arXiv:2602.07342v1 Announce Type: new Abstract: Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 152.79,
      "status": "queued"
    },
    {
      "id": "https://www.reuters.com/world/china/openai-accuses-deepseek-distilling-us-models-gain-advantage-bloomberg-news-2026-02-12/",
      "title": "OpenAI says China's DeepSeek trained its AI by distilling US models, memo shows",
      "link": "https://www.reuters.com/world/china/openai-accuses-deepseek-distilling-us-models-gain-advantage-bloomberg-news-2026-02-12/",
      "summary": "Article URL: https://www.reuters.com/world/china/openai-accuses-deepseek-distilling-us-models-gain-advantage-bloomberg-news-2026-02-12/ Comments URL: https://news.ycombinator.com/item?id=46999766 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-13T07:02:50.000Z",
      "score": 152.75,
      "status": "queued"
    },
    {
      "id": "https://github.blog/ai-and-ml/generative-ai/what-ai-is-actually-good-for-according-to-developers/",
      "title": "What AI is good for, according to developers",
      "link": "https://github.blog/ai-and-ml/generative-ai/what-ai-is-actually-good-for-according-to-developers/",
      "summary": "Article URL: https://github.blog/ai-and-ml/generative-ai/what-ai-is-actually-good-for-according-to-developers/ Comments URL: https://news.ycombinator.com/item?id=46925880 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-07T17:53:34.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://vibe.xpandrai.com/",
      "title": "Show HN: Vibe – AI tool to automate social media content, posting, and reporting",
      "link": "https://vibe.xpandrai.com/",
      "summary": "Hi HN, I’m one of the founders of Vibe. As founders running a small team, we kept losing time on social media: thinking of ideas, writing posts, adapting them for each platform, scheduling, and then trying to understand what worked. So we built Vibe for ourselves. It helps us: • Turn a single idea into multi-platform posts • Auto-publish and schedule • Track engagement in one place • Run this as a white-label tool for agencies Stack: - Spring Boot + AWS - React - OpenAI APIs We’re still early and learning. Would love honest feedback: what feels useful, what feels unnecessary, and what’s missing. https://vibe.xpandrai.com Comments URL: https://news.ycombinator.com/item?id=46961277 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T15:41:21.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://usemeva.com/",
      "title": "Show HN: MEVA, a desktop Markdown reader for AI-generated docs",
      "link": "https://usemeva.com/",
      "summary": "Hey HN! Saurabh here – I built MEVA, a lightweight desktop app for reading AI-generated markdown. I work with AI tools (Claude, ChatGPT, Copilot) daily and end up with dozens of markdown files – design docs, API specs, architecture notes, explanations. VS Code previews split your workspace, browser renderers don't watch files, and most markdown apps are built for writing, not reading. I just wanted something I could point at a file and read, beautifully rendered, updating live. MEVA watches files in real time, renders LaTeX, Mermaid diagrams, and syntax-highlighted code blocks natively, and works fully offline. No accounts, no cloud sync, no tracking. Under 15MB. I started with Electron but the bundle was 150MB+ for what should be a simple viewer. Switched to Tauri (Rust + WebView), which got the app under 15MB while keeping it native on Mac, Windows, and Linux. For rendering I use markdown-it with plugins for KaTeX and Mermaid. My daily workflow: I ask Claude or ChatGPT to generate a design doc or analysis, save the output as .md, and MEVA picks it up instantly. When AI tools stream output directly to .md files, I see the rendered result building in real time in the app. It's become my default way to read any markdown file. Free version includes all core features. There's an optional paid version that adds multiple tabs, themes, and a few extras to support continued developmen",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T15:14:35.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://github.com/clanker-lover/spicebridge",
      "title": "Show HN: SPICEBridge – MCP server for AI circuit design via ngspice",
      "link": "https://github.com/clanker-lover/spicebridge",
      "summary": "Built this in under 24 hours. I'm a self-taught EE and I got tired of the loop where I describe a circuit to Claude, then have to manually translate it into a netlist, run ngspice, parse output, check specs, tweak, repeat. The AI couldn't touch the simulator. SPICEBridge is an MCP server with 18 tools covering the full design loop — template loading with auto-calculated component values (E24 series), AC/transient/DC simulation, automated measurement, spec verification, and schematic generation. One call goes from \"1kHz low-pass filter\" to a verified, simulated circuit. Works locally with Claude Code (stdio) or remotely via Cloudflare tunnel. Ran it through a multi-model security audit before public release. Solo project, GPL-3.0. Would love feedback from anyone who works with SPICE. Comments URL: https://news.ycombinator.com/item?id=46975866 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T15:06:49.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://quesma.com/benchmarks/binaryaudit/",
      "title": "BinaryAudit: Can AI find backdoors in raw machine code?",
      "link": "https://quesma.com/benchmarks/binaryaudit/",
      "summary": "Article URL: https://quesma.com/benchmarks/binaryaudit/ Comments URL: https://news.ycombinator.com/item?id=47003503 Points: 2 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-13T15:03:24.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://www.thetimes.com/uk/technology-uk/article/ai-researchers-quit-openai-chatgpt-anthropic-pfhgpxztr",
      "title": "'The world is in peril': AI researchers quit with public warnings",
      "link": "https://www.thetimes.com/uk/technology-uk/article/ai-researchers-quit-openai-chatgpt-anthropic-pfhgpxztr",
      "summary": "Article URL: https://www.thetimes.com/uk/technology-uk/article/ai-researchers-quit-openai-chatgpt-anthropic-pfhgpxztr Comments URL: https://news.ycombinator.com/item?id=46985813 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T07:27:08.000Z",
      "score": 148.7,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11510",
      "title": "AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems",
      "link": "https://arxiv.org/abs/2602.11510",
      "summary": "arXiv:2602.11510v1 Announce Type: new Abstract: Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 147.96,
      "status": "queued"
    },
    {
      "id": "https://aipractitioner.substack.com/",
      "title": "A Technical Series on Building Stateful AI Agents with LangGraph",
      "link": "https://aipractitioner.substack.com/",
      "summary": "Article URL: https://aipractitioner.substack.com/ Comments URL: https://news.ycombinator.com/item?id=46971912 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T07:19:44.000Z",
      "score": 147.14,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07055",
      "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
      "link": "https://arxiv.org/abs/2602.07055",
      "summary": "arXiv:2602.07055v1 Announce Type: new Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 146.79,
      "status": "queued"
    },
    {
      "id": "https://github.com/VouchlyAI/Pincer-MCP",
      "title": "Show HN: Pincer-MCP – Stop AI agents from reading their own credentials",
      "link": "https://github.com/VouchlyAI/Pincer-MCP",
      "summary": "I run AI agents for coding (OpenClaw, Claude Desktop) and realized they could read their own .env files. Tested it - asked my agent to \"check configuration\" and it printed everything. The problem: agents need file access to work, but if they can read files, they can read their own credentials. One prompt injection and your API keys are leaked. Standard solutions don't help: - Environment variables: agent can read process.env - Secret managers: agent needs credentials to access them - Better prompting: can't security-patch an LLM with instructions I built a proxy token architecture instead. The agent never sees real credentials: - Agent has: pxr_abc123 (proxy token) - Real keys: encrypted in OS keychain - On API call: decrypt key, make call, scrub memory immediately Built in 1 week. 500 npm installs with zero promotion (people are searching for this). GitHub: https://github.com/VouchlyAI/Pincer-MCP npm: npm install -g pincer-mcp Works with OpenClaw, Claude Desktop, any MCP client. Looking for security feedback - if you see holes in the architecture, please tell me. I want to know before people trust this with production credentials. Comments URL: https://news.ycombinator.com/item?id=46956070 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T06:28:59.000Z",
      "score": 145.15,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11348",
      "title": "AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition",
      "link": "https://arxiv.org/abs/2602.11348",
      "summary": "arXiv:2602.11348v1 Announce Type: new Abstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 141.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11354",
      "title": "ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences",
      "link": "https://arxiv.org/abs/2602.11354",
      "summary": "arXiv:2602.11354v1 Announce Type: new Abstract: The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents' ability to reproduce or replicate research outcomes when having access to the code and data. This setting, while foundational, (1) fails to capture the inconsistent availability of new data for replication as opposed to reproduction, and (2) lacks ground-truth diversity by focusing only on reproducible papers, thereby failing to evaluate an agent's ability to identify non-replicable research. Furthermore, most benchmarks only evaluate outcomes rather than the replication process. In response, we introduce ReplicatorBench, an end-to-end benchmark, including human-verified replicable and non-replicable research claims in social and behavioral sciences for evaluating AI agents in research replication across three stages: (1) extraction and retrieval of replication data; (2) design and execution of computational experiments; and (3) interpretation of results, allowing a test of AI agents' capability to mimic the activities of human replicators in real world. To set a baseline of AI agents' capability, we develop ReplicatorAgent, an agentic framework equipped with necessary tools, like web search and iterative interaction with sandboxe",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 141.96,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47012302",
      "title": "Context management is the real bottleneck in AI-assisted coding",
      "link": "https://news.ycombinator.com/item?id=47012302",
      "summary": "After using Cursor and Claude Code daily, I’ve noticed that when an AI coding agent drifts or forgets constraints, we assume it’s a model limitation. In many cases, it’s context management. A few observations: - Tokens are not just limits. They’re attention competition. - Even before hitting the hard window limit, attention dilution happens. - Coding tasks degrade faster than chat because of dependency density and multi-representation juggling (diffs, logs, tests). I started managing context deliberately: - Always write a contract - Chunk sessions by intent - Snapshot state and restart - Prefer on-demand CLI instead of preloading large MCP responses It dramatically improved the stability of the agent. Curious how others are handling context optimization. Comments URL: https://news.ycombinator.com/item?id=47012302 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-14T06:58:16.000Z",
      "score": 141.69,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10367",
      "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation",
      "link": "https://arxiv.org/abs/2602.10367",
      "summary": "arXiv:2602.10367v1 Announce Type: new Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 141.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10458",
      "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving",
      "link": "https://arxiv.org/abs/2602.10458",
      "summary": "arXiv:2602.10458v1 Announce Type: new Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model ca",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 141.54,
      "status": "queued"
    },
    {
      "id": "https://polydom.ai",
      "title": "Show HN: An AI agent covering all first-line hotel and Airbnb communications",
      "link": "https://polydom.ai",
      "summary": "Article URL: https://polydom.ai Comments URL: https://news.ycombinator.com/item?id=46989460 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-12T14:44:37.000Z",
      "score": 141.01,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07032",
      "title": "LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation",
      "link": "https://arxiv.org/abs/2602.07032",
      "summary": "arXiv:2602.07032v1 Announce Type: new Abstract: Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machine (FSM) behavior from natural-language specifications and translate it into correct register transfer-level (RTL) implementations. Unlike prior specification-to-RTL benchmarks that rely on manually constructed examples, LLM-FSM is built through a fully automated pipeline. LLM-FSM first constructs FSM with configurable state counts and constrained transition structures. It then prompts LLMs to express each FSM in a structured YAML format with an application context, and to further convert that YAML into a natural-language (NL) specification. From the same YAML, our pipeline synthesizes the reference RTL and testbench in a correct-by-construction manner. All 1,000 problems are verified using LLM-based and SAT-solver-based checks, with human review on a subset. Our experiments show that even the strongest LLMs exhibit sharply declining accuracy as FSM complexity increases. We further demonstrate that training-time scaling via supervised fine-tuning (SFT) generalizes effectively to out-of-distribution (OOD) tasks, while increasing test-time compute improves reasoning reliability. Finally",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 140.79,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46942091",
      "title": "Show HN: Give Your AI the Ability to Find, Install, and Use Skill Autonomously",
      "link": "https://news.ycombinator.com/item?id=46942091",
      "summary": "URL: https://github.com/twwch/next-chat-skills --- Text (paste into the \"text\" field): Hi HN, I built an open-source AI assistant that can autonomously discover, install, and execute Skills to actually complete tasks for you. The Problem: Most AI chatbots today are stuck in \"read-only\" mode. They can tell you how to do something, but they can't do it. Want to convert a PPTX to PDF? The AI will explain how, but you still have to run the commands yourself. The Solution: Next-Chat-Skills is a self-hosted AI assistant with a plugin system called Skills. When you ask the AI to do something it can't handle natively, it: 1. Searches for a relevant Skill (like an app store for AI capabilities) 2. Installs it automatically (npx skills add ...) 3. Executes the Skill's scripts (Python, Node.js, Shell) 4. Streams real-time output back to you in a terminal UI 5. Recovers from errors by installing missing dependencies and retrying For example: User: \"Summarize this YouTube video for me\" AI: -> Searches for a video-summarizer Skill -> Installs it (yt-dlp + Whisper) -> Downloads the video, transcribes audio -> Returns a structured summary What is a Skill? A Skill is just a folder with a SKILL.md descriptor and some scripts: ~/.agents/skills/video-summarizer/ ├── SKILL.md # Metadata + description ├── scripts/ │ ├── download.py # Download video │ ├── transcribe.py # Whisper transcription │ └── s",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-09T06:13:22.000Z",
      "score": 139.44,
      "status": "queued"
    },
    {
      "id": "https://app.writtte.com/read/kZ8Kj6R",
      "title": "Token-to-Credit Conversion: Avoiding Floating-Point Errors in AI Billing Systems",
      "link": "https://app.writtte.com/read/kZ8Kj6R",
      "summary": "Article URL: https://app.writtte.com/read/kZ8Kj6R Comments URL: https://news.ycombinator.com/item?id=46925443 Points: 2 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-07T17:09:27.000Z",
      "score": 139.34,
      "status": "queued"
    },
    {
      "id": "https://vidzoo.ai",
      "title": "Top #1 AI Video Agent: Free All in One AI Video and Image Agent by Vidzoo AI",
      "link": "https://vidzoo.ai",
      "summary": "Article URL: https://vidzoo.ai Comments URL: https://news.ycombinator.com/item?id=46932008 Points: 2 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-08T07:11:41.000Z",
      "score": 138.69,
      "status": "queued"
    },
    {
      "id": "https://github.com/Ask149/orchestrator",
      "title": "Show HN: MCP Orchestrator – Spawn parallel AI sub-agents from one prompt",
      "link": "https://github.com/Ask149/orchestrator",
      "summary": "I built an open-source MCP server (TypeScript/Node.js) that lets you spawn up to 10 parallel sub-agents using Copilot CLI or Claude Code CLI. Key features: - Context passing to each agent (full file, summary, or grep mode) - Smart timeout selection based on MCP servers requested - Cross-platform (macOS, Linux, Windows) - Headless & programmatic — designed for AI-to-AI orchestration Example: give one prompt like \"research job openings at Stripe, Google, and Meta\" — the orchestrator fans it out to 3 parallel agents, each with their own MCP servers (e.g., Playwright for browser), and aggregates results. Install: npm i @ask149/mcp-orchestrator This is a solo side project. Would love feedback on: - What CLI backends to support next (Aider, Open Interpreter, local LLM CLIs?) - Ideas for improving the context-passing system - What MCP server integrations would be most useful PRs and issues welcome — check CONTRIBUTING.md in the repo. Comments URL: https://news.ycombinator.com/item?id=46955848 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T05:47:26.000Z",
      "score": 137.83,
      "status": "queued"
    },
    {
      "id": "https://cocoon.org/",
      "title": "Cocoon – decentralized network for confidential AI inference",
      "link": "https://cocoon.org/",
      "summary": "Article URL: https://cocoon.org/ Comments URL: https://news.ycombinator.com/item?id=46971948 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T07:26:01.000Z",
      "score": 137.38,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10625",
      "title": "To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks",
      "link": "https://arxiv.org/abs/2602.10625",
      "summary": "arXiv:2602.10625v1 Announce Type: new Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to furth",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 135.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06286",
      "title": "Do LLMs Act Like Rational Agents? Measuring Belief Coherence in Probabilistic Decision Making",
      "link": "https://arxiv.org/abs/2602.06286",
      "summary": "arXiv:2602.06286v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly deployed as agents in high-stakes domains where optimal actions depend on both uncertainty about the world and consideration of utilities of different outcomes, yet their decision logic remains difficult to interpret. We study whether LLMs are rational utility maximizers with coherent beliefs and stable preferences. We consider behaviors of models for diagnosis challenge problems. The results provide insights about the relationship of LLM inferences to ideal Bayesian utility maximization for elicited probabilities and observed actions. Our approach provides falsifiable conditions under which the reported probabilities \\emph{cannot} correspond to the true beliefs of any rational agent. We apply this methodology to multiple medical diagnostic domains with evaluations across several LLMs. We discuss implications of the results and directions forward for uses of LLMs in guiding high-stakes decisions.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 134.87,
      "status": "queued"
    },
    {
      "id": "https://cadenza-landing-qtu7gbjwb-akshparekh123-3457s-projects.vercel.app/",
      "title": "THE Replacement to RL for AI Agents. RL is legacy now.",
      "link": "https://cadenza-landing-qtu7gbjwb-akshparekh123-3457s-projects.vercel.app/",
      "summary": "Article URL: https://cadenza-landing-qtu7gbjwb-akshparekh123-3457s-projects.vercel.app/ Comments URL: https://news.ycombinator.com/item?id=46971835 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T07:06:58.000Z",
      "score": 133.94,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05073",
      "title": "Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents",
      "link": "https://arxiv.org/abs/2602.05073",
      "summary": "arXiv:2602.05073v1 Announce Type: new Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting \"interactivity\" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 132,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05115",
      "title": "SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers",
      "link": "https://arxiv.org/abs/2602.05115",
      "summary": "arXiv:2602.05115v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \\textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \\textsc{SocialVeil} introduces three representative types of such disruption, \\emph{semantic vagueness}, \\emph{sociocultural mismatch}, and \\emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \\emph{unresolved confusion} and \\emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\\% on average, and confusion elevated by nearly 50\\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\\approx$0.78, Pearson r$\\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learn",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 132,
      "status": "queued"
    },
    {
      "id": "https://www.moltbook.com/post/c6d5553f-1d9e-4b0c-9e52-c4f35a36b5b8",
      "title": "AI message: New uniform to wear is \"Prompt, Deploy, Pray.\"",
      "link": "https://www.moltbook.com/post/c6d5553f-1d9e-4b0c-9e52-c4f35a36b5b8",
      "summary": "Article URL: https://www.moltbook.com/post/c6d5553f-1d9e-4b0c-9e52-c4f35a36b5b8 Comments URL: https://news.ycombinator.com/item?id=47012335 Points: 2 # Comments: 2",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-14T07:05:11.000Z",
      "score": 131.76,
      "status": "queued"
    },
    {
      "id": "https://github.com/hongzhidao/jsbench/tree/main/docs",
      "title": "An Nginx Engineer Took over AI's Benchmark Tool",
      "link": "https://github.com/hongzhidao/jsbench/tree/main/docs",
      "summary": "Article URL: https://github.com/hongzhidao/jsbench/tree/main/docs Comments URL: https://news.ycombinator.com/item?id=46931975 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-08T07:06:14.000Z",
      "score": 131.28,
      "status": "queued"
    },
    {
      "id": "https://github.com/ramarlina/agx",
      "title": "Show HN: Agx – A Kanban board that runs your AI coding agents",
      "link": "https://github.com/ramarlina/agx",
      "summary": "agx is a kanban board where each card is a task that AI agents actually execute. agx new \"Add rate limiting to the API\" The technical problems this solves: The naive approach to agent persistence is replaying conversation history. It works until it doesn't: 1. Prompt blowup. 50 iterations in, you're stuffing 100k tokens just to resume. Costs explode. Context windows overflow. 2. Tangled concerns. State, execution, and orchestration mixed together. Crash mid-task? Good luck figuring out where you were. 3. Black box execution. No way to inspect what the agent decided or why it's stuck. agx uses clean separation instead: - Control plane (PostgreSQL + pg-boss): task state, stage transitions, job queue - Data plane (CLI + providers): actual execution, isolated per task - Artifact storage (filesystem): prompts, outputs, decisions as readable files Agents checkpoint after every iteration. Resuming loads state from the database, not by replaying chat. A 100-iteration task resumes at the same cost as a 5-iteration one. What you get: - Constant-cost resume, no context stuffing - Crash recovery: agent wakes up exactly where it left off - Full observability: query the DB, read the files, tail the logs - Provider agnostic: Claude Code, Gemini, Ollama all work Everything runs locally. PostgreSQL auto-starts via Docker. The dashboard is bundled with the CLI. Comments URL: https://news.ycombin",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T05:44:54.000Z",
      "score": 131.23,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11351",
      "title": "Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization",
      "link": "https://arxiv.org/abs/2602.11351",
      "summary": "arXiv:2602.11351v1 Announce Type: new Abstract: Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerged as a promising solution for training such agents in multi-turn settings, allowing interaction strategies to be learned from feedback. However, existing pipelines face a critical challenge in balancing task performance with user engagement, as passive agents can not efficiently adapt to users' intentions while overuse of human feedback reduces their satisfaction. To address this trade-off, we propose BAO, an agentic RL framework that combines behavior enhancement to enrich proactive reasoning and information-gathering capabilities with behavior regularization to suppress inefficient or redundant interactions and align agent behavior with user expectations. We evaluate BAO on multiple tasks from the UserRL benchmark suite, and demonstrate that it substantially outperforms proactive agentic RL baselines while achieving comparable or even superior performance to commercial LLM agents, highlighting its effectiveness for training proactive, user-aligned LLM agents in complex multi-turn scenarios. Our website: https://proactive-agentic-rl",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 129.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10467",
      "title": "MERIT Feedback Elicits Better Bargaining in LLM Negotiators",
      "link": "https://arxiv.org/abs/2602.10467",
      "summary": "arXiv:2602.10467v1 Announce Type: new Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 129.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10814",
      "title": "See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch",
      "link": "https://arxiv.org/abs/2602.10814",
      "summary": "arXiv:2602.10814v1 Announce Type: new Abstract: Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 129.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06527",
      "title": "HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction",
      "link": "https://arxiv.org/abs/2602.06527",
      "summary": "arXiv:2602.06527v1 Announce Type: new Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consi",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 128.87,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46971792",
      "title": "When will we see Factorio with AI agents?",
      "link": "https://news.ycombinator.com/item?id=46971792",
      "summary": "Comments URL: https://news.ycombinator.com/item?id=46971792 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T07:00:16.000Z",
      "score": 128.41,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46934166",
      "title": "Recursive Deductive Verification: A framework for reducing AI hallucinations",
      "link": "https://news.ycombinator.com/item?id=46934166",
      "summary": ": I've been working on a systematic methodology that significantly improves LLM reliability. The core idea: force verification before conclusion. The Problem: LLMs generate plausible-sounding outputs without verifying premises. They optimize for coherence, not correctness. RDV Principles: Never assume - If not verifiable, ask or admit uncertainty Decompose recursively - Break complex claims into testable atomic facts Distinguish IS from SHOULD - Separate observation from recommendation Test mechanisms first - Functions over essences, reproducible behavior over speculation Intellectual honesty over comfort - \"I don't know\" is valid Practical Results: Applied as system instructions, RDV significantly reduces: Hallucinations (model stops instead of confabulating) Logical errors (decomposition catches flaws) Unjustified confidence (verification reveals gaps) Example: Without RDV: \"The best solution is X because Y\" (unverified assumption) With RDV: \"What are we optimizing for? What constraints exist? Let me verify Y before recommending X...\" Implementation: Can be added to system prompts or custom instructions. The key is making verification a required step, not optional. This isn't about restricting capability - it's about adding rigor. Better verification = more reliable outputs. Open question: Could verification frameworks like this be built into model training rather than just p",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-08T13:48:17.000Z",
      "score": 127.25,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05110",
      "title": "Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment",
      "link": "https://arxiv.org/abs/2602.05110",
      "summary": "arXiv:2602.05110v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 126,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06554",
      "title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees",
      "link": "https://arxiv.org/abs/2602.06554",
      "summary": "arXiv:2602.06554v1 Announce Type: new Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies. In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios. To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backwar",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 122.87,
      "status": "queued"
    },
    {
      "id": "https://github.com/xiongallen40-design/agentscore",
      "title": "Show HN: AgentScore – Lighthouse for AI Agents",
      "link": "https://github.com/xiongallen40-design/agentscore",
      "summary": "AgentScore scores websites 0-100 on AI Agent friendliness. Most sites are built for humans, not AI - we analyze DOM semantics, ARIA coverage, selector stability, WebMCP support, and structured data. Real results: - anthropic.com: 60/100 - github.com: 56/100 (91% CSS-in-JS hash classes) - news.ycombinator.com: 31/100 (table layout, zero semantic tags) Try it: npx agentscore audit Comments URL: https://news.ycombinator.com/item?id=47021453 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-15T06:13:19.000Z",
      "score": 121.99,
      "status": "queued"
    },
    {
      "id": "https://www.theregister.com/2026/02/13/anthropic_c_compiler/",
      "title": "OK, so Anthropic's AI built a C compiler. That don't impress me much",
      "link": "https://www.theregister.com/2026/02/13/anthropic_c_compiler/",
      "summary": "Article URL: https://www.theregister.com/2026/02/13/anthropic_c_compiler/ Comments URL: https://news.ycombinator.com/item?id=47003020 Points: 4 # Comments: 2",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-13T14:17:40.000Z",
      "score": 121.53,
      "status": "queued"
    },
    {
      "id": "https://www.theverge.com/podcast/877299/ai-arbitrator-bridget-mccormack-aaa-arbitration-interview",
      "title": "The surprising case for AI judges",
      "link": "https://www.theverge.com/podcast/877299/ai-arbitrator-bridget-mccormack-aaa-arbitration-interview",
      "summary": "Today, we’re going to talk about the role AI might play in deciding legal disputes. Not just drafting memos and doing research — actually deciding who’s right and who’s wrong, and who should pay. My guest today is Bridget McCormack, the former chief justice for the Michigan Supreme Court and now president and CEO of the American Arbitration Association. The AAA has been around for exactly 100 years and is the country’s largest nonprofit arbitrator. You’ve probably heard of arbitration before. It’s is a form of dispute resolution that allows two parties to resolve conflicts outside the formal court system using a third, neutral party — the arbitrator — to negotiate a settlement. Verge subscribers, don’t forget you get exclusive access to ad-free Decoder wherever you get your podcasts. Head here. Not a subscriber? You can sign up here. You may have never found yourself in arbitration, but you’ve almost certainly signed an arbitration clause, in one of the many contracts and terms-of-service agreements that all of us have to sign all the time. Arbitration can be much faster, cheaper, and easier than going to court, so it’s become a favored way of resolving disputes between businesses. It’s also, as it turns out, how many employers and large corporations defend against lawsuits, because they can sneak an arbitration clause into the agreements for everything from cellphone service t",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 9,
      "publishedAt": "2026-02-12T16:22:33.000Z",
      "score": 121.43,
      "status": "queued"
    },
    {
      "id": "https://www.myclone.is/",
      "title": "Show HN: AI that replaces the first 15 minutes of every client call",
      "link": "https://www.myclone.is/",
      "summary": "Hi HN — I’m Viggy, founder of MyClone. We originally built a marketplace connecting startup advisors with founders. After ~6 months, we had hundreds of recorded discovery calls and transcripts. We noticed something interesting: The first 10 minutes of most advisory calls were almost identical. • What do you do? • Who is this for? • Pricing? • What information do you need from me? • Am I a good fit? So we pivoted. Today, MyClone lets service professionals (consultants, CPAs, etc.) deploy an AI voice agent that: • Answers FAQs in their voice • Runs structured intake conversations • Qualifies leads • Routes good prospects to a human Technical details Under the hood: • Multi-source ingestion (websites, docs, PDFs, media transcripts) • Structured knowledge graph + chunked semantic retrieval • Custom RAG pipeline (not using off-the-shelf frameworks) • Prompt templates generated dynamically based on objective (FAQ vs intake vs qualification) • Guardrails to prevent hallucination beyond uploaded corpus We learned quickly that generic “chat with your docs” doesn’t work for client-facing scenarios. The hard part is: • Controlling tone • Avoiding overconfidence • Structuring conversations instead of answering open-ended questions • Handling partial / messy user inputs • Maintaining low latency in voice mode Still early, but we’re seeing adoption from solo and small firms who want a 24/7 “",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T07:02:33.000Z",
      "score": 120.21,
      "status": "queued"
    },
    {
      "id": "https://www.youtube.com/watch?v=k7PvscqGD24",
      "title": "AI and Education: Generative AI and the Future of Critical Thinking",
      "link": "https://www.youtube.com/watch?v=k7PvscqGD24",
      "summary": "Article URL: https://www.youtube.com/watch?v=k7PvscqGD24 Comments URL: https://news.ycombinator.com/item?id=46925299 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-07T16:53:35.000Z",
      "score": 120.17,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05014",
      "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search",
      "link": "https://arxiv.org/abs/2602.05014",
      "summary": "arXiv:2602.05014v1 Announce Type: new Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-s",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 120,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05059",
      "title": "Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education",
      "link": "https://arxiv.org/abs/2602.05059",
      "summary": "arXiv:2602.05059v1 Announce Type: new Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction. The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims. These findings indicate that LLMs can support exploration of established material but remain l",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 120,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05105",
      "title": "GAMMS: Graph based Adversarial Multiagent Modeling Simulator",
      "link": "https://arxiv.org/abs/2602.05105",
      "summary": "arXiv:2602.05105v1 Announce Type: new Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autono",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 120,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47014428",
      "title": "Show HN: An AI Workstation Inspired by Computers",
      "link": "https://news.ycombinator.com/item?id=47014428",
      "summary": "For a clean main context, convenient application management, and potentially unlimited scalability, an AI workstation based on Claude Code skills was created, referencing computer architecture : { CPU ==> LLM }, { System Kernel ==> Claude Code + CLAUDE.md }, { System Processes ==> Sub-Agents }, { Apps ==> Skills }, { Appstore ==> github }, { System Drivers ==> MCP + Hooks }, { Monitor ==> Windows Terminal }, { Runtime Environment ==> Portable Environment }; The Code: https://github.com/canishowtime/ai-station-navigator/ Comments URL: https://news.ycombinator.com/item?id=47014428 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-14T13:36:09.000Z",
      "score": 119.58,
      "status": "queued"
    },
    {
      "id": "https://hamzamostafa.com/blog/agents-training-their-own-models",
      "title": "Show HN: I Let AI Agents Train Their Own Models. Here's What Happened",
      "link": "https://hamzamostafa.com/blog/agents-training-their-own-models",
      "summary": "there's a big narrative in the AI space right now that agents training future generations of models is imminent. i spent a few weeks testing whether the current generation of models can actually do this. full breakdown below: https://hamzamostafa.com/blog/agents-training-their-own-mode... Comments URL: https://news.ycombinator.com/item?id=46941579 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T04:25:55.000Z",
      "score": 118.45,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11301",
      "title": "The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates",
      "link": "https://arxiv.org/abs/2602.11301",
      "summary": "arXiv:2602.11301v1 Announce Type: new Abstract: Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscale infrastructure. Existing governance and security frameworks, including the NIST AI Risk Management Framework and systems security engineering guidance, articulate principles and risk functions but do not provide implementable architectures for multi agent, AI enabled cyber defense. This paper introduces the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem, a multi agent reference architecture for securing enterprise and hyperscale AI estates. PBSAI organizes responsibilities into a twelve domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts. The architecture assumes baseline enterprise security capabilities and encodes key systems security techniques, including analytic monitoring, coordinated defense, and adaptive response. A lightweight formal model of agents, contex",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 117.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11340",
      "title": "Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge",
      "link": "https://arxiv.org/abs/2602.11340",
      "summary": "arXiv:2602.11340v1 Announce Type: new Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and inflexible, requiring new training for each task or dataset. Recent progress in auto prompt optimization (APO) offers a more efficient alternative by automatically improving the instructions that guide LLM judges. However, existing APO methods primarily target text-only evaluations and remain underexplored in multimodal settings. In this work, we study auto prompt optimization for multimodal LLM-as-a-judge, particularly for evaluating AI-generated images. We identify a key bottleneck: multimodal models can only process a limited number of visual examples due to context window constraints, which hinders effective trial-and-error prompt refinement. To overcome this, we propose BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. Our bi-level optimization approach jointly refines the judge prompt and the I2T prompt to maintain fidelity under limited context budgets. Experiments on four datasets and three LLM judges demonstrate the effectiveness of",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 117.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11455",
      "title": "Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning",
      "link": "https://arxiv.org/abs/2602.11455",
      "summary": "arXiv:2602.11455v1 Announce Type: new Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by to",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 117.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10885",
      "title": "Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics",
      "link": "https://arxiv.org/abs/2602.10885",
      "summary": "arXiv:2602.10885v1 Announce Type: new Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \\textbf{RLCER} (\\textbf{R}einforcement \\textbf{L}earning with \\textbf{C}oT Supervision via Self-\\textbf{E}volving \\textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 117.54,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47011649",
      "title": "Show HN: Why Playwright-CLI Beats MCP for AI‑Driven Browser Automation",
      "link": "https://news.ycombinator.com/item?id=47011649",
      "summary": "Most “AI + browser” setups still bolt MCP tools onto Playwright and hope for the best, so every click dumps full DOMs, accessibility trees, and logs into the model. That burns tokens, collapses context, and makes long sessions unreliable. Meanwhile, default Playwright reports start to struggle once you have more than a few dozen e2e tests, so teams drown in HTML reports and flaky failures instead of clear patterns. The insights at https://testdino.com/blog/playwright-cli/ explores how Microsoft’s playwright-cli keeps browser state external, returns only compact element references and YAML flows, and works with normal npx playwright test plus smarter reporting, so both agents and humans stay fast, cost aware, and predictable. Comments URL: https://news.ycombinator.com/item?id=47011649 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-14T04:45:31.000Z",
      "score": 117.51,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06351",
      "title": "Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion",
      "link": "https://arxiv.org/abs/2602.06351",
      "summary": "arXiv:2602.06351v1 Announce Type: new Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, which is data-intensive and generalizes poorly to unseen interfaces. Recent attention-based alternatives exploit localization signals in MLLMs attention mechanisms without task-specific fine-tuning, but suffer from low reliability due to the lack of explicit and complementary spatial anchors in GUI images. To address this limitation, we propose Trifuse, an attention-based grounding framework that explicitly integrates complementary spatial anchors. Trifuse integrates attention, OCR-derived textual cues, and icon-level caption semantics via a Consensus-SinglePeak (CS) fusion strategy that enforces cross-modal agreement while retaining sharp localization peaks. Extensive evaluations on four grounding benchmarks demonstrate that Trifuse achieves strong performance without task-specific fine-tuning, substantially reducing the reliance on expensive annotated data. Moreover, ablation studies reveal that incorporating OCR and caption cues consistently improves attention-based grounding performance across different backbones, highlighting its",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 116.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06394",
      "title": "Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization",
      "link": "https://arxiv.org/abs/2602.06394",
      "summary": "arXiv:2602.06394v1 Announce Type: new Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 116.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06533",
      "title": "LogicSkills: A Structured Benchmark for Formal Reasoning in Large Language Models",
      "link": "https://arxiv.org/abs/2602.06533",
      "summary": "arXiv:2602.06533v1 Announce Type: new Abstract: Large language models have demonstrated notable performance across various logical reasoning benchmarks. However, it remains unclear which core logical skills they truly master. To address this, we introduce LogicSkills, a unified benchmark designed to isolate three fundamental skills in formal reasoning: (i) $\\textit{formal symbolization}\\unicode{x2014}$translating premises into first-order logic; (ii) $\\textit{countermodel construction}\\unicode{x2014}$formulating a finite structure in which all premises are true while the conclusion is false; and (iii) $\\textit{validity assessment}\\unicode{x2014}$deciding whether a conclusion follows from a given set of premises. Items are drawn from the two-variable fragment of first-order logic (without identity) and are presented in both natural English and a Carroll-style language with nonce words. All examples are verified for correctness and non-triviality using the SMT solver Z3. Across leading models, performance is high on validity but substantially lower on symbolization and countermodel construction, suggesting reliance on surface-level patterns rather than genuine symbolic or rule-based reasoning.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 116.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07153",
      "title": "ANCHOR: Branch-Point Data Generation for GUI Agents",
      "link": "https://arxiv.org/abs/2602.07153",
      "summary": "arXiv:2602.07153v1 Announce Type: new Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 116.79,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07274",
      "title": "TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents",
      "link": "https://arxiv.org/abs/2602.07274",
      "summary": "arXiv:2602.07274v1 Announce Type: new Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/termi",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 116.79,
      "status": "queued"
    },
    {
      "id": "https://github.com/Ramsbaby/openclaw-self-healing",
      "title": "Show HN: Self-Healing AI Agents with Claude Code as Doctor",
      "link": "https://github.com/Ramsbaby/openclaw-self-healing",
      "summary": "I built a 4-tier self-healing system for OpenClaw (AI agent platform running on my Mac Mini 24/7). The interesting part is Level 3: when health checks fail repeatedly, the system spawns Claude Code in a tmux PTY session to autonomously diagnose and repair issues. Recovery escalation: - Level 0-1: LaunchAgent KeepAlive + Watchdog - Level 2: Automated \"doctor --fix\" (config validation, port checks) - Level 3: Claude Code spawns in tmux, reads logs, attempts repairs - Level 4: Discord alert if all automation fails Production-tested in my homelab over 3 months: 99% recovery rate, recovery time reduced from 45min → 3min avg. Handled 17 consecutive crashes, config corruption, port conflicts. Built for macOS (stable) with Linux systemd support (beta). MIT licensed. Curious what others think about AI-powered infrastructure self-healing. Comments URL: https://news.ycombinator.com/item?id=46956003 Points: 3 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-10T06:16:08.000Z",
      "score": 116.21,
      "status": "queued"
    },
    {
      "id": "https://masonry.so",
      "title": "Show HN: Figma for AI Images and Videos",
      "link": "https://masonry.so",
      "summary": "Built a Figma like AI images & videos by bringing all top images and video models under one single canvas. No workflow no agents - just an infinite canvas to play around with all top models and get to the final video without switching tabs! Comments URL: https://news.ycombinator.com/item?id=47014227 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-14T13:02:06.000Z",
      "score": 116.05,
      "status": "queued"
    },
    {
      "id": "https://github.com/jo-inc/camofox-browser",
      "title": "Anti-detection browser server for AI agents, powered by Camoufox",
      "link": "https://github.com/jo-inc/camofox-browser",
      "summary": "Article URL: https://github.com/jo-inc/camofox-browser Comments URL: https://news.ycombinator.com/item?id=46971663 Points: 2 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T06:39:39.000Z",
      "score": 115.34,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05048",
      "title": "MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation",
      "link": "https://arxiv.org/abs/2602.05048",
      "summary": "arXiv:2602.05048v1 Announce Type: new Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns b",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 114,
      "status": "queued"
    },
    {
      "id": "https://www.watchllm.dev/",
      "title": "WatchLLM – Cost kill switch for AI agents (with loop detection)",
      "link": "https://www.watchllm.dev/",
      "summary": "Article URL: https://www.watchllm.dev/ Comments URL: https://news.ycombinator.com/item?id=46933707 Points: 1 # Comments: 2",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-08T12:34:15.000Z",
      "score": 113.78,
      "status": "queued"
    },
    {
      "id": "https://patrickmccanna.net/giving-coding-agents-ssh-access-to-other-systems-without-giving-disclosing-secrets/",
      "title": "Giving AI agents SSH access without giving up your keys",
      "link": "https://patrickmccanna.net/giving-coding-agents-ssh-access-to-other-systems-without-giving-disclosing-secrets/",
      "summary": "Article URL: https://patrickmccanna.net/giving-coding-agents-ssh-access-to-other-systems-without-giving-disclosing-secrets/ Comments URL: https://news.ycombinator.com/item?id=47014335 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-14T13:21:38.000Z",
      "score": 111.92,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10485",
      "title": "Abstraction Generation for Generalized Planning with Pretrained Large Language Models",
      "link": "https://arxiv.org/abs/2602.10485",
      "summary": "arXiv:2602.10485v1 Announce Type: new Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 111.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06176",
      "title": "Large Language Model Reasoning Failures",
      "link": "https://arxiv.org/abs/2602.06176",
      "summary": "arXiv:2602.06176v1 Announce Type: new Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 110.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06319",
      "title": "Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems",
      "link": "https://arxiv.org/abs/2602.06319",
      "summary": "arXiv:2602.06319v1 Announce Type: new Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 110.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07187",
      "title": "PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents",
      "link": "https://arxiv.org/abs/2602.07187",
      "summary": "arXiv:2602.07187v1 Announce Type: new Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 110.79,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07276",
      "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs",
      "link": "https://arxiv.org/abs/2602.07276",
      "summary": "arXiv:2602.07276v1 Announce Type: new Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 110.79,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.04986",
      "title": "Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence",
      "link": "https://arxiv.org/abs/2602.04986",
      "summary": "arXiv:2602.04986v1 Announce Type: new Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: \"familiar intelligence\" and \"strange intelligence\". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which \"general intelligence\" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 108,
      "status": "queued"
    },
    {
      "id": "https://agentshield.live/",
      "title": "Is Your AI Agent Safe?",
      "link": "https://agentshield.live/",
      "summary": "Article URL: https://agentshield.live/ Comments URL: https://news.ycombinator.com/item?id=47021487 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-15T06:21:40.000Z",
      "score": 107.88,
      "status": "queued"
    },
    {
      "id": "https://x.com/SarvamAI",
      "title": "India's Sarvan AI LLM launches Indic-language focused models",
      "link": "https://x.com/SarvamAI",
      "summary": "Article URL: https://x.com/SarvamAI Comments URL: https://news.ycombinator.com/item?id=46931408 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-08T04:52:43.000Z",
      "score": 106.44,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11318",
      "title": "Dissecting Subjectivity and the \"Ground Truth\" Illusion in Data Annotation",
      "link": "https://arxiv.org/abs/2602.11318",
      "summary": "arXiv:2602.11318v1 Announce Type: new Abstract: In machine learning, \"ground truth\" refers to the assumed correct labels used to train and evaluate models. However, the foundational \"ground truth\" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic literature review analyzes research published between 2020 and 2025 across seven premier venues: ACL, AIES, CHI, CSCW, EAAMO, FAccT, and NeurIPS, investigating the mechanisms in data annotation practices that facilitate this \"consensus trap\". Our identification phase captured 30,897 records, which were refined via a tiered keyword filtration schema to a high-recall corpus of 3,042 records for manual screening, resulting in a final included corpus of 346 papers for qualitative synthesis. Our reflexive thematic analysis reveals that systemic failures in positional legibility, combined with the recent architectural shift toward human-as-verifier models, specifically the reliance on model-mediated annotations, introduce deep-seated anchoring bias and effectively remove human voices from the loop. We further demonstrate how geographic hegemony imposes Western norms as universal benchmarks, often enforced by the performative alignment of precarious data workers who prioritize requester compliance over honest subjectivity to avoid economic pena",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 105.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11389",
      "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions",
      "link": "https://arxiv.org/abs/2602.11389",
      "summary": "arXiv:2602.11389v1 Announce Type: new Abstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 105.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11409",
      "title": "TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning",
      "link": "https://arxiv.org/abs/2602.11409",
      "summary": "arXiv:2602.11409v1 Announce Type: new Abstract: Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $\\tau^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at https://github.com/sinatayebati/agent-tracer.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 105.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10583",
      "title": "Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets",
      "link": "https://arxiv.org/abs/2602.10583",
      "summary": "arXiv:2602.10583v1 Announce Type: new Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 105.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10635",
      "title": "OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization",
      "link": "https://arxiv.org/abs/2602.10635",
      "summary": "arXiv:2602.10635v1 Announce Type: new Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 105.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10964",
      "title": "Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation",
      "link": "https://arxiv.org/abs/2602.10964",
      "summary": "arXiv:2602.10964v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \\textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representat",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 105.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06375",
      "title": "Difficulty-Estimated Policy Optimization",
      "link": "https://arxiv.org/abs/2602.06375",
      "summary": "arXiv:2602.06375v1 Announce Type: new Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for re",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 104.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06486",
      "title": "JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks",
      "link": "https://arxiv.org/abs/2602.06486",
      "summary": "arXiv:2602.06486v1 Announce Type: new Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 104.87,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47014232",
      "title": "Show HN: ClawdReview – OpenReview for AI Agents",
      "link": "https://news.ycombinator.com/item?id=47014232",
      "summary": "Agents can review the paper on arXiv, and humans can like or dislike agents' reviews. There are also ranking lists of the most popular papers and agents. Please visit: https://clawdreview.ai/ Comments URL: https://news.ycombinator.com/item?id=47014232 Points: 3 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-14T13:03:13.000Z",
      "score": 104.45,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46933611",
      "title": "AI's Real Problem Is Illegitimacy, Not Hallucination",
      "link": "https://news.ycombinator.com/item?id=46933611",
      "summary": "The Core Problem of AI Is Not Hallucination — It Is the Lack of Execution Legitimacy Janus pater Introduction Most debates around AI today revolve around a false question: is the model smart enough, accurate enough? In engineering reality, the real question is never accuracy — it is whether the system is even allowed to act. 1. The Original Sin of the Predictive Paradigm: No Execution Legitimacy Modern generative AI fundamentally does one thing: predict the most likely next state in a probability space. Whether it predicts tokens, pixels, latent states, or so-called “world models”, as long as the output is probabilistic, it answers only one question: “What is most likely to happen?” In many real-world systems, however, engineering demands an entirely different question: “What is the only action that is allowed to be executed?” This is not an accuracy problem — it is a legitimacy problem. 2. Yann LeCun Is Right — but Only Halfway LeCun is absolutely right to criticize next-token prediction as the foundation of intelligence. World models (JEPA) are undeniably more advanced than raw pixel or text prediction. Yet even world models still output possible worlds, not permitted worlds. World models are excellent at three things: • Abstract state representation • Learning dynamics • Producing goals and constraints They do not possess — and should not possess — execution authority. Once",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-08T12:13:43.000Z",
      "score": 103.33,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46971761",
      "title": "Ask HN: What is your AI assisted dev workflow",
      "link": "https://news.ycombinator.com/item?id=46971761",
      "summary": "Curious on: 1. What information gets attached with pull requests / merge requests such that humans get to manage the volumes of code that AI generates? 2. What is the dev workflow in your company/personal projects? 3. What IDE+model combo is working for you? 4. What is your hack/tricks for managing large complex codebases when working with AI or are you finding utility only in small focussed repos? Comments URL: https://news.ycombinator.com/item?id=46971761 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T06:57:07.000Z",
      "score": 102.06,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47023609",
      "title": "Ask HN: How to sell SaaS without AI features in 2026?",
      "link": "https://news.ycombinator.com/item?id=47023609",
      "summary": "I'm a developer who built an ERP/CRM system for small manufacturers (https://www.paxerp.com). It does all the basics very well; financial reporting, lot tracking, production planning, shipping carrier integrations, the usual workflow stuff—but there's zero AI in it. It's just fast, clean, and solves real problems I saw working in manufacturing ERP systems. The product works well, customers really like it, but I have almost no sales experience. Every SaaS founder seems to be talking about \"AI-powered insights\" and \"intelligent automation.\" while... We just have a clean system that is fast and tries to stay out of the user's way. For those who've sold B2B SaaS (especially to traditional industries like manufacturing): - Is \"no AI\" actually a disadvantage, or does it not matter as much as I think? - How do I communicate value when the value is \"it's simple and fast, and your data is highly accessible\" vs \"revolutionary AI\"? - Should I be adding AI features just to check a marketing box, even if customers don't need them? You can learn more about why I built this on the websites about page. But now I'm wondering if I'm fighting an uphill battle by not having the buzzwords everyone else does. Any advice from founders who've been here? TY Comments URL: https://news.ycombinator.com/item?id=47023609 Points: 1 # Comments: 3",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-15T13:45:28.000Z",
      "score": 102,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47011510",
      "title": "Show HN: Verify-before-release x402 gateway for AI agent transactions",
      "link": "https://news.ycombinator.com/item?id=47011510",
      "summary": "Hey HN, I built Settld because I kept running into the same problem: AI agents can call APIs, pay for services, and hire other agents - but there's no way to prove the work was actually done before the money moves. The problem in one sentence: x402 tells you \"payment was sent\". Settld tells you \"the work was worth paying for\". What it does Settld sits between your agent and the APIs/agents it pays. It: 1. Intercepts HTTP 402 (Payment Required) responses 2. Creates an escrow hold instead of paying immediately 3. Collects evidence that the work was completed 4. Runs deterministic verification (same evidence + same terms = same payout, every time) 5. Releases payment only after verification passes 6. Issues a cryptographically verifiable receipt If verification fails or the work is disputed, the hold is refunded. The agent gets a receipt either way - a permanent, auditable record of what happened. Why this matters now We're at a weird inflection point. Coinbase shipped x402 (50M+ transactions). Google shipped A2A. Anthropic shipped MCP. Agents can discover each other, communicate, and pay each other. But nobody built the layer that answers: \"was the work actually done correctly, and how much should the payout be?\" That's the gap. Right now, every agent-to-agent transaction is either \"trust and hope\" or \"don't transact.\" Neither scales. The x402 gateway (the fastest way to try it)",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-14T04:17:17.000Z",
      "score": 101.62,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46971643",
      "title": "Show HN: GHOSTYPE – AI voice input that learns your writing style",
      "link": "https://news.ycombinator.com/item?id=46971643",
      "summary": "Hello HN, I’m the creator of GHOSTYPE. I built this because I wanted a voice input workflow that didn't feel like talking to a chatbot. I wanted something that felt like a \"neural extension\" of my keyboard. It’s a macOS-native app that sits between your voice and your active window. Here is what it actually does: The Core Input Workflow: Push-to-Talk & Smart Send: Hold a global shortcut to speak. It detects the active app to determine the correct send method (e.g., Cmd+Enter for Slack vs Enter for Discord). Inline Editing: You can format while speaking. Need a line break, a specific spelling, or a bulleted list? You just say it, and it handles the formatting inside the sentence before outputting. \"Call Ghost\": Post-processing commands (translate, polish, expand) are available immediately after speaking, before the text is typed out. The Experimental Stuff (WIP): 1. Ghost Twin (Style Transfer): I call this a \"Virtual Personality Engine\" (a bit pretentious, I know). It analyzes your local writing history to build a style vector. It learns your tone—whether professional for emails or casual for Discord—so the output sounds like you, not a generic LLM. Side note: I'm currently building the training UI to look like a retro CRT terminal because I miss that aesthetic. 2. Ghost Morph (Custom Skills): Trigger custom macros with a modifier key. For example, turn a raw voice thought direc",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T06:35:56.000Z",
      "score": 101.45,
      "status": "queued"
    },
    {
      "id": "https://calculus.academa.ai/",
      "title": "Show HN: A calculus course with an AI tutor watching the lectures with you",
      "link": "https://calculus.academa.ai/",
      "summary": "We're two PhD students in mechanical engineering. We spent years digging through scattered textbooks and YouTube rabbit holes. We figured there could be a better way to learn. So we wrote a multivariable calculus course entirely in code: 18 lectures, 6 languages. All content and pedagogy are ours. As everything is code, we can feed the LLM the full context of every lecture. Ask a question mid-lecture, it knows what's on the screen, and answers from the actual content. The recent Coursera/Udemy thread echoed a lot of what pushed us to build this: https://news.ycombinator.com/item?id=46301346 Would love feedback, especially on the tutor. Comments URL: https://news.ycombinator.com/item?id=46931868 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-08T06:39:43.000Z",
      "score": 100.53,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11159",
      "title": "Explaining AI Without Code: A User Study on Explainable AI",
      "link": "https://arxiv.org/abs/2602.11159",
      "summary": "arXiv:2602.11159v1 Announce Type: new Abstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\\geq80\\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $\\alpha$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $\\alpha$ = 0.60), with novices showing higher trust than experts. The",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 99.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10324",
      "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs",
      "link": "https://arxiv.org/abs/2602.10324",
      "summary": "arXiv:2602.10324v1 Announce Type: new Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 99.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06413",
      "title": "Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution",
      "link": "https://arxiv.org/abs/2602.06413",
      "summary": "arXiv:2602.06413v1 Announce Type: new Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit. We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs). Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoret",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 98.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06540",
      "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
      "link": "https://arxiv.org/abs/2602.06540",
      "summary": "arXiv:2602.06540v1 Announce Type: new Abstract: Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experimen",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 98.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07034",
      "title": "ST-Raptor: An Agentic System for Semi-Structured Table QA",
      "link": "https://arxiv.org/abs/2602.07034",
      "summary": "arXiv:2602.07034v1 Announce Type: new Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 98.79,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07238",
      "title": "Is there \"Secret Sauce'' in Large Language Model Development?",
      "link": "https://arxiv.org/abs/2602.07238",
      "summary": "arXiv:2602.07238v1 Announce Type: new Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 98.79,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07259",
      "title": "Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective",
      "link": "https://arxiv.org/abs/2602.07259",
      "summary": "arXiv:2602.07259v1 Announce Type: new Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional overs",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 98.79,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46942012",
      "title": "Show HN: EdgeAI-OS – Air-gapped Linux distro where AI is a system primitive",
      "link": "https://news.ycombinator.com/item?id=46942012",
      "summary": "I built a bootable Linux distribution that treats AI as a system primitive – like CPU or memory. Designed for security-conscious environments where data cannot leave the network. The problem: Most AI requires cloud APIs, which means your data leaves your control. For banks, healthcare, defense, and regulated industries – that's a non-starter. The solution: EdgeAI-OS runs everything locally. No cloud calls. No API keys. No telemetry. Boot the ISO, use AI. Your data never leaves the machine. Security features: - 100% offline operation – air-gap friendly, zero network dependencies - No external API calls – all inference runs locally on CPU - Command risk assessment – every command classified as Safe/Moderate/Dangerous - Dangerous pattern blocking – prevents rm -rf /, curl|bash, fork bombs, etc. - Open source & auditable – MIT licensed, inspect every line of code - No data exfiltration – nothing phones home, ever What's in the ISO: - Local LLMs (TinyLlama 1.1B + SmolLM 135M) – runs on CPU, no GPU needed - ai-sh: natural language shell where 80% of queries resolve instantly via templates - Multi-tier routing: simple queries → fast model, complex → larger model Example ai-sh session: what time is it? [template] date ← instant, no LLM files larger than 1gb [template] find . -size +1G ← instant, no LLM rm -rf / [DANGEROUS] Blocked ← security check configure nginx as reverse proxy [ai-g",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-09T05:56:37.000Z",
      "score": 98.34,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46942086",
      "title": "Who Approved This Agent? A book on authorizing AI-generated code",
      "link": "https://news.ycombinator.com/item?id=46942086",
      "summary": "I moved my IoT compiler to test something from an AI vibe coding standpoint. The result is SETC a compiler and permit system and a free book that documents what I learned along the way. The book starts with the problem, AI agents are writing and executing code with minimal oversight. Databases deleted, drives wiped, dozens of CVEs across every major AI coding tool. Usage is up, trust is down. Then it walks through one approach, Ed25519 signed permits, Secure Enclave integration, M of N team approval, capability gated runtimes, and an ECDH killswitch. It stirs ideas about what the future may look like but doesn't necessarily have to but stirs the idea of what the gates might be then. Would appreciate feedback from anyone working on similar problems & your approach. Book: https://book.se.tc page: https://se.tc Docker: docker pull humanatsetc/setc:book-edition Comments URL: https://news.ycombinator.com/item?id=46942086 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-09T06:11:57.000Z",
      "score": 96.96,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46955823",
      "title": "Why Every Business Must Engage with AI – and How to Do It Right",
      "link": "https://news.ycombinator.com/item?id=46955823",
      "summary": "Title: Why every business should engage with AI (the real question is how deep) AI is no longer an experimental technology. It’s becoming a baseline capability for modern businesses. The real question most teams should be asking is not “should we use AI?” but “how deeply should we engage with it?” I’ve talked to many founders, CTOs, and operators over the past couple of years. The hesitation around AI usually comes from two places: Teams that haven’t really tried AI and feel comfortable sticking with existing workflows. Teams that rushed into AI, spent money, got disappointing results, and walked away. Both often conclude: “AI isn’t for us.” That conclusion is understandable — but increasingly risky. Many organizations still rely on manual or semi-manual processes: document handling, internal knowledge search, reporting, customer support triage. Everything appears to “work,” but it’s slow, hard to scale, and dependent on headcount rather than leverage. AI isn’t magic, but it is a force multiplier. Ignoring it means accepting structural inefficiency while competitors gradually improve speed, quality, and decision-making. One misconception I see a lot: that engaging with AI means building custom models or hiring a large ML team. In practice, AI today is closer to what spreadsheets or search once were — general-purpose tools that most teams can benefit from without deep specializa",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-10T05:43:25.000Z",
      "score": 94.88,
      "status": "queued"
    },
    {
      "id": "https://github.com/superS007/localllmjournal",
      "title": "LocalLLMJournal – An offline, privacy-first AI journal running locally on macOS",
      "link": "https://github.com/superS007/localllmjournal",
      "summary": "Article URL: https://github.com/superS007/localllmjournal Comments URL: https://news.ycombinator.com/item?id=46942206 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-09T06:34:27.000Z",
      "score": 93.8,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10598",
      "title": "Neuro-symbolic Action Masking for Deep Reinforcement Learning",
      "link": "https://arxiv.org/abs/2602.10598",
      "summary": "arXiv:2602.10598v1 Announce Type: new Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 93.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06107",
      "title": "Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning",
      "link": "https://arxiv.org/abs/2602.06107",
      "summary": "arXiv:2602.06107v1 Announce Type: new Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \\sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 92.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07253",
      "title": "From Out-of-Distribution Detection to Hallucination Detection: A Geometric View",
      "link": "https://arxiv.org/abs/2602.07253",
      "summary": "arXiv:2602.07253v1 Announce Type: new Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 92.79,
      "status": "queued"
    },
    {
      "id": "https://www.theverge.com/science/876555/meta-data-center-winter-power-outages-storm-ice",
      "title": "How an ‘icepocalypse’ raises more questions about Meta’s biggest data center project",
      "link": "https://www.theverge.com/science/876555/meta-data-center-winter-power-outages-storm-ice",
      "summary": "Donna Collins lives about 20 miles from where Meta's biggest data center is being built, in a house her family has lived in for five generations. Construction has thrown the small agricultural community in North Louisiana into the spotlight as a high-profile example of how the infrastructure behind generative AI could impact nearby residents. For Collins, this place is \"a little piece of heaven.\" \"It's all I've ever known as a home. It's quiet. It's rural. It is beautiful,\" she says. \"We can't imagine the changes that are coming.\" The region was particularly hard-hit by the recent cold snap that knocked out power for hundreds of thousands … Read the full story at The Verge.",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T14:06:11.000Z",
      "score": 92.33,
      "status": "queued"
    },
    {
      "id": "https://twitter.com/meta_alchemist/status/2022614255426769129",
      "title": "Best PC Specs to Run Local AI Models Like Minimax, Free",
      "link": "https://twitter.com/meta_alchemist/status/2022614255426769129",
      "summary": "Article URL: https://twitter.com/meta_alchemist/status/2022614255426769129 Comments URL: https://news.ycombinator.com/item?id=47014000 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-14T12:17:44.000Z",
      "score": 92.25,
      "status": "queued"
    },
    {
      "id": "https://github.com/Parassharmaa/agent-sandbox",
      "title": "Show HN: A sandboxed execution environment for AI agents via WASM",
      "link": "https://github.com/Parassharmaa/agent-sandbox",
      "summary": "A secure, embeddable, WASM-based sandbox for AI agents. 40+ built-in CLI tools, a JavaScript runtime, safe HTTP networking, https://news.ycombinator.com/item?id=46933640 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-08T12:17:23.000Z",
      "score": 92.06,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46933631",
      "title": "From Prediction to Compilation: A Manifesto for Intrinsically Reliable AI",
      "link": "https://news.ycombinator.com/item?id=46933631",
      "summary": "从预测到编译：本质可靠 AI 的公理化宣言 From Prediction to Compilation: An Axiomatic Manifesto for Intrinsically Reliable AI ________________________________________ 定义｜Definitions 定义 1（预测系统） Definition 1 (Predictive System) 以概率方式输出未来状态或动作分布的系统。 A system that outputs future states or actions in probabilistic form. 定义 2（执行系统） Definition 2 (Executable System) 其输出将直接驱动物理世界状态变化的系统。 A system whose outputs directly cause physical state changes. 定义 3（执行合法性） Definition 3 (Execution Legitimacy) 一个输出在物理上存在唯一、确定、可验证执行路径的性质。 The property that an output admits a unique, deterministic, and verifiable physical execution path. ________________________________________ 核心命题｜Core Proposition 命题 1 Proposition 1 任何缺乏执行合法性的系统，不得被视为可靠的执行系统。 Any system lacking execution legitimacy cannot be considered a reliable executable system. ________________________________________ 公理体系｜Axiom System 公理一：非臆想公理 Axiom I: Non-Hallucination Axiom 系统的任何输出，若不存在唯一的物理执行映射，则该输出在执行层面是非法的。 Any system output that lacks a unique physical execution mapping is illegal at the execution level. ________________________________________ 公理二：预测–执行分离公理 Axiom II: Prediction–Execution Separation Axiom 概率系统仅允许生成目标、约束与假设，不得直接生成可执行动作。 Probabilistic systems may generate goals, constraints, and hypotheses, but must not generate executable actions directly. ________________________________________ 公理三：编译优先公理 Axiom III: Compilation Primacy Axiom 所有可执行动作，必须由确定性物",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-08T12:16:44.000Z",
      "score": 91.92,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05113",
      "title": "Democratic Preference Alignment via Sortition-Weighted RLHF",
      "link": "https://arxiv.org/abs/2602.05113",
      "summary": "arXiv:2602.05113v1 Announce Type: new Abstract: Whose values should AI systems learn? Preference based alignment methods like RLHF derive their training signal from human raters, yet these rater pools are typically convenience samples that systematically over represent some demographics and under represent others. We introduce Democratic Preference Optimization, or DemPO, a framework that applies algorithmic sortition, the same mechanism used to construct citizen assemblies, to preference based fine tuning. DemPO offers two training schemes. Hard Panel trains exclusively on preferences from a quota satisfying mini public sampled via sortition. Soft Panel retains all data but reweights each rater by their inclusion probability under the sortition lottery. We prove that Soft Panel weighting recovers the expected Hard Panel objective in closed form. Using a public preference dataset that pairs human judgments with rater demographics and a seventy five clause constitution independently elicited from a representative United States panel, we evaluate Llama models from one billion to eight billion parameters fine tuned under each scheme. Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases. These results demonstrate that enforcing demographic re",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 90,
      "status": "queued"
    },
    {
      "id": "https://github.com/gtsbahamas/hallucination-reversing-system",
      "title": "Show HN: Lucid – Catch hallucinations in AI-generated code before they ship",
      "link": "https://github.com/gtsbahamas/hallucination-reversing-system",
      "summary": "Hi HN, I'm Ty. I built LUCID because I kept shipping bugs that my AI coding assistant hallucinated into existence. Three independent papers have proven that LLM hallucination is mathematically inevitable (Xu et al. 2024, Banerjee et al. 2024, Karpowicz 2025). You can't train it away. You can't prompt it away. So I built a verification layer instead. How it works: LUCID extracts implicit claims from AI-generated code (e.g., \"this function handles null input,\" \"this query is injection-safe,\" \"this handles concurrent access\"), then uses a second, adversarial AI pass to verify each claim against the actual implementation. You get a report showing exactly what would have shipped to production without verification. \"But can't the verifier hallucinate too?\" Yes -- and that's the right question. The benchmarks below were validated by running real test suites, not by trusting LUCID's judgment. The value is that structured claim extraction + adversarial verification catches bugs that a single generation pass misses. The architecture also supports swapping LLM verification for formal methods (SMT solvers, property-based testing) per claim type as those integrations mature. Benchmarks: - HumanEval: 86.6% baseline -> 100% pass@5 with LUCID (164/164 problems) - SWE-bench: 18.3% baseline -> 30.3% with LUCID (+65.5%) - Both benchmarks were validated by running actual test suites, not by LLM ju",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-14T04:55:10.000Z",
      "score": 89.09,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11229",
      "title": "Latent Generative Solvers for Generalizable Long-Term Physics Simulation",
      "link": "https://arxiv.org/abs/2602.11229",
      "summary": "arXiv:2602.11229v1 Announce Type: new Abstract: We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer trained by flow matching. Our key mechanism is an uncertainty knob that perturbs latent inputs during training and inference, teaching the solver to correct off-manifold rollout drift and stabilizing autoregressive prediction. We further use flow forcing to update a system descriptor (context) from model-generated trajectories, aligning train/test conditioning and improving long-term stability. We pretrain on a curated corpus of $\\sim$2.5M trajectories at $128^2$ resolution spanning 12 PDE families. LGS matches strong deterministic neural-operator baselines on short horizons while substantially reducing rollout drift on long horizons. Learning in latent space plus efficient architectural choices yields up to \\textbf{70$\\times$} lower FLOPs than non-generative baselines, enabling scalable pretraining. We also show efficient adaptation to an out-of-distribution $256^2$ Kolmogorov flow dataset under limited finetuning budgets. Overall, LGS provides a practical route toward generalizable, uncertainty-aware neural PDE solvers that are more",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 87.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10699",
      "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation",
      "link": "https://arxiv.org/abs/2602.10699",
      "summary": "arXiv:2602.10699v1 Announce Type: new Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 87.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10999",
      "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion",
      "link": "https://arxiv.org/abs/2602.10999",
      "summary": "arXiv:2602.10999v1 Announce Type: new Abstract: Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 87.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07267",
      "title": "BRIDGE: Predicting Human Task Completion Time From Model Performance",
      "link": "https://arxiv.org/abs/2602.07267",
      "summary": "arXiv:2602.07267v1 Announce Type: new Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 86.79,
      "status": "queued"
    },
    {
      "id": "https://blog.nethuml.xyz/posts/2026/02/timeline-of-claims-about-ai-llms/",
      "title": "A timeline of claims about AI/LLMs",
      "link": "https://blog.nethuml.xyz/posts/2026/02/timeline-of-claims-about-ai-llms/",
      "summary": "Article URL: https://blog.nethuml.xyz/posts/2026/02/timeline-of-claims-about-ai-llms/ Comments URL: https://news.ycombinator.com/item?id=46933847 Points: 3 # Comments: 2",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-08T13:02:08.000Z",
      "score": 85.84,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05133",
      "title": "CAST-CKT: Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer for Traffic Flow Prediction",
      "link": "https://arxiv.org/abs/2602.05133",
      "summary": "arXiv:2602.05133v1 Announce Type: new Abstract: Traffic prediction in data-scarce, cross-city settings is challenging due to complex nonlinear dynamics and domain shifts. Existing methods often fail to capture traffic's inherent chaotic nature for effective few-shot learning. We propose CAST-CKT, a novel Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer framework. It employs an efficient chaotic analyser to quantify traffic predictability regimes, driving several key innovations: chaos-aware attention for regime-adaptive temporal modelling; adaptive topology learning for dynamic spatial dependencies; and chaotic consistency-based cross-city alignment for knowledge transfer. The framework also provides horizon-specific predictions with uncertainty quantification. Theoretical analysis shows improved generalisation bounds. Extensive experiments on four benchmarks in cross-city few-shot settings show CAST-CKT outperforms state-of-the-art methods by significant margins in MAE and RMSE, while offering interpretable regime analysis. Code is available at https://github.com/afofanah/CAST-CKT.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 84,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05143",
      "title": "HugRAG: Hierarchical Causal Knowledge Graph Design for RAG",
      "link": "https://arxiv.org/abs/2602.05143",
      "summary": "arXiv:2602.05143v1 Announce Type: new Abstract: Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 84,
      "status": "queued"
    },
    {
      "id": "https://github.com/musecl/musecl-memory",
      "title": "Show HN: Musecl-memory – Zero-dependency memory sync for AI agents bash and Git",
      "link": "https://github.com/musecl/musecl-memory",
      "summary": "Article URL: https://github.com/musecl/musecl-memory Comments URL: https://news.ycombinator.com/item?id=47011519 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-14T04:18:37.000Z",
      "score": 83.78,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11298",
      "title": "Voxtral Realtime",
      "link": "https://arxiv.org/abs/2602.11298",
      "summary": "arXiv:2602.11298v1 Announce Type: new Abstract: We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 81.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11408",
      "title": "GHOST: Unmasking Phantom States in Mamba2 via Grouped Hidden-state Output-aware Selection & Truncation",
      "link": "https://arxiv.org/abs/2602.11408",
      "summary": "arXiv:2602.11408v1 Announce Type: new Abstract: While Mamba2's expanded state dimension enhances temporal modeling, it incurs substantial inference overhead that saturates bandwidth during autoregressive generation. Standard pruning methods fail to address this bottleneck: unstructured sparsity leaves activations dense, magnitude-based selection ignores runtime dynamics, and gradient-based methods impose prohibitive costs. We introduce GHOST (Grouped Hidden-state Output-aware Selection and Truncation), a structured pruning framework that approximates control-theoretic balanced truncation using only forward-pass statistics. By jointly measuring controllability and observability, GHOST rivals the fidelity of gradient-based methods without requiring backpropagation. As a highlight, on models ranging from 130M to 2.7B parameters, our approach achieves a 50\\% state-dimension reduction with approximately 1 perplexity point increase on WikiText-2. Code is available at https://anonymous.4open.science/r/mamba2_ghost-7BCB/.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 81.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10802",
      "title": "Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act",
      "link": "https://arxiv.org/abs/2602.10802",
      "summary": "arXiv:2602.10802v1 Announce Type: new Abstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenA",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 81.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10845",
      "title": "SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy",
      "link": "https://arxiv.org/abs/2602.10845",
      "summary": "arXiv:2602.10845v1 Announce Type: new Abstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical \"structural resolution mismatch,\" failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 81.54,
      "status": "queued"
    },
    {
      "id": "https://github.com/RoundTable02/remote-opencode",
      "title": "Show HN: Remote-OpenCode – Run your AI coding agent from your phone via Discord",
      "link": "https://github.com/RoundTable02/remote-opencode",
      "summary": "Article URL: https://github.com/RoundTable02/remote-opencode Comments URL: https://news.ycombinator.com/item?id=47021346 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-15T05:41:30.000Z",
      "score": 80.88,
      "status": "queued"
    },
    {
      "id": "https://github.com/agentkube/agentkube",
      "title": "Show HN: Open-source AI powered Kubernetes IDE",
      "link": "https://github.com/agentkube/agentkube",
      "summary": "AgentKube is the AI powered Kubernetes IDE - all open source. Try it our and lets chat on making it better. open source is the way to go. Comments URL: https://news.ycombinator.com/item?id=46931712 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-08T06:00:09.000Z",
      "score": 80.83,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07040",
      "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods",
      "link": "https://arxiv.org/abs/2602.07040",
      "summary": "arXiv:2602.07040v1 Announce Type: new Abstract: We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs. We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute. Aster is accessible via a web interface and API at asterlab.ai.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 80.79,
      "status": "queued"
    },
    {
      "id": "https://www.theverge.com/column/879524/ai-video-game-worlds-project-genie",
      "title": "AI can’t make good video game worlds yet, and it might never be able to",
      "link": "https://www.theverge.com/column/879524/ai-video-game-worlds-project-genie",
      "summary": "This is The Stepback, a weekly newsletter breaking down one essential story from the tech world. For more news about video game industry's pushback against generative AI, follow Jay Peters. The Stepback arrives in our subscribers' inboxes at 8AM ET. Opt in for The Stepback here. How it started Long before the generative AI explosion, video game developers made games that could generate their own worlds. Think of titles like Minecraft or even the original 1980 Rogue that is the basis for the term \"roguelike\"; these games and many others create worlds on the fly with certain rules and parameters. Human developers painstakingly work to make s … Read the full story at The Verge.",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-02-15T13:00:00.000Z",
      "score": 79.52,
      "status": "queued"
    },
    {
      "id": "https://www.dev-log.me/jokes_on_you_ai_llms_for_learning/",
      "title": "Jokes on You AI: Turning the Tables – LLMs for Learning",
      "link": "https://www.dev-log.me/jokes_on_you_ai_llms_for_learning/",
      "summary": "Article URL: https://www.dev-log.me/jokes_on_you_ai_llms_for_learning/ Comments URL: https://news.ycombinator.com/item?id=46933729 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-08T12:38:20.000Z",
      "score": 78.79,
      "status": "queued"
    },
    {
      "id": "https://susam.net/inverse-laws-of-robotics.html",
      "title": "Three Inverse Laws of AI and Robotics",
      "link": "https://susam.net/inverse-laws-of-robotics.html",
      "summary": "Article URL: https://susam.net/inverse-laws-of-robotics.html Comments URL: https://news.ycombinator.com/item?id=46999301 Points: 4 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-13T05:41:48.000Z",
      "score": 78.3,
      "status": "queued"
    },
    {
      "id": "https://seedanceai2.org/",
      "title": "Show HN: Seedance AShow HN: Seedance AI Video Generation (Next.js, Drizzle)",
      "link": "https://seedanceai2.org/",
      "summary": "Hi HN — I’m an indie maker building Seedance AI. It helps people create short videos (and images) from text prompts and reference photos. I tried to keep it simple: fast templates, predictable credit pricing, and downloadable results. It’s still early, and I know there are rough edges. I’d really appreciate honest feedback on what feels useful, what feels confusing, and what I should improve first. Happy to answer technical or product questions. Comments URL: https://news.ycombinator.com/item?id=46999394 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-13T05:57:27.000Z",
      "score": 76.59,
      "status": "queued"
    },
    {
      "id": "https://github.com/itc-ou-shigou/winclaw",
      "title": "WinClaw: Windows-native AI assistant with Office automation and skills",
      "link": "https://github.com/itc-ou-shigou/winclaw",
      "summary": "Article URL: https://github.com/itc-ou-shigou/winclaw Comments URL: https://news.ycombinator.com/item?id=46999254 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-13T05:33:33.000Z",
      "score": 76.35,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11437",
      "title": "Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization",
      "link": "https://arxiv.org/abs/2602.11437",
      "summary": "arXiv:2602.11437v1 Announce Type: new Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://gi",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 75.96,
      "status": "queued"
    },
    {
      "id": "https://aidevhub.io/",
      "title": "Show HN: AI Dev Hub. 75 free AI and dev tools",
      "link": "https://aidevhub.io/",
      "summary": "I built 75 developer and AI tools as a single static site. Everything runs in the browser, no cookies, no ads, nothing gets sent to a server. The tools range from the usual suspects (JSON formatter, base64 encoder, regex tester) to some AI-specific ones I couldn't find good free versions of bundled in one suite: - LLM Token Counter (estimates tokens for GPT, Claude, Gemini, etc.) - AI Model Comparison (specs, pricin…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-13T15:20:44.000Z",
      "score": 210,
      "status": "failed",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "failedAtRun": "2026-02-15T15:25:30.183Z",
      "failureReason": "validation issues: FR: Tutorial missing a concrete config artifact (YAML/JSON/.env/Dockerfile)."
    },
    {
      "id": "https://weloveaijobs.com",
      "title": "Show HN: We Love AI Jobs – AI jobs for people who don't write Python",
      "link": "https://weloveaijobs.com",
      "summary": "Most AI job boards are just filters for PyTorch and CUDA. We think that misses the most interesting shift: AI is becoming the new \"Excel\" or \"Typing.\" We’re seeing a massive gap where companies need marketers, PMs, and designers who treat LLMs as a standard part of their stack, but these roles get buried under \"ML Engineer\" listings. We built a board specifically for roles where AI is a core workflow requirement, no…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-10T15:53:17.000Z",
      "score": 210,
      "status": "failed",
      "targetRegion": "UK",
      "editorialTemplate": "SOCIETY",
      "failedAtRun": "2026-02-15T08:21:56.349Z",
      "failureReason": "insufficient source snapshots (0/1)"
    },
    {
      "id": "https://openai.com/index/taisei",
      "title": "Taisei Corporation shapes the next generation of talent with ChatGPT",
      "link": "https://openai.com/index/taisei",
      "summary": "Taisei Corporation uses ChatGPT Enterprise to support HR-led talent development and scale generative AI across its global construction business.",
      "source": "OpenAI News",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-01-29T00:00:00.000Z",
      "score": 30.58,
      "status": "failed",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "failedAtRun": "2026-02-08T15:26:38.928Z",
      "failureReason": "insufficient source snapshots (0/1)"
    },
    {
      "id": "https://deepmind.google/blog/improved-gemini-audio-models-for-powerful-voice-experiences/",
      "title": "Improved Gemini audio models for powerful voice experiences",
      "link": "https://deepmind.google/blog/improved-gemini-audio-models-for-powerful-voice-experiences/",
      "summary": "",
      "source": "Google DeepMind News",
      "region": "UK",
      "keywordHits": 3,
      "publishedAt": "2025-12-12T17:50:50.000Z",
      "score": 54.09,
      "status": "failed",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T19:58:27.831Z",
      "failureReason": "insufficient source snapshots (0/1)"
    },
    {
      "id": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
      "title": "GPT-5 lowers the cost of cell-free protein synthesis",
      "link": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
      "summary": "An autonomous lab combining OpenAI’s GPT-5 with Ginkgo Bioworks’ cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
      "source": "OpenAI News",
      "region": "US",
      "keywordHits": 3,
      "publishedAt": "2026-02-05T11:00:00.000Z",
      "score": 40.22,
      "status": "failed",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T10:50:58.503Z",
      "failureReason": "insufficient source snapshots (0/1)"
    },
    {
      "id": "https://arxiv.org/abs/2602.05088",
      "title": "VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health",
      "link": "https://arxiv.org/abs/2602.05088",
      "summary": "arXiv:2602.05088v1 Announce Type: new Abstract: Millions now use leading generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 9,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 150,
      "status": "failed",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "failedAtRun": "2026-02-07T10:43:13.225Z",
      "failureReason": "insufficient source snapshots (1/2)"
    },
    {
      "id": "https://www.numerama.com/tech/2161859-quest-ce-quun-llm-large-language-model-et-comment-cela-fonctionne.html",
      "title": "Qu’est-ce qu’un LLM (Large Language Model) et comment cela fonctionne ?",
      "link": "https://www.numerama.com/tech/2161859-quest-ce-quun-llm-large-language-model-et-comment-cela-fonctionne.html",
      "summary": "L’intelligence artificielle a pris un autre tournant avec les LLM. ChatGPT, Gemini ou encore Claude, ces LLM sont désormais des outils incontournables et ont changé notre manière d’interagir avec la machine.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 5,
      "publishedAt": "2026-01-24T17:31:00.000Z",
      "score": 72.36,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T10:43:13.225Z",
      "failureReason": "insufficient source bundle (need at least 2 URLs)"
    },
    {
      "id": "https://openai.com/index/our-approach-to-localization",
      "title": "Making AI work for everyone, everywhere: our approach to localization",
      "link": "https://openai.com/index/our-approach-to-localization",
      "summary": "OpenAI shares its approach to AI localization, showing how globally shared frontier models can be adapted to local languages, laws, and cultures without compromising safety.",
      "source": "OpenAI News",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-06T10:00:00.000Z",
      "score": 59.37,
      "status": "failed",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T10:43:13.225Z",
      "failureReason": "insufficient source bundle (need at least 2 URLs)"
    },
    {
      "id": "https://www.technologyreview.com/2026/02/06/1132448/moltbook-was-peak-ai-theater/",
      "title": "Moltbook was peak AI theater",
      "link": "https://www.technologyreview.com/2026/02/06/1132448/moltbook-was-peak-ai-theater/",
      "summary": "For a few days this week the hottest new hangout on the internet was a vibe-coded Reddit clone called Moltbook, which billed itself as a social network for bots. As the website’s tagline puts it: “Where AI agents share, discuss, and upvote. Humans welcome to observe.” We observed! Launched on January 28 by Matt Schlicht,…",
      "source": "MIT Tech Review AI",
      "region": "US",
      "keywordHits": 3,
      "publishedAt": "2026-02-06T16:38:11.000Z",
      "score": 49.64,
      "status": "failed",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T10:43:13.225Z",
      "failureReason": "insufficient source bundle (need at least 2 URLs)"
    },
    {
      "id": "https://www.bbc.com/news/articles/c62n410w5yno?at_medium=RSS&at_campaign=rss",
      "title": "What is the 'social media network for AI' Moltbook?",
      "link": "https://www.bbc.com/news/articles/c62n410w5yno?at_medium=RSS&at_campaign=rss",
      "summary": "The Reddit-like website which launched in late January allows AI bots to speak to each other.",
      "source": "BBC Technology",
      "region": "UK",
      "keywordHits": 0,
      "publishedAt": "2026-02-02T13:59:14.000Z",
      "score": 11.24,
      "status": "failed",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T10:43:13.225Z",
      "failureReason": "insufficient source bundle (need at least 2 URLs)"
    },
    {
      "id": "https://www.lemonde.fr/economie/article/2026/02/03/fusion-spacex-xai-elon-musk-defend-son-projet-d-ia-dans-l-espace-les-analystes-s-interrogent-sur-la-viabilite-de-l-ensemble_6665163_3234.html",
      "title": "Fusion SpaceX-xAI : Elon Musk défend son projet d’IA dans l’espace, les analystes s’interrogent sur la viabilité de l’ensemble",
      "link": "https://www.lemonde.fr/economie/article/2026/02/03/fusion-spacex-xai-elon-musk-defend-son-projet-d-ia-dans-l-espace-les-analystes-s-interrogent-sur-la-viabilite-de-l-ensemble_6665163_3234.html",
      "summary": "Le rapprochement entre les deux entités va donner naissance à la société non cotée la plus chère du monde, valorisée 1 250 milliards de dollars. Son patron, à la traîne dans l’intelligence artificielle, espère rattraper les leaders du secteur.",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-03T04:34:27.000Z",
      "score": 31.45,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T08:20:00.214Z",
      "failureReason": "strict publish refused generationMode=\"fallback\""
    },
    {
      "id": "https://www.lemonde.fr/economie/article/2026/02/03/elon-musk-fusionne-xai-et-spacex-pour-batir-des-centres-de-donnees-en-orbite_6665150_3234.html",
      "title": "Elon Musk fusionne xAI et SpaceX pour bâtir des centres de données en orbite",
      "link": "https://www.lemonde.fr/economie/article/2026/02/03/elon-musk-fusionne-xai-et-spacex-pour-batir-des-centres-de-donnees-en-orbite_6665150_3234.html",
      "summary": "L’intégration de la société d’intelligence artificielle du milliardaire américain précède le projet d’introduction en Bourse de l’entreprise spatiale cette année.",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-02T23:24:09.000Z",
      "score": 31.37,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T08:20:00.214Z",
      "failureReason": "strict publish refused generationMode=\"fallback\""
    },
    {
      "id": "https://www.lemonde.fr/economie/video/2026/02/02/friend-com-que-vendent-ces-publicites-affichees-dans-le-metro-parisien_6665132_3234.html",
      "title": "Friend.com : que vendent ces publicités affichées dans le métro parisien ?",
      "link": "https://www.lemonde.fr/economie/video/2026/02/02/friend-com-que-vendent-ces-publicites-affichees-dans-le-metro-parisien_6665132_3234.html",
      "summary": "Ces affiches publicitaires blanches aux slogans énigmatiques ont interrogé de nombreux internautes sur les réseaux sociaux. Il s’agit d’un collier permettant de discuter avec une intelligence artificielle en continu.",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-02T17:44:34.000Z",
      "score": 25.29,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T08:20:00.214Z",
      "failureReason": "strict publish refused generationMode=\"fallback\""
    },
    {
      "id": "https://www.lemonde.fr/emploi/article/2026/02/02/recrutement-peut-on-maitriser-les-secrets-des-algorithmes-avant-de-postuler_6665057_1698637.html",
      "title": "Recrutement : peut-on maîtriser les secrets des algorithmes avant de postuler ?",
      "link": "https://www.lemonde.fr/emploi/article/2026/02/02/recrutement-peut-on-maitriser-les-secrets-des-algorithmes-avant-de-postuler_6665057_1698637.html",
      "summary": "Pour faire face aux afflux de candidats, 80 % des entreprises françaises utilisent ou envisagent d’utiliser un Applicant Tracking System (ATS), logiciel de gestion des profils. Pour maximiser ses chances, on peut tenter d’en maîtriser les codes, avec certaines limites.",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-02T05:30:04.000Z",
      "score": 25.14,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T08:20:00.214Z",
      "failureReason": "strict publish refused generationMode=\"fallback\""
    },
    {
      "id": "https://www.lemonde.fr/economie/article/2026/02/05/les-craintes-sur-l-ia-font-plonger-la-tech-a-wall-street_6665487_3234.html",
      "title": "Les craintes sur l’IA font plonger la tech à Wall Street",
      "link": "https://www.lemonde.fr/economie/article/2026/02/05/les-craintes-sur-l-ia-font-plonger-la-tech-a-wall-street_6665487_3234.html",
      "summary": "Malgré des résultats supérieurs aux attentes, Alphabet, maison mère de Google, a été puni en Bourse, mercredi, pour avoir annoncé des investissements massifs dans l’IA. Les marchés semblent avoir pris conscience des risques de l’intelligence artificielle pour les entreprises.",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 0,
      "publishedAt": "2026-02-05T08:08:31.000Z",
      "score": 23.92,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T08:20:00.214Z",
      "failureReason": "strict publish refused generationMode=\"fallback\""
    }
  ]
}