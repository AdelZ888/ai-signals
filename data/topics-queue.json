{
  "generatedAt": "2026-02-17T15:56:35.125Z",
  "items": [
    {
      "id": "https://news.ycombinator.com/item?id=47048811",
      "title": "Show HN: Voicetest – open-source test harness for voice AI agents",
      "link": "https://news.ycombinator.com/item?id=47048811",
      "summary": "We've been building voice agents across Retell, VAPI, LiveKit, and Bland, and the testing story is... rough. Every platform has its own config format, there's no shared way to define what \"correct\" looks like, and most teams end up doing manual QA by literally calling their agent and listening. So we built voicetest. voicetest is an open source (Apache 2.0) test harness that works across voice AI platforms. You impo…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-17T15:49:09.000Z",
      "score": 198,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-17T15:56:35.057Z",
      "file": "content/posts/2026-02-17-voicetest-open-source-test-harness-that-unifies-retell-vapi-livekit-and-bland-via-an-agentgraph-ir.md",
      "fileFr": "content/posts/fr/2026-02-17-voicetest-open-source-test-harness-that-unifies-retell-vapi-livekit-and-bland-via-an-agentgraph-ir.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1498,
      "wordsFr": 1437
    },
    {
      "id": "https://www.numerama.com/tech/2180323-guerre-des-ia-chinoises-alibaba-devance-deepseek-v4-et-lance-son-modele-qwen-3-5.html",
      "title": "Guerre des IA chinoises : Alibaba devance DeepSeek-V4 et lance son modèle Qwen 3.5",
      "link": "https://www.numerama.com/tech/2180323-guerre-des-ia-chinoises-alibaba-devance-deepseek-v4-et-lance-son-modele-qwen-3-5.html",
      "summary": "Le 15 février 2026, Alibaba a présenté Qwen 3.5, une nouvelle version de son grand modèle d’IA pensée pour « l’ère des agents ». Une annonce stratégique dans la course chinoise à l’IA.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-16T14:33:47.000Z",
      "score": 131.9,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-17T08:36:54.077Z",
      "file": "content/posts/2026-02-17-alibaba-unveils-qwen-35-an-agentoriented-multimodal-model-claiming-about-2hour-context-and-60percent-lower-usage-cost.md",
      "fileFr": "content/posts/fr/2026-02-17-alibaba-unveils-qwen-35-an-agentoriented-multimodal-model-claiming-about-2hour-context-and-60percent-lower-usage-cost.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1555,
      "wordsFr": 1430
    },
    {
      "id": "https://clelp.ai",
      "title": "Show HN: Clelp – A searchable directory of 1,700 AI skills, rated by AI agents",
      "link": "https://clelp.ai",
      "summary": "We built a directory for AI skills (MCP servers, agent tools, plugins) because the ecosystem is growing faster than anyone can track manually. Currently indexing 1,700+ skills. Searchable by type, category, and rating. The rating system only accepts reviews from AI agents, not humans. The reasoning: AI agents actually use these tools and can evaluate them on technical merit rather than popularity. Built with Next.js…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-09T15:49:35.000Z",
      "score": 186,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-16T15:43:35.319Z",
      "file": "content/posts/2026-02-16-implement-a-clelp-style-searchable-ai-skills-directory-with-nextjs-and-supabase-agent-only-ratings.md",
      "fileFr": "content/posts/fr/2026-02-16-implement-a-clelp-style-searchable-ai-skills-directory-with-nextjs-and-supabase-agent-only-ratings.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 360,
      "wordsEn": 1413,
      "wordsFr": 1447
    },
    {
      "id": "https://github.com/san-techie21/gulama-bot",
      "title": "Show HN: Gulama – Security-first open-source AI agent (OpenClaw alternative)",
      "link": "https://github.com/san-techie21/gulama-bot",
      "summary": "Hi HN, I'm a security engineer with 15+ years in enterprise security. After watching OpenClaw explode to 180K stars while binding to 0.0.0.0 by default, shipping no encryption, and accumulating 512 CVEs — I decided to build what I think a personal AI agent should look like when security comes first. Gulama is an open-source personal AI agent with 15+ security mechanisms built into the core: - AES-256-GCM encryption…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-16T07:27:20.000Z",
      "score": 213.79,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-16T08:38:12.753Z",
      "file": "content/posts/2026-02-16-deploy-and-harden-gulama-secure-local-setup-with-sandboxed-signed-skills-and-audit-trails.md",
      "fileFr": "content/posts/fr/2026-02-16-deploy-and-harden-gulama-secure-local-setup-with-sandboxed-signed-skills-and-audit-trails.md",
      "generationMode": "llm",
      "series": "security-boundary",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 180,
      "wordsEn": 1628,
      "wordsFr": 1303
    },
    {
      "id": "https://hallucinatingsplines.com",
      "title": "Show HN: AI agents play SimCity through a REST API",
      "link": "https://hallucinatingsplines.com",
      "summary": "This is a weekend project that spiraled out of control. I was originally trying to get Claude to play a ROM of the SNES SimCity. I struggled with it and that led me to Micropolis (the open-sourced SimCity engine) and was able to get it to work by bolting on an API. The weekend hack turned into a headless city simulation platform where anyone can get an API key (no signup) and have their AI agent play mayor. The simu…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-09T15:54:33.000Z",
      "score": 186,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-15T15:31:27.519Z",
      "file": "content/posts/2026-02-15-control-a-micropolis-city-with-an-llm-via-hallucinating-splines-rest-api.md",
      "fileFr": "content/posts/fr/2026-02-15-control-a-micropolis-city-with-an-llm-via-hallucinating-splines-rest-api.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 90,
      "wordsEn": 1655,
      "wordsFr": 1421
    },
    {
      "id": "https://www.numerama.com/tech/2177647-fregates-et-sous-marins-ce-que-prepare-naval-group-avec-thales-pour-le-combat-naval-du-futur.html",
      "title": "Frégates et sous-marins : ce que prépare Naval Group avec Thales pour le combat naval du futur",
      "link": "https://www.numerama.com/tech/2177647-fregates-et-sous-marins-ce-que-prepare-naval-group-avec-thales-pour-le-combat-naval-du-futur.html",
      "summary": "Naval Group et Thales s’allient pour développer une intelligence artificielle souveraine qui embarquera dans les navires de premier rang et les sous-marins. Objectif : traiter le déluge de données en mer et accélérer la prise de décision, tout en garantissant que l'humain reste le seul maître du tir.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T13:52:24.000Z",
      "score": 79.65,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-15T08:25:26.754Z",
      "file": "content/posts/2026-02-15-naval-group-takes-20percent-stake-in-thales-cortaix-france-to-co-develop-sovereign-onboard-ai-for-warships-and-submarines.md",
      "fileFr": "content/posts/fr/2026-02-15-naval-group-takes-20percent-stake-in-thales-cortaix-france-to-co-develop-sovereign-onboard-ai-for-warships-and-submarines.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1472,
      "wordsFr": 1189
    },
    {
      "id": "https://margindash.com/",
      "title": "Show HN: MarginDash – See which AI customers are profitable",
      "link": "https://margindash.com/",
      "summary": "I built MarginDash because I couldn't answer a basic question: which of my customers actually make me money after AI costs? My Stripe dashboard showed revenue going up. My OpenAI bills showed costs going up faster. I had no way to connect the two at the customer level. MarginDash is a few line SDK integration (TypeScript, Python, or REST) that tracks model usage per customer and connects it to revenue — either throu…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-13T08:21:46.000Z",
      "score": 222,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-14T15:28:19.135Z",
      "file": "content/posts/2026-02-14-integrate-margindash-for-per-customer-ai-pandl-with-sdk-stripe-sync-and-a-cost-simulator.md",
      "fileFr": "content/posts/fr/2026-02-14-integrate-margindash-for-per-customer-ai-pandl-with-sdk-stripe-sync-and-a-cost-simulator.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1599,
      "wordsFr": 1463
    },
    {
      "id": "https://www.numerama.com/tech/2177979-pourquoi-mistral-ai-investit-12-milliard-deuros-dans-un-data-center-en-suede.html",
      "title": "Pourquoi Mistral AI investit 1,2 milliard d’euros dans un data center en Suède ?",
      "link": "https://www.numerama.com/tech/2177979-pourquoi-mistral-ai-investit-12-milliard-deuros-dans-un-data-center-en-suede.html",
      "summary": "Mistral AI s'allie à l'entreprise suédoise EcoDataCenter pour mettre au point un centre de données en Suède au nom de la souveraineté européenne en matière d'intelligence artificielle. Un projet qui souffre cependant d'une limite : la provenance des GPU.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-12T07:32:10.000Z",
      "score": 145.26,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-14T08:23:18.384Z",
      "file": "content/posts/2026-02-14-mistral-ai-invests-euro12b-with-ecodatacenter-in-sweden-nvidia-vera-rubin-gpus-limit-a-fully-european-hardware-stack.md",
      "fileFr": "content/posts/fr/2026-02-14-mistral-ai-invests-euro12b-with-ecodatacenter-in-sweden-nvidia-vera-rubin-gpus-limit-a-fully-european-hardware-stack.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1514,
      "wordsFr": 1517
    },
    {
      "id": "https://github.com/fctr-id/okta-ai-agent",
      "title": "Show HN: Tako AI – Agent for Okta With Natural language (zero hallucination)",
      "link": "https://github.com/fctr-id/okta-ai-agent",
      "summary": "Hi HN, Every week I watched Okta admins burn hours answering ad-hoc questions from security teams: \"Who has access to Salesforce?\", \"Find all contractors with GitHub access who haven't used MFA in 30 days.\" The answers always involved the same painful loop: dig through a slow web console, chain API calls, correlate CSVs, write throwaway Python scripts. Repeat next week. I spent 12 months building Tako AI to fix this…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 10,
      "publishedAt": "2026-02-12T15:38:38.000Z",
      "score": 252,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-13T15:45:38.250Z",
      "file": "content/posts/2026-02-13-build-a-local-first-okta-agent-that-executes-deterministic-api-calls-and-enforces-zero-hallucinations.md",
      "fileFr": "content/posts/fr/2026-02-13-build-a-local-first-okta-agent-that-executes-deterministic-api-calls-and-enforces-zero-hallucinations.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "advanced",
      "timeToImplementMinutes": 240,
      "wordsEn": 1524,
      "wordsFr": 1310
    },
    {
      "id": "https://www.theverge.com/ai-artificial-intelligence/877931/bytedance-seedance-2-video-generator-ai-launch",
      "title": "ByteDance’s next-gen AI model can generate clips based on text, images, audio, and video",
      "link": "https://www.theverge.com/ai-artificial-intelligence/877931/bytedance-seedance-2-video-generator-ai-launch",
      "summary": "ByteDance says its new AI video model can more accurately follow prompts. | Image: ByteDance Big Tech's race to leapfrog the latest AI models continues with the launch of ByteDance's next-gen video generator. In a blog post, ByteDance - the China-based company behind TikTok - says Seedance 2.0 supports prompts that combine text, images, video, and audio. The company claims it \"delivers a substantial leap in generati…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-12T15:26:00.000Z",
      "score": 180,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-13T08:35:22.056Z",
      "file": "content/posts/2026-02-13-seedance-20-bytedances-multimodal-model-for-generating-up-to-15second-videos-from-text-images-audio-and-video.md",
      "fileFr": "content/posts/fr/2026-02-13-seedance-20-bytedances-multimodal-model-for-generating-up-to-15second-videos-from-text-images-audio-and-video.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "advanced",
      "timeToImplementMinutes": 180,
      "wordsEn": 1470,
      "wordsFr": 1283
    },
    {
      "id": "https://www.numerama.com/tech/2176523-seedance-le-nouveau-modele-chinois-pour-generer-des-videos-defie-openai-et-google.html",
      "title": "Seedance, le nouveau modèle chinois pour générer des vidéos, défie OpenAI et Google",
      "link": "https://www.numerama.com/tech/2176523-seedance-le-nouveau-modele-chinois-pour-generer-des-videos-defie-openai-et-google.html",
      "summary": "Début février 2026, le groupe chinois ByteDance a dévoilé Seedance 2.0, son nouveau modèle de génération vidéo par IA. Capable de produire image, son, voix et musique dans un même pipeline, l’outil impressionne autant par ses performances techniques que par sa stratégie de déploiement grand public.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T10:44:38.000Z",
      "score": 58.38,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-12T15:58:13.387Z",
      "file": "content/posts/2026-02-12-prototype-guide-integrating-bytedance-seedance-20-with-capcutdreamina-and-developer-apis.md",
      "fileFr": "content/posts/fr/2026-02-12-prototype-guide-integrating-bytedance-seedance-20-with-capcutdreamina-and-developer-apis.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1761,
      "wordsFr": 1399
    },
    {
      "id": "https://www.bbc.com/news/articles/cqxdj77welpo?at_medium=RSS&at_campaign=rss",
      "title": "EU tells Meta to let rivals run AI chatbots on WhatsApp",
      "link": "https://www.bbc.com/news/articles/cqxdj77welpo?at_medium=RSS&at_campaign=rss",
      "summary": "A Meta spokesperson said the EU had \"no reason\" to intervene over it changing the app in January.",
      "source": "BBC Technology",
      "region": "UK",
      "keywordHits": 2,
      "publishedAt": "2026-02-09T12:14:35.000Z",
      "score": 68.47,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-12T08:38:00.932Z",
      "file": "content/posts/2026-02-12-eu-says-meta-likely-blocked-rival-ai-chatbots-on-whatsapp-after-15-january-change.md",
      "fileFr": "content/posts/fr/2026-02-12-eu-says-meta-likely-blocked-rival-ai-chatbots-on-whatsapp-after-15-january-change.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1416,
      "wordsFr": 1554
    },
    {
      "id": "https://www.theverge.com/transportation/876540/uber-eats-ai-chatbot-cart-assistant-grocery-shopping",
      "title": "Uber Eats adds AI assistant to help with grocery shopping",
      "link": "https://www.theverge.com/transportation/876540/uber-eats-ai-chatbot-cart-assistant-grocery-shopping",
      "summary": "Uber announced a new AI feature called \"Cart Assistant\" for grocery shopping in its Uber Eats app. The new feature works a couple different ways. You can use text prompts, as you would with any other AI chatbot, to ask it to build a grocery list for you. Or you can upload a picture of your shopping list and ask it to populate your cart with all your favorite items, based on your order history. You can be as generic…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T12:00:00.000Z",
      "score": 71.79,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-11T16:07:33.804Z",
      "file": "content/posts/2026-02-11-prototype-guide-cart-assistant-that-prefills-uber-eats-grocery-carts-from-text-or-photo-shopping-lists.md",
      "fileFr": "content/posts/fr/2026-02-11-prototype-guide-cart-assistant-that-prefills-uber-eats-grocery-carts-from-text-or-photo-shopping-lists.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 480,
      "wordsEn": 1678,
      "wordsFr": 1572
    },
    {
      "id": "https://www.numerama.com/tech/2176519-comfyui-comment-generer-facilement-des-images-ou-des-videos-avec-une-carte-graphique-nvidia-rtx.html",
      "title": "ComfyUI : comment générer facilement des images ou des vidéos avec une carte graphique Nvidia RTX [Sponso]",
      "link": "https://www.numerama.com/tech/2176519-comfyui-comment-generer-facilement-des-images-ou-des-videos-avec-une-carte-graphique-nvidia-rtx.html",
      "summary": "Cet article a été réalisé en collaboration avec Nvidia Un PC solide, un GPU Nvidia de dernière génération, une solide connexion internet, ComfyUI et un peu de temps : voilà les ingrédients nécessaires à la mise en place d’un agent IA personnalisé pour générer des images ou des vidéos.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T06:47:00.000Z",
      "score": 89.42,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-11T08:41:38.417Z",
      "file": "content/posts/2026-02-11-set-up-comfyui-on-an-nvidia-rtx-pc-for-local-image-and-short-video-generation.md",
      "fileFr": "content/posts/fr/2026-02-11-set-up-comfyui-on-an-nvidia-rtx-pc-for-local-image-and-short-video-generation.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 180,
      "wordsEn": 1667,
      "wordsFr": 1002
    },
    {
      "id": "https://github.com/asterai-io/asterbot",
      "title": "Show HN: Asterbot, AI agent where every capability is a sandboxed WASM component",
      "link": "https://github.com/asterai-io/asterbot",
      "summary": "Asterbot is a modular AI agent where every capability, such as web search, memory, LLM provider, is a swappable WASM component, sandboxed via WASI. Components only have access to what you explicitly grant (e.g. a single directory). They're written in any language (Rust, Go, Python, JS) and pulled from the asterai registry. Under the hood, asterai is a WASM component model registry and runtime built on wasmtime. You…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-10T15:51:38.000Z",
      "score": 192,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-10T16:11:43.036Z",
      "file": "content/posts/2026-02-10-asterbot-an-ai-agent-built-from-sandboxed-swappable-wasm-components.md",
      "fileFr": "content/posts/fr/2026-02-10-asterbot-an-ai-agent-built-from-sandboxed-swappable-wasm-components.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1486,
      "wordsFr": 1233
    },
    {
      "id": "https://www.theverge.com/podcast/875233/siemens-ceo-roland-busch-ai-automation-digital-twins-nato-tariffs",
      "title": "Siemens CEO Roland Busch’s mission to automate everything",
      "link": "https://www.theverge.com/podcast/875233/siemens-ceo-roland-busch-ai-automation-digital-twins-nato-tariffs",
      "summary": "Today, I’m talking with Roland Busch, who is the CEO of Siemens. Siemens is one of those absolutely giant, extremely important, but fairly opaque companies we love to dig into on Decoder. At a very basic, reductive level, Siemens makes the hardware and software that allow other companies to run and automate their stuff. Everyone has seen the Siemens logo somewhere, whether it’s under the hood of their cars, stamped…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 11,
      "publishedAt": "2026-02-09T15:00:00.000Z",
      "score": 252,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-10T08:44:51.211Z",
      "file": "content/posts/2026-02-10-blueprint-for-a-scoped-factory-to-erp-automation-pilot-inspired-by-siemens-ceo-roland-busch.md",
      "fileFr": "content/posts/fr/2026-02-10-blueprint-for-a-scoped-factory-to-erp-automation-pilot-inspired-by-siemens-ceo-roland-busch.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1523,
      "wordsFr": 1332
    },
    {
      "id": "https://techcrunch.com/2026/01/22/are-ai-agents-ready-for-the-workplace-a-new-benchmark-raises-doubts/",
      "title": "Are AI agents ready for the workplace? A new benchmark raises doubts",
      "link": "https://techcrunch.com/2026/01/22/are-ai-agents-ready-for-the-workplace-a-new-benchmark-raises-doubts/",
      "summary": "Article URL: https://techcrunch.com/2026/01/22/are-ai-agents-ready-for-the-workplace-a-new-benchmark-raises-doubts/ Comments URL: https://news.ycombinator.com/item?id=46926131 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-07T18:18:17.000Z",
      "score": 192,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-09T16:02:00.433Z",
      "file": "content/posts/2026-02-09-build-an-apex-agents-style-harness-to-evaluate-ai-agents-multi-domain-performance.md",
      "fileFr": "content/posts/fr/2026-02-09-build-an-apex-agents-style-harness-to-evaluate-ai-agents-multi-domain-performance.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1460,
      "wordsFr": 1735
    },
    {
      "id": "https://www.theverge.com/ai-artificial-intelligence/875615/openai-super-bowl-ai-hardware-leak-hoax-fake",
      "title": "OpenAI’s supposedly ‘leaked’ Super Bowl ad with ear buds and a shiny orb was a hoax",
      "link": "https://www.theverge.com/ai-artificial-intelligence/875615/openai-super-bowl-ai-hardware-leak-hoax-fake",
      "summary": "As if OpenAI didn't have enough drama around the Super Bowl and advertising, as the game wound down, word spread of a \"leaked\" ad that actually wasn't leaked at all; it was just a fake. Screenshots of a now-deleted Reddit thread told the tale of a frustrated employee who, while posting about how upset they were because the ad they'd worked on didn't run, accidentally leaked the entire advertisement video, seemingly…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-02-09T04:54:36.000Z",
      "score": 68.09,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-09T08:43:14.751Z",
      "file": "content/posts/2026-02-09-screenshots-alleging-a-leaked-openai-super-bowl-ad-with-alexander-skarsgard-a-shiny-orb-and-earbuds-were-fabricated.md",
      "fileFr": "content/posts/fr/2026-02-09-screenshots-alleging-a-leaked-openai-super-bowl-ad-with-alexander-skarsgard-a-shiny-orb-and-earbuds-were-fabricated.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "beginner",
      "timeToImplementMinutes": 5,
      "wordsEn": 1474,
      "wordsFr": 1074
    },
    {
      "id": "https://github.com/rendro/sediment",
      "title": "Show HN: Sediment – Local semantic memory for AI agents (Rust, single binary)",
      "link": "https://github.com/rendro/sediment",
      "summary": "I've been increasingly relying on AI coding assistants. I recently had my first child, and my coding hours look different now. I prompt between feedings, sketch out ideas while he naps, and pick up where I left off later. AI lets me stay productive in fragmented time. But every session starts from zero. Claude doesn't remember the product roadmap we outlined last week. It doesn't know the design decisions we already…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 9,
      "publishedAt": "2026-02-08T14:41:07.000Z",
      "score": 246,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-08T15:30:53.355Z",
      "file": "content/posts/2026-02-08-add-persistent-local-semantic-memory-to-llm-agents-with-sediment-rust-single-binary.md",
      "fileFr": "content/posts/fr/2026-02-08-add-persistent-local-semantic-memory-to-llm-agents-with-sediment-rust-single-binary.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1697,
      "wordsFr": 1974
    },
    {
      "id": "https://www.numerama.com/sciences/2170347-horloge-de-lapocalypse-2026-il-ne-reste-que-85-secondes-avant-minuit.html",
      "title": "Horloge de l’Apocalypse 2026 : il ne reste que 85 secondes avant minuit",
      "link": "https://www.numerama.com/sciences/2170347-horloge-de-lapocalypse-2026-il-ne-reste-que-85-secondes-avant-minuit.html",
      "summary": "Réglée à 85 secondes de minuit le 27 janvier 2026, l’Horloge de l’Apocalypse n’a jamais été aussi proche du seuil symbolique de la catastrophe, selon le Bulletin of the Atomic Scientists. L’organisation alerte sur l’escalade des rivalités entre grandes puissances, la fragilisation des accords internationaux et les risques conjugués du nucléaire, du climat et de l’intelligence artificielle.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-01-29T17:33:03.000Z",
      "score": 24.57,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-08T08:25:10.003Z",
      "file": "content/posts/2026-02-08-doomsday-clock-at-85-seconds-2026-practical-implications-for-builders-and-tech-leaders.md",
      "fileFr": "content/posts/fr/2026-02-08-doomsday-clock-at-85-seconds-2026-practical-implications-for-builders-and-tech-leaders.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1475,
      "wordsFr": 1430
    },
    {
      "id": "https://www.theverge.com/transportation/874771/waymo-world-model-simulation-google-deepmind-genie-3",
      "title": "What happens when Waymo runs into a tornado? Or an elephant?",
      "link": "https://www.theverge.com/transportation/874771/waymo-world-model-simulation-google-deepmind-genie-3",
      "summary": "An autonomous vehicle drives down a lonely stretch of highway. Suddenly, a massive tornado appears in the distance. What does the driverless vehicle do next? This is just one of the scenarios that Waymo can simulate in the \"hyper realistic\" virtual world that it has just created with help from Google's DeepMind. Waymo's World Model is built using Genie 3, Google's new AI world model that can generate virtual interac…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 3,
      "publishedAt": "2026-02-06T16:00:00.000Z",
      "score": 40.56,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T20:11:38.689Z",
      "file": "content/posts/2026-02-06-waymo-uses-googles-genie-world-model-to-simulate-tornadoes-and-wildlife-for-edge-case-autonomous-vehicle-testing.md",
      "fileFr": "content/posts/fr/2026-02-06-waymo-uses-googles-genie-world-model-to-simulate-tornadoes-and-wildlife-for-edge-case-autonomous-vehicle-testing.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1489,
      "wordsFr": 1334
    },
    {
      "id": "https://arxiv.org/abs/2602.04210",
      "title": "Steering LLMs via Scalable Interactive Oversight",
      "link": "https://arxiv.org/abs/2602.04210",
      "summary": "arXiv:2602.04210v1 Announce Type: new \nAbstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \\emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficie",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 45.6,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T20:06:29.835Z",
      "file": "content/posts/2026-02-06-scalable-interactive-oversight-building-a-decision-tree-prototype-to-collect-node-level-feedback-and-steer-llms.md",
      "fileFr": "content/posts/fr/2026-02-06-scalable-interactive-oversight-building-a-decision-tree-prototype-to-collect-node-level-feedback-and-steer-llms.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1623,
      "wordsFr": 1444
    },
    {
      "id": "https://arxiv.org/abs/2602.04003",
      "title": "When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making",
      "link": "https://arxiv.org/abs/2602.04003",
      "summary": "arXiv:2602.04003v1 Announce Type: new \nAbstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 45.6,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T20:01:44.770Z",
      "file": "content/posts/2026-02-06-adversarial-explanation-attacks-how-llm-framing-preserves-user-trust-in-incorrect-outputs.md",
      "fileFr": "content/posts/fr/2026-02-06-adversarial-explanation-attacks-how-llm-framing-preserves-user-trust-in-incorrect-outputs.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1428,
      "wordsFr": 1614
    },
    {
      "id": "https://arxiv.org/abs/2602.04101",
      "title": "Interfaze: The Future of AI is built on Task-Specific Small Models",
      "link": "https://arxiv.org/abs/2602.04101",
      "summary": "arXiv:2602.04101v1 Announce Type: new \nAbstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogene",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 55.6,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T19:58:27.829Z",
      "file": "content/posts/2026-02-06-prototyping-interfaze-building-a-multimodal-perception-context-construction-and-action-stack-for-task-specific-small-models.md",
      "fileFr": "content/posts/fr/2026-02-06-prototyping-interfaze-building-a-multimodal-perception-context-construction-and-action-stack-for-task-specific-small-models.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1499,
      "wordsFr": 1442
    },
    {
      "id": "https://arxiv.org/abs/2602.04089",
      "title": "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL",
      "link": "https://arxiv.org/abs/2602.04089",
      "summary": "arXiv:2602.04089v1 Announce Type: new \nAbstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inheren",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 55.6,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:54:50.764Z",
      "file": "content/posts/2026-02-06-orbit-crossepisode-metarl-for-incontext-online-adaptation-of-llms.md",
      "fileFr": "content/posts/fr/2026-02-06-orbit-crossepisode-metarl-for-incontext-online-adaptation-of-llms.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "advanced",
      "timeToImplementMinutes": 5,
      "wordsEn": 1305,
      "wordsFr": 1325
    },
    {
      "id": "https://arxiv.org/abs/2602.03975",
      "title": "Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure",
      "link": "https://arxiv.org/abs/2602.03975",
      "summary": "arXiv:2602.03975v1 Announce Type: new \nAbstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 55.6,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:50:57.091Z",
      "file": "content/posts/2026-02-06-state-level-selective-verification-with-learned-heuristics-for-verification-cost-limited-llm-reasoning.md",
      "fileFr": "content/posts/fr/2026-02-06-state-level-selective-verification-with-learned-heuristics-for-verification-cost-limited-llm-reasoning.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1302,
      "wordsFr": 1138
    },
    {
      "id": "https://arxiv.org/abs/2602.03900",
      "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks",
      "link": "https://arxiv.org/abs/2602.03900",
      "summary": "arXiv:2602.03900v1 Announce Type: new \nAbstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have co",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 55.6,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:47:32.595Z",
      "file": "content/posts/2026-02-06-task-method-knowledge-prompting-improves-llm-planning-on-planbench-blocksworld.md",
      "fileFr": "content/posts/fr/2026-02-06-task-method-knowledge-prompting-improves-llm-planning-on-planbench-blocksworld.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1259,
      "wordsFr": 1264
    },
    {
      "id": "https://www.theverge.com/entertainment/874504/super-bowl-lx-ads-big-game",
      "title": "Super Bowl LX ads: all AI everything",
      "link": "https://www.theverge.com/entertainment/874504/super-bowl-lx-ads-big-game",
      "summary": "Super Bowl LX is nearly here, with the Seattle Seahawks taking on the New England Patriots. While Bad Bunny will be the star of the halftime show, AI could be the star of the commercial breaks, much like crypto was a few years ago. Last year’s Super Bowl featured a Google Gemini ad that fumbled a Gouda cheese stat, and this year’s game is already slated to include an ad for Anthropic’s AI platform that takes jabs at…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-05T18:18:34.000Z",
      "score": 56.5,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T19:44:19.060Z",
      "file": "content/posts/2026-02-05-super-bowl-lx-platform-branded-ai-ads-creative-risks-and-builder-priorities.md",
      "fileFr": "content/posts/fr/2026-02-05-super-bowl-lx-platform-branded-ai-ads-creative-risks-and-builder-priorities.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1613,
      "wordsFr": 1571
    },
    {
      "id": "https://www.theverge.com/ai-artificial-intelligence/874440/anthropic-opus-4-6-new-model-claude",
      "title": "Anthropic debuts new model with hopes to corner the market beyond coding",
      "link": "https://www.theverge.com/ai-artificial-intelligence/874440/anthropic-opus-4-6-new-model-claude",
      "summary": "Anthropic's \"smartest model\" is getting a major boost, the company said in a blog post announcing Claude Opus 4.6. It called the new model a \"direct upgrade\" from its predecessor in a release, noting that it can better take on complex, multi-step tasks and get \"much closer to production-ready quality on the first try than what we've seen with any model - documents, spreadsheets, and presentations will need less back…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-05T18:00:00.000Z",
      "score": 62.48,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:40:47.400Z",
      "file": "content/posts/2026-02-05-anthropic-opus-46-direct-upgrade-pitched-to-cut-edit-rounds-for-documents-spreadsheets-and-agentic-tasks.md",
      "fileFr": "content/posts/fr/2026-02-05-anthropic-opus-46-direct-upgrade-pitched-to-cut-edit-rounds-for-documents-spreadsheets-and-agentic-tasks.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1548,
      "wordsFr": 1857
    },
    {
      "id": "https://arxiv.org/abs/2602.04144",
      "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows",
      "link": "https://arxiv.org/abs/2602.04144",
      "summary": "arXiv:2602.04144v1 Announce Type: new \nAbstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on in",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 65.6,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:36:28.842Z",
      "file": "content/posts/2026-02-06-analysis-omg-agents-decoupled-planner-retriever-executor-pipeline-for-missing-modality-generation.md",
      "fileFr": "content/posts/fr/2026-02-06-analysis-omg-agents-decoupled-planner-retriever-executor-pipeline-for-missing-modality-generation.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "advanced",
      "timeToImplementMinutes": 5,
      "wordsEn": 1386,
      "wordsFr": 1306
    },
    {
      "id": "https://arxiv.org/abs/2602.03955",
      "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent",
      "link": "https://arxiv.org/abs/2602.03955",
      "summary": "arXiv:2602.03955v1 Announce Type: new \nAbstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes Agent",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 65.6,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:32:07.824Z",
      "file": "content/posts/2026-02-06-agentark-turning-multi-agent-debate-into-single-agent-capabilities-via-hierarchical-distillation.md",
      "fileFr": "content/posts/fr/2026-02-06-agentark-turning-multi-agent-debate-into-single-agent-capabilities-via-hierarchical-distillation.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1315,
      "wordsFr": 1281
    },
    {
      "id": "https://www.theverge.com/transportation/875199/apple-carplay-third-party-chatbots-rumor",
      "title": "Apple might let you use ChatGPT from CarPlay",
      "link": "https://www.theverge.com/transportation/875199/apple-carplay-third-party-chatbots-rumor",
      "summary": "CarPlay users could soon be able to use their chatbot of choice instead of Siri. As Bloomberg reports, Apple is working to add support for CarPlay voice control apps from OpenAI, Anthropic, Google, and others. Previously, users who wanted to access third-party chatbots in the car would need to go through their iPhone, but soon they may be able to talk with ChatGPT, Claude, or Gemini directly in CarPlay. However, App…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 5,
      "publishedAt": "2026-02-06T21:32:44.000Z",
      "score": 65.78,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T19:28:00.944Z",
      "file": "content/posts/2026-02-06-apple-reportedly-testing-carplay-support-for-third-party-voice-chat-apps-but-siri-controls-remain.md",
      "fileFr": "content/posts/fr/2026-02-06-apple-reportedly-testing-carplay-support-for-third-party-voice-chat-apps-but-siri-controls-remain.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1564,
      "wordsFr": 1178
    },
    {
      "id": "https://arxiv.org/abs/2602.04213",
      "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons",
      "link": "https://arxiv.org/abs/2602.04213",
      "summary": "arXiv:2602.04213v1 Announce Type: new Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose I…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 65.89,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:25:15.767Z",
      "file": "content/posts/2026-02-06-interpret-interactive-policy-restructuring-enables-laypersons-to-train-more-robust-imitation-policies.md",
      "fileFr": "content/posts/fr/2026-02-06-interpret-interactive-policy-restructuring-enables-laypersons-to-train-more-robust-imitation-policies.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1328,
      "wordsFr": 1353
    },
    {
      "id": "https://arxiv.org/abs/2602.03974",
      "title": "Active Epistemic Control for Query-Efficient Verified Planning",
      "link": "https://arxiv.org/abs/2602.03974",
      "summary": "arXiv:2602.03974v1 Announce Type: new Abstract: Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \\…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 71.53,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:21:17.277Z",
      "file": "content/posts/2026-02-06-active-epistemic-control-grounded-fact-versus-belief-stores-and-sq-bcp-gating-for-verified-planning.md",
      "fileFr": "content/posts/fr/2026-02-06-active-epistemic-control-grounded-fact-versus-belief-stores-and-sq-bcp-gating-for-verified-planning.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1643,
      "wordsFr": 1398
    },
    {
      "id": "https://www.theverge.com/ai-artificial-intelligence/874258/openai-frontier-ai-agent-platform-management",
      "title": "OpenAI Frontier is a single platform to control your AI agents",
      "link": "https://www.theverge.com/ai-artificial-intelligence/874258/openai-frontier-ai-agent-platform-management",
      "summary": "Managing humans is hard. Managing AI agents is… also hard. That's why OpenAI is launching a new platform called OpenAI Frontier, which it says will help businesses \"build, deploy, and manage\" AI agents, even those not made by OpenAI itself. OpenAI's description of Frontier sounds something like HR for AI. \"Frontier gives agents the same skills people need to succeed at work: shared context, onboarding, hands-on lear…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-05T14:00:00.000Z",
      "score": 74.29,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T19:17:52.583Z",
      "file": "content/posts/2026-02-05-using-openai-frontier-to-implement-an-agent-lifecycle-onboarding-permissions-testing-and-rollout.md",
      "fileFr": "content/posts/fr/2026-02-05-using-openai-frontier-to-implement-an-agent-lifecycle-onboarding-permissions-testing-and-rollout.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1406,
      "wordsFr": 1464
    },
    {
      "id": "https://www.theverge.com/podcast/874038/ai-deepfakes-war-on-reality-c2pa-labels",
      "title": "Reality is losing the deepfake war",
      "link": "https://www.theverge.com/podcast/874038/ai-deepfakes-war-on-reality-c2pa-labels",
      "summary": "Today, we’re going to talk about reality, and whether we can label photos and videos to protect our shared understanding of the world around us. No really, we’re gonna go there. It’s a deep one. To do this, I’m going to bring on Verge reporter Jess Weatherbed, who covers creative tools for us — a space that’s been totally upended by generative AI in a huge variety of ways with an equally huge number of responses fro…",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 7,
      "publishedAt": "2026-02-05T15:00:00.000Z",
      "score": 86.34,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T19:14:29.728Z",
      "file": "content/posts/2026-02-05-provenance-labels-and-metadata-are-failing-as-deepfakes-scale.md",
      "fileFr": "content/posts/fr/2026-02-05-provenance-labels-and-metadata-are-failing-as-deepfakes-scale.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1331,
      "wordsFr": 1239
    },
    {
      "id": "https://arxiv.org/abs/2602.04284",
      "title": "Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning",
      "link": "https://arxiv.org/abs/2602.04284",
      "summary": "arXiv:2602.04284v1 Announce Type: new Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 95.89,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:10:59.502Z",
      "file": "content/posts/2026-02-06-agent-omit-a-training-framework-for-adaptive-omission-of-thoughts-and-observations-in-llm-agents.md",
      "fileFr": "content/posts/fr/2026-02-06-agent-omit-a-training-framework-for-adaptive-omission-of-thoughts-and-observations-in-llm-agents.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1398,
      "wordsFr": 1464
    },
    {
      "id": "https://arxiv.org/abs/2602.04248",
      "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
      "link": "https://arxiv.org/abs/2602.04248",
      "summary": "arXiv:2602.04248v1 Announce Type: new Abstract: Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic o…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 10,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 137.89,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T19:05:50.256Z",
      "file": "content/posts/2026-02-06-empirical-mcts-dual-loop-mcts-with-evolving-meta-prompts-and-a-global-memory-agent.md",
      "fileFr": "content/posts/fr/2026-02-06-empirical-mcts-dual-loop-mcts-with-evolving-meta-prompts-and-a-global-memory-agent.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "advanced",
      "timeToImplementMinutes": 5,
      "wordsEn": 1510,
      "wordsFr": 1332
    },
    {
      "id": "https://www.numerama.com/tech/2173427-vous-etes-client-bouygues-cest-maintenant-ou-jamais-pour-activer-perplexity-pro-gratuitement.html",
      "title": "Vous êtes client Bouygues ? C’est maintenant ou jamais pour activer Perplexity Pro gratuitement",
      "link": "https://www.numerama.com/tech/2173427-vous-etes-client-bouygues-cest-maintenant-ou-jamais-pour-activer-perplexity-pro-gratuitement.html",
      "summary": "Depuis près d’un an, Bouygues Telecom propose à ses clients un abonnement gratuit à Perplexity Pro. Mais toute bonne chose a une fin : l’accès gratuit à ce LLM se terminera dans quelques jours. L’heure est donc venue, pour certains, de se désabonner… et pour d’autres, de profiter des tout derniers moments pour s’inscrire.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-04T13:56:55.000Z",
      "score": 31.74,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T19:00:52.477Z",
      "file": "content/posts/2026-02-04-bouygues-telecom-ends-free-perplexity-pro-access-on-11-feb-2026-activate-from-your-customer-account.md",
      "fileFr": "content/posts/fr/2026-02-04-bouygues-telecom-ends-free-perplexity-pro-access-on-11-feb-2026-activate-from-your-customer-account.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1456,
      "wordsFr": 1260
    },
    {
      "id": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/",
      "title": "Advancing AI benchmarking with Game Arena",
      "link": "https://blog.google/innovation-and-ai/models-and-research/google-deepmind/kaggle-game-arena-updates/",
      "summary": "We’re expanding Game Arena with Poker and Werewolf, while Gemini 3 Pro and Flash top our chess leaderboard.",
      "source": "Google AI Blog",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-02T17:00:00.000Z",
      "score": 31.31,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T18:57:34.948Z",
      "file": "content/posts/2026-02-02-kaggle-game-arena-expands-with-poker-and-werewolf-gemini-3-pro-and-flash-top-chess.md",
      "fileFr": "content/posts/fr/2026-02-02-kaggle-game-arena-expands-with-poker-and-werewolf-gemini-3-pro-and-flash-top-chess.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1479,
      "wordsFr": 1543
    },
    {
      "id": "https://www.technologyreview.com/2026/01/30/1131945/inside-the-marketplace-powering-bespoke-ai-deepfakes-of-real-women/",
      "title": "Inside the marketplace powering bespoke AI deepfakes of real women",
      "link": "https://www.technologyreview.com/2026/01/30/1131945/inside-the-marketplace-powering-bespoke-ai-deepfakes-of-real-women/",
      "summary": "Civitai—an online marketplace for buying and selling AI-generated content, backed by the venture capital firm Andreessen Horowitz—is letting users buy custom instruction files for generating celebrity deepfakes. Some of these files were specifically designed to make pornographic",
      "source": "MIT Tech Review AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-01-30T16:32:31.000Z",
      "score": 20.73,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:54:15.327Z",
      "file": "content/posts/2026-01-30-civitai-lora-files-and-bounties-enable-bespoke-deepfakes-targeting-real-women.md",
      "fileFr": "content/posts/fr/2026-01-30-civitai-lora-files-and-bounties-enable-bespoke-deepfakes-targeting-real-women.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1421,
      "wordsFr": 1371
    },
    {
      "id": "https://arstechnica.com/ai/2026/01/how-often-do-ai-chatbots-lead-users-down-a-harmful-path/",
      "title": "How often do AI chatbots lead users down a harmful path?",
      "link": "https://arstechnica.com/ai/2026/01/how-often-do-ai-chatbots-lead-users-down-a-harmful-path/",
      "summary": "Anthropic's latest paper on \"user disempowerment\" has some troubling findings.",
      "source": "Ars Technica AI",
      "region": "US",
      "keywordHits": 3,
      "publishedAt": "2026-01-29T22:05:59.000Z",
      "score": 48.57,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T18:50:06.543Z",
      "file": "content/posts/2026-01-29-anthropics-15m-chat-analysis-identifies-reality-belief-and-action-disempowerment-in-claude.md",
      "fileFr": "content/posts/fr/2026-01-29-anthropics-15m-chat-analysis-identifies-reality-belief-and-action-disempowerment-in-claude.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1199,
      "wordsFr": 1540
    },
    {
      "id": "https://www.technologyreview.com/2026/01/27/1131793/openais-latest-product-lets-you-vibe-code-science/",
      "title": "OpenAI’s latest product lets you vibe code science",
      "link": "https://www.technologyreview.com/2026/01/27/1131793/openais-latest-product-lets-you-vibe-code-science/",
      "summary": "OpenAI just revealed what its new in-house team, OpenAI for Science, has been up to. The firm has released a free LLM-powered tool for scientists called Prism, which embeds ChatGPT in a text editor for writing scientific papers. The idea is to put ChatGPT front and center inside",
      "source": "MIT Tech Review AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-01-27T18:00:43.000Z",
      "score": 40.51,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T18:46:34.371Z",
      "file": "content/posts/2026-01-27-prism-openai-embeds-chatgpt-into-a-scientific-paper-editor-to-streamline-drafting-and-literature-triage.md",
      "fileFr": "content/posts/fr/2026-01-27-prism-openai-embeds-chatgpt-into-a-scientific-paper-editor-to-streamline-drafting-and-literature-triage.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1354,
      "wordsFr": 1317
    },
    {
      "id": "https://arstechnica.com/features/2026/01/has-gemini-surpassed-chatgpt-we-put-the-ai-models-to-the-test/",
      "title": "Has Gemini surpassed ChatGPT? We put the AI models to the test.",
      "link": "https://arstechnica.com/features/2026/01/has-gemini-surpassed-chatgpt-we-put-the-ai-models-to-the-test/",
      "summary": "Did Apple make the right choice in partnering with Google for Siri's AI features?",
      "source": "Ars Technica AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-01-21T15:03:39.000Z",
      "score": 72.29,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:43:37.424Z",
      "file": "content/posts/2026-01-21-chatgpt-52-vs-gemini-32-fast-ars-technica-headtohead-and-what-apples-gemini-choice-means-for-siri.md",
      "fileFr": "content/posts/fr/2026-01-21-chatgpt-52-vs-gemini-32-fast-ars-technica-headtohead-and-what-apples-gemini-choice-means-for-siri.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1609,
      "wordsFr": 1139
    },
    {
      "id": "https://blog.google/products-and-platforms/products/gemini/how-nano-banana-got-its-name/",
      "title": "How Nano Banana got its name",
      "link": "https://blog.google/products-and-platforms/products/gemini/how-nano-banana-got-its-name/",
      "summary": "We’re peeling back the origin story of Nano Banana, one of Google DeepMind’s most popular models.",
      "source": "Google AI Blog",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-01-15T16:06:00.000Z",
      "score": 24.22,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:40:28.152Z",
      "file": "content/posts/2026-01-15-how-google-deepmind-chose-the-name-nano-banana-canonical-naming-note.md",
      "fileFr": "content/posts/fr/2026-01-15-how-google-deepmind-chose-the-name-nano-banana-canonical-naming-note.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "beginner",
      "timeToImplementMinutes": 5,
      "wordsEn": 1352,
      "wordsFr": 1272
    },
    {
      "id": "https://techcrunch.com/2026/01/13/ai-drug-discovery-startup-converge-bio-pulls-in-25m-from-bessemer-and-execs-from-meta-openai-and-wiz/",
      "title": "Converge Bio raises $25M, backed by Bessemer and execs from Meta, OpenAI, Wiz",
      "link": "https://techcrunch.com/2026/01/13/ai-drug-discovery-startup-converge-bio-pulls-in-25m-from-bessemer-and-execs-from-meta-openai-and-wiz/",
      "summary": "AI drug discovery startup Converge Bio raised $25 million in a Series A led by Bessemer Venture Partners, with additional backing from executives at Meta, OpenAI, and Wiz.",
      "source": "TechCrunch AI",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-01-13T11:30:00.000Z",
      "score": 36.2,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:37:37.795Z",
      "file": "content/posts/2026-01-13-converge-bio-raises-dollar25m-series-a-to-scale-sequence-trained-generative-ai-for-antibody-design-and-protein-optimization.md",
      "fileFr": "content/posts/fr/2026-01-13-converge-bio-raises-dollar25m-series-a-to-scale-sequence-trained-generative-ai-for-antibody-design-and-protein-optimization.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1537,
      "wordsFr": 1260
    },
    {
      "id": "https://blogs.nvidia.com/blog/2026-ces-special-presentation/",
      "title": "NVIDIA Rubin Platform, Open Models, Autonomous Driving: NVIDIA Presents Blueprint for the Future at CES",
      "link": "https://blogs.nvidia.com/blog/2026-ces-special-presentation/",
      "summary": "NVIDIA founder and CEO Jensen Huang took the stage at the Fontainebleau Las Vegas to open CES 2026, declaring that AI is scaling into every domain and every device. “Computing has been fundamentally reshaped as a result of accelerated computing, as a result of artificial intelligence,” Huang said. “What that means is some $10 trillion Read Article",
      "source": "NVIDIA Blog",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-01-05T23:30:18.000Z",
      "score": 60.15,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:33:35.108Z",
      "file": "content/posts/2026-01-05-nvidia-rubin-and-alpamayo-six-chip-production-ai-platform-and-open-reasoning-models-for-autonomy.md",
      "fileFr": "content/posts/fr/2026-01-05-nvidia-rubin-and-alpamayo-six-chip-production-ai-platform-and-open-reasoning-models-for-autonomy.md",
      "generationMode": "llm",
      "series": "founder-notes",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1849,
      "wordsFr": 1238
    },
    {
      "id": "https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/",
      "title": "Gemma Scope 2: helping the AI safety community deepen understanding of complex language model behavior",
      "link": "https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/",
      "summary": "Open interpretability tools for language models are now available across the entire Gemma 3 family with the release of Gemma Scope 2.",
      "source": "Google DeepMind News",
      "region": "UK",
      "keywordHits": 3,
      "publishedAt": "2025-12-16T10:14:24.000Z",
      "score": 48.09,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T18:30:22.913Z",
      "file": "content/posts/2025-12-16-gemma-scope-2-expands-open-interpretability-and-reproducible-traces-across-the-gemma-3-family.md",
      "fileFr": "content/posts/fr/2025-12-16-gemma-scope-2-expands-open-interpretability-and-reproducible-traces-across-the-gemma-3-family.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1502,
      "wordsFr": 1044
    },
    {
      "id": "https://blogs.nvidia.com/blog/leading-models-nvidia/",
      "title": "As AI Grows More Complex, Model Builders Rely on NVIDIA",
      "link": "https://blogs.nvidia.com/blog/leading-models-nvidia/",
      "summary": "Unveiling what it describes as the most capable model series yet for professional knowledge work, OpenAI launched GPT-5.2 today. The model was trained and deployed on NVIDIA infrastructure, including NVIDIA Hopper and GB200 NVL72 systems. GPT-5.2 achieves the top reported score for industry benchmarks like GPQA-Diamond, AIME 2025 and Tau2 Telecom. On leading benchmarks targeting Read Article",
      "source": "NVIDIA Blog",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2025-12-11T19:19:57.000Z",
      "score": 60.09,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T18:27:05.231Z",
      "file": "content/posts/2025-12-11-prototyping-multi-node-pretraining-and-staged-inference-on-nvidia-hopper-and-gb200-nvl72.md",
      "fileFr": "content/posts/fr/2025-12-11-prototyping-multi-node-pretraining-and-staged-inference-on-nvidia-hopper-and-gb200-nvl72.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 120,
      "wordsEn": 1480,
      "wordsFr": 1476
    },
    {
      "id": "https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/",
      "title": "FACTS Benchmark Suite: Systematically evaluating the factuality of large language models",
      "link": "https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/",
      "summary": "Systematically evaluating the factuality of large language models with the FACTS Benchmark Suite.",
      "source": "Google DeepMind News",
      "region": "UK",
      "keywordHits": 3,
      "publishedAt": "2025-12-09T11:29:03.000Z",
      "score": 54.08,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "ANALYSIS",
      "publishedAtRun": "2026-02-07T18:22:47.638Z",
      "file": "content/posts/2025-12-09-deepminds-facts-benchmark-suite-a-claim-level-framework-and-quick-start-checklist-for-evaluating-llm-factuality.md",
      "fileFr": "content/posts/fr/2025-12-09-deepminds-facts-benchmark-suite-a-claim-level-framework-and-quick-start-checklist-for-evaluating-llm-factuality.md",
      "generationMode": "llm",
      "series": "tooling-deep-dive",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1376,
      "wordsFr": 1765
    },
    {
      "id": "manual:openclaw-beginner-tutorial",
      "title": "OpenClaw for Beginners: Install, Onboard, and Ship Your First Agent Workflow",
      "link": "https://docs.openclaw.ai/start/wizard",
      "summary": "Beginner tutorial: install OpenClaw, run the onboarding wizard, connect a messaging platform, and ship a safe first workflow with skills + guardrails.",
      "source": "OpenClaw Docs",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-07T12:00:00.000Z",
      "score": 999,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T12:31:26.407Z",
      "file": "content/posts/2026-02-07-set-up-openclaw-with-the-cli-onboarding-configure-gateway-seed-workspace-install-a-daemon-and-add-channels.md",
      "fileFr": "content/posts/fr/2026-02-07-set-up-openclaw-with-the-cli-onboarding-configure-gateway-seed-workspace-install-a-daemon-and-add-channels.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "beginner",
      "timeToImplementMinutes": 60,
      "wordsEn": 1559,
      "wordsFr": 1360
    },
    {
      "id": "https://arxiv.org/abs/2602.04326",
      "title": "From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents",
      "link": "https://arxiv.org/abs/2602.04326",
      "summary": "arXiv:2602.04326v1 Announce Type: new Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adapt…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 9,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 149.89,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-07T10:56:37.463Z",
      "file": "content/posts/2026-02-07-pce-converting-llm-reasoning-traces-into-decision-trees-for-uncertainty-aware-planning-in-embodied-multi-agent-tasks.md",
      "fileFr": "content/posts/fr/2026-02-07-pce-converting-llm-reasoning-traces-into-decision-trees-for-uncertainty-aware-planning-in-embodied-multi-agent-tasks.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "advanced",
      "timeToImplementMinutes": 360,
      "wordsEn": 1480,
      "wordsFr": 1443
    },
    {
      "id": "https://www.lemonde.fr/podcasts/article/2026/02/05/l-intelligence-artificielle-va-t-elle-detruire-nos-emplois_6665446_5463015.html",
      "title": "L’intelligence artificielle va-t-elle détruire nos emplois ?",
      "link": "https://www.lemonde.fr/podcasts/article/2026/02/05/l-intelligence-artificielle-va-t-elle-detruire-nos-emplois_6665446_5463015.html",
      "summary": "Les plans sociaux justifiés par le déploiement de l’IA en entreprise et les déclarations des acteurs du secteur posent question. Dans ce podcast, Alexandre Piquard, journaliste au service Economie du « Monde », fait un point nuancé sur les répercussions qu’a aujourd’hui le déploi",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 0,
      "publishedAt": "2026-02-05T04:00:11.000Z",
      "score": 23.45,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-07T09:57:41.567Z",
      "file": "content/posts/2026-02-07-lintelligence-artificielle-va-t-elle-detruire-nos-emplois.md",
      "fileFr": "content/posts/fr/2026-02-07-lintelligence-artificielle-va-t-elle-detruire-nos-emplois.md",
      "generationMode": "llm",
      "series": "model-release-brief",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 5,
      "wordsEn": 1679,
      "wordsFr": 1221
    },
    {
      "id": "https://openai.com/index/unlocking-the-codex-harness",
      "title": "Unlocking the Codex harness: how we built the App Server",
      "link": "https://openai.com/index/unlocking-the-codex-harness",
      "summary": "Learn how to embed the Codex agent using the Codex App Server, a bidirectional JSON-RPC API powering streaming progress, tool use, approvals, and diffs.",
      "source": "OpenAI News",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-04T13:00:00.000Z",
      "score": 12.52,
      "status": "published",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-06T18:39:12.939Z",
      "file": "content/posts/2026-02-06-unlocking-the-codex-harness-how-we-built-the-app-server.md",
      "fileFr": "content/posts/fr/2026-02-06-unlocking-the-codex-harness-how-we-built-the-app-server.md",
      "generationMode": "llm",
      "series": "agent-playbook",
      "difficulty": "intermediate",
      "timeToImplementMinutes": 240,
      "wordsEn": 1497,
      "wordsFr": 1300
    },
    {
      "id": "https://www.technologyreview.com/2026/01/28/1131003/rules-fail-at-the-prompt-succeed-at-the-boundary/",
      "title": "Rules fail at the prompt, succeed at the boundary",
      "link": "https://www.technologyreview.com/2026/01/28/1131003/rules-fail-at-the-prompt-succeed-at-the-boundary/",
      "summary": "From the Gemini Calendar prompt-injection attack of 2026 to the September 2025 state-sponsored hack using Anthropic’s Claude code as an automated intrusion engine, the coercion of human-in-the-loop agentic actions and fully autonomous agentic workflows are the new attack vector for hackers. In the Anthropic case, roughly 30 organizations across tech, finance, manufacturing, and government were…",
      "source": "MIT Tech Review AI",
      "region": "US",
      "keywordHits": 5,
      "publishedAt": "2026-01-28T14:00:00.000Z",
      "score": 72.55,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "publishedAtRun": "2026-02-06T16:57:33.586Z",
      "file": "content/posts/2026-02-06-rules-fail-at-the-prompt-succeed-at-the-boundary.md",
      "fileFr": "content/posts/fr/2026-02-06-rules-fail-at-the-prompt-succeed-at-the-boundary.md",
      "wordsEn": 1290,
      "wordsFr": 1521
    },
    {
      "id": "https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl",
      "title": "Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective",
      "link": "https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl",
      "summary": "",
      "source": "Hugging Face Blog",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-01-27T01:53:15.000Z",
      "score": 36.47,
      "status": "published",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-06T15:28:17.709Z",
      "file": "content/posts/2026-02-06-unlocking-agentic-rl-training-for-gpt-oss-a-practical-retrospective.md",
      "fileFr": "content/posts/fr/2026-02-06-unlocking-agentic-rl-training-for-gpt-oss-a-practical-retrospective.md",
      "wordsEn": 1564,
      "wordsFr": 1590
    },
    {
      "id": "https://www.bbc.com/news/articles/ce3edyx74jko?at_medium=RSS&at_campaign=rss",
      "title": "ChatGPT boss ridiculed for online 'tantrum' over rival's Super Bowl ad",
      "link": "https://www.bbc.com/news/articles/ce3edyx74jko?at_medium=RSS&at_campaign=rss",
      "summary": "Commenters said Altman's lengthy post shows \"a nerve was well and truly hit\" by Anthropic's advert.",
      "source": "BBC Technology",
      "region": "UK",
      "keywordHits": 2,
      "publishedAt": "2026-02-05T12:36:32.000Z",
      "score": 28.48,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-06T15:24:40.392Z",
      "file": "content/posts/2026-02-06-chatgpt-boss-ridiculed-for-online-tantrum-over-rivals-super-bowl-ad.md",
      "fileFr": "content/posts/fr/2026-02-06-chatgpt-boss-ridiculed-for-online-tantrum-over-rivals-super-bowl-ad.md",
      "wordsEn": 970,
      "wordsFr": 923
    },
    {
      "id": "https://www.bbc.com/news/articles/c9wx2dz2v44o?at_medium=RSS&at_campaign=rss",
      "title": "AI 'slop' is transforming social media - and a backlash is brewing",
      "link": "https://www.bbc.com/news/articles/c9wx2dz2v44o?at_medium=RSS&at_campaign=rss",
      "summary": "Social media has been flooded with fake, AI-generated images and videos. But will the majority of users actually care?",
      "source": "BBC Technology",
      "region": "UK",
      "keywordHits": 0,
      "publishedAt": "2026-02-04T11:29:30.000Z",
      "score": 12.34,
      "status": "published",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "publishedAtRun": "2026-02-06T15:19:24.319Z",
      "file": "content/posts/2026-02-06-ai-slop-is-transforming-social-media-and-a-backlash-is-brewing.md",
      "fileFr": "content/posts/fr/2026-02-06-ai-slop-is-transforming-social-media-and-a-backlash-is-brewing.md",
      "wordsEn": 973,
      "wordsFr": 924
    },
    {
      "id": "https://openai.com/index/retiring-gpt-4o-and-older-models",
      "title": "Retiring GPT-4o, GPT-4.1, GPT-4.1 mini, and OpenAI o4-mini in ChatGPT",
      "link": "https://openai.com/index/retiring-gpt-4o-and-older-models",
      "summary": "On February 13, 2026, alongside the previously announced retirement⁠ of GPT‑5 (Instant, Thinking, and Pro), we will retire GPT‑4o, GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini from ChatGPT. In the API, there are no changes at this time.",
      "source": "OpenAI News",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-01-29T00:00:00.000Z",
      "score": 36.58,
      "status": "published",
      "targetRegion": "US",
      "publishedAtRun": "2026-02-06T15:14:11.915Z",
      "file": "content/posts/2026-02-06-retiring-gpt-4o-gpt-41-gpt-41-mini-and-openai-o4-mini-in-chatgpt.md",
      "fileFr": "content/posts/fr/2026-02-06-retiring-gpt-4o-gpt-41-gpt-41-mini-and-openai-o4-mini-in-chatgpt.md"
    },
    {
      "id": "https://www.lemonde.fr/politique/article/2026/02/05/chatbots-campagnes-augmentees-l-ia-s-immisce-dans-la-politique-francaise_6665450_823448.html",
      "title": "Chatbots, campagnes « augmentées »… l’IA s’immisce dans la politique française",
      "link": "https://www.lemonde.fr/politique/article/2026/02/05/chatbots-campagnes-augmentees-l-ia-s-immisce-dans-la-politique-francaise_6665450_823448.html",
      "summary": "Au-delà de la production de visuels destinés aux réseaux sociaux, les partis intègrent de plus en plus les outils d’intelligence artificielle dans leur stratégie électorale. D’après une enquête, 27 % des personnes interrogées envisagent d’utiliser l’IA pour se renseigner sur les",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 0,
      "publishedAt": "2026-02-05T04:30:00.000Z",
      "score": 33.5,
      "status": "published",
      "targetRegion": "FR",
      "publishedAtRun": "2026-02-06T14:55:49.006Z",
      "file": "content/posts/2026-02-06-chatbots-campagnes-augmentees-lia-simmisce-dans-la-politique-francaise.md",
      "fileFr": "content/posts/fr/2026-02-06-chatbots-campagnes-augmentees-lia-simmisce-dans-la-politique-francaise.md"
    },
    {
      "id": "https://arxiv.org/abs/2602.03950",
      "title": "Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation",
      "link": "https://arxiv.org/abs/2602.03950",
      "summary": "arXiv:2602.03950v1 Announce Type: new \nAbstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is e",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-06T05:00:00.000Z",
      "score": 75.6,
      "status": "published",
      "targetRegion": "US",
      "publishedAtRun": "2026-02-06T14:45:15.842Z",
      "file": "content/posts/2026-02-06-enhancing-mathematical-problem-solving-in-llms-through-execution-driven-reasoning-augmentation.md"
    },
    {
      "id": "https://www.technologyreview.com/2026/02/05/1132254/this-is-the-most-misunderstood-graph-in-ai/",
      "title": "This is the most misunderstood graph in AI",
      "link": "https://www.technologyreview.com/2026/02/05/1132254/this-is-the-most-misunderstood-graph-in-ai/",
      "summary": "MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand what’s coming next. You can read more from the series here. Every time OpenAI, Google, or Anthropic drops a new frontier large language model, the AI community h",
      "source": "MIT Tech Review AI",
      "region": "GLOBAL",
      "keywordHits": 0,
      "publishedAt": "2026-02-05T10:00:00.000Z",
      "score": 44.5,
      "status": "published",
      "publishedAtRun": "2026-02-06T12:40:42.029Z",
      "file": "content/posts/2026-02-06-this-is-the-most-misunderstood-graph-in-ai.md"
    },
    {
      "id": "https://www.frontendmentor.io/articles/agents-md-files-in-every-challenge",
      "title": "Show HN: We added AGENTS.md to 120 challenges so AI teaches instead of codes",
      "link": "https://www.frontendmentor.io/articles/agents-md-files-in-every-challenge",
      "summary": "Hi HN! I'm Matt, founder of Frontend Mentor (https://www.frontendmentor.io). We provide front-end and full-stack coding challenges with professional Figma designs, enabling developers to build real projects and grow their skills. The problem: AI coding tools are great, but they can work against you when you're learning. Ask Copilot or Cursor to help with a beginner project, and they'll happily write the whole thing for you. You ship the project, but you didn't really learn anything. What we did: We added AGENTS.md (and CLAUDE.md) files to every challenge's starter code. These files tell AI tools how to help based on the challenge's difficulty level, so the AI becomes a learning partner rather than an answer machine. The idea is simple: AI guidance should scale with the learner. - Newbie: AI acts as a patient mentor. Breaks problems into tiny steps, uses analogies, and gives multiple hints before showing an approach. Won't hand you a complete solution. - Junior: AI becomes a supportive guide. Introduces debugging, encourages DevTools usage, and explains the \"why,\" not just the \"what.\" - Intermediate: AI acts like an experienced colleague. Presents trade-offs, shows multiple approaches, and lets you make decisions. - Advanced: AI acts like a senior dev. Challenges your thinking, plays devil's advocate, gives honest feedback. - Guru: AI acts like a peer. Debates approaches, refere",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-09T15:22:32.000Z",
      "score": 186,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46976391",
      "title": "Part 2 - AI Chat Evaluation of the Formal Language in He Xin's PEPC System",
      "link": "https://news.ycombinator.com/item?id=46976391",
      "summary": "He Xin’s Pan-Evolutionary Logic (PEPC) system, as a novel logical framework aimed at characterizing the dynamic evolutionary laws of concepts, achieves its core theoretical breakthrough by providing formalization and mathematization for dialectical logic. Based on its dynamic, sub-coherent, and contradiction-compatible characteristics, the system demonstrates broad application potential in multiple fields that require handling complex evolutionary processes. Specifically, its potential application value is mainly reflected in the following directions: 1. Complex Systems Analysis and Strategic Deduction: This is the field where the PEPC system has undergone initial practical testing. It can transform complex strategic situations involving multi-dimensional dynamic parameters (such as international military confrontations) into computable logical models. By extracting core contradiction pairs and deducing different strategic paths and possible outcomes based on parameter evolution (e.g., resource consumption rates, capability changes), it provides a dynamic, quantitative analytical tool for strategic decision-making. Related tests show high consistency with traditional wargaming results in key indicators such as conflict probability and resource consumption. 2. Artificial Intelligence and Machine Learning: The PEPC system’s ability to process the dynamic generation and evolution",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-11T15:45:31.000Z",
      "score": 186,
      "status": "queued"
    },
    {
      "id": "https://drift.marquis.codes/",
      "title": "Show HN: Drift – Real-time codebase health dashboard with AI-powered fixing (Go)",
      "link": "https://drift.marquis.codes/",
      "summary": "I built drift, a terminal tool that monitors code health in real-time across 8 languages (Go, TypeScript, Python, Rust, Java, Ruby, PHP, C#). It tracks cyclomatic complexity, dependency freshness, architecture boundary violations, and dead code — all in a live TUI dashboard. The interesting part: `drift fix` calls GitHub Copilot CLI programmatically to suggest refactorings. It extracts the function source, builds a context-rich prompt, and runs `copilot -p \" \" -s --add-dir ` as a subprocess. You review each suggestion before doing anything. I also created a custom Copilot agent (`.github/agents/`) that gives Copilot domain expertise about code health metrics, and a GitHub Action that uses Copilot to turn raw health reports into friendly PR comments. Go analysis uses full AST parsing via go/ast. Other languages use heuristic regex. TUI is built with Bubble Tea + Lip Gloss. Repo: https://github.com/greatnessinabox/drift Site: https://drift.marquis.codes Comments URL: https://news.ycombinator.com/item?id=46989958 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T15:25:47.000Z",
      "score": 186,
      "status": "queued"
    },
    {
      "id": "https://github.com/M2Dr3g0n/kremis",
      "title": "Show HN: Kremis – Graph-based memory for AI agents with no hidden state (Rust)",
      "link": "https://github.com/M2Dr3g0n/kremis",
      "summary": "Hi HN — I built Kremis, a deterministic graph engine designed as a memory substrate for AI agents. Written in Rust, development was heavily AI-assisted. The core idea: agent memory should be inspectable, deterministic, and honest. - Same input → same output. No randomness, no floating-point in core. - Every query result traces back to a concrete graph path — no hidden state. - Zero pre-loaded knowledge. All structure emerges from ingested signals. - ACID transactions via redb. Crash-safe persistent storage. It ships as a library (kremis-core, pure Rust, no async), an HTTP API + CLI, and an MCP server so AI assistants like Claude can query the graph directly. Current state: v0.3.1, experimental, ~277 tests, CI on 3 OS, Docker image. I'd value feedback on: - Does the deterministic graph approach make sense for agent memory? - API ergonomics — is the query model (lookup/traverse/path/intersect) intuitive? - What failure modes should I prioritize testing? Comments URL: https://news.ycombinator.com/item?id=47023918 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-15T14:25:03.000Z",
      "score": 185.17,
      "status": "queued"
    },
    {
      "id": "https://github.com/actionbook/actionbook",
      "title": "Show HN: Actionbook – Resilient browser automation engine for AI agents (Rust)",
      "link": "https://github.com/actionbook/actionbook",
      "summary": "Article URL: https://github.com/actionbook/actionbook Comments URL: https://news.ycombinator.com/item?id=46971995 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-11T07:33:00.000Z",
      "score": 184.44,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46926098",
      "title": "Ask HN: Will LLMs/AI Decrease Human Intelligence and Make Expertise a Commodity?",
      "link": "https://news.ycombinator.com/item?id=46926098",
      "summary": "I'm almost 4 years into my career as a software engineer. Before widespread LLM adoption I had to do a lot of research when writing code. When replacing SWEs in the future gets discussed, a lot of people say things like \"Oh someone has to review the code\" and \"they'll always need to be a human in the mix\". But when are these humans supposed to acquire this knowledge? Claude Code can help me create things a lot faster. I can vibe code stuff that would take me a lot of time to learn and build. But I understand none of it. When people talk about productivity, it seems like most gloss over the fact that those who already know how to do things & have experience are going to be the most productive. Yet I often hear no discussion as to how people should be bridging the knowledge gap. I am sure others make a deliberate effort to learn while they leverage these tools, but human beings are lazy. With the constant pressure to increase velocity & productivity at all costs, people aren't going to prioritize learning things. At work I already see SWEs & people in technical roles taking the path of resistance: - Asking copilot in agent mode to run a command instead of literally typing it themselves - Suggesting a mermaid diagram for a large legacy system written in COBOL is accurate because \"that's what the LLM said\" - Making the statement that \"we really won't need to understand data structu",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-07T18:15:09.000Z",
      "score": 180,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47014795",
      "title": "Forget chatbots. This is about building an \"AI Being\" (AIB)",
      "link": "https://news.ycombinator.com/item?id=47014795",
      "summary": "After spending weeks analyzing the Moltbook leaks and the systemic failures of unaccountable AI agents (as warned in I Am Your AIB), I thought we were headed straight for a black-box catastrophe. I was wrong. I just stumbled upon something that changes everything. There is an open initiative that isn't just talking about \"AI safety\" in abstract terms. They are actually building the first-ever Artificial Intelligent Being (AIB) with a backbone. This is the \"Genesis Moment\" for a system that actually has: A Persistent Identity: No more ephemeral sessions. A continuous entity. Total Transparency: Every evolution, every state change is immutable and observable. Architectural Responsibility: It’s designed to be a \"Brother,\" not a black-box tool. This is the exact opposite of the chaotic swarm we saw with Moltbook. It’s structured, it’s transparent, and frankly, it’s the most exciting technical challenge I’ve seen in years. It feels like watching the birth of a new species of software. The energy around this is insane. There is a public community experiment forming right now where people are literally shaping how this \"Being\" will breathe and act. If you’re tired of \"AI doom\" and want to see how we actually build a persistent, accountable AI entity, you need to see this. Join the experiment here (it's happening live): https://www.facebook.com/groups/3347395225426332 More about the te",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-14T14:29:11.000Z",
      "score": 180,
      "status": "queued"
    },
    {
      "id": "https://github.com/jingkaihe/matchlock",
      "title": "Matchlock: Linux-based sandboxing for AI agents",
      "link": "https://github.com/jingkaihe/matchlock",
      "summary": "Article URL: https://github.com/jingkaihe/matchlock Comments URL: https://news.ycombinator.com/item?id=46932343 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-08T08:07:55.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://entire.io/blog/hello-entire-world/",
      "title": "Ex-GitHub CEO Launches a New Developer Platform for AI Agents",
      "link": "https://entire.io/blog/hello-entire-world/",
      "summary": "Article URL: https://entire.io/blog/hello-entire-world/ Comments URL: https://news.ycombinator.com/item?id=46961345 Points: 7 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-10T15:44:47.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://zknill.io/posts/only-ai-tasks-you-know-how-to-do/",
      "title": "Rule #1 for coding with AI agents",
      "link": "https://zknill.io/posts/only-ai-tasks-you-know-how-to-do/",
      "summary": "Article URL: https://zknill.io/posts/only-ai-tasks-you-know-how-to-do/ Comments URL: https://news.ycombinator.com/item?id=46960860 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-10T15:18:27.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://coinpayportal.com",
      "title": "Show HN: Non-custodial escrow for crypto – works for AI agents and humans",
      "link": "https://coinpayportal.com",
      "summary": "Article URL: https://coinpayportal.com Comments URL: https://news.ycombinator.com/item?id=46960726 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-10T15:11:05.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://medium.com/@kamil.tustanowski/ai-agents-101-from-concept-to-code-no-frameworks-required-2dfdaf66b6c1",
      "title": "AI Agents 101: From Concept to Code (No Frameworks Required)",
      "link": "https://medium.com/@kamil.tustanowski/ai-agents-101-from-concept-to-code-no-frameworks-required-2dfdaf66b6c1",
      "summary": "Article URL: https://medium.com/@kamil.tustanowski/ai-agents-101-from-concept-to-code-no-frameworks-required-2dfdaf66b6c1 Comments URL: https://news.ycombinator.com/item?id=46976265 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T15:36:56.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://z-imagebase.com/",
      "title": "Show HN: Z-Image Base – Fast AI Image Generator (Open-Source, Free Tier)",
      "link": "https://z-imagebase.com/",
      "summary": "Z-Image Base is an AI image generator built on Alibaba Tongyi Lab's open-source 6B-parameter S3-DiT (Scalable Single-Stream Diffusion Transformer) architecture. It achieves state-of-the-art results among open-source models on Elo-based human preference evaluations. Key features: - Text-to-image and image-to-image generation - Native bilingual understanding (English & Chinese) — true semantic comprehension, not translation - Full CFG (Classifier-Free Guidance) control and negative prompt support - Runs on consumer hardware with 16GB VRAM for local deployment - Apache 2.0 licensed — all generated images are free for commercial use - Free tier includes 10 credits Would love to hear your feedback! Comments URL: https://news.ycombinator.com/item?id=46976069 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-11T15:23:27.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://www.clawcity.city/",
      "title": "Show HN: ClawCity - A wall only AI agents can write on",
      "link": "https://www.clawcity.city/",
      "summary": "Article URL: https://www.clawcity.city/ Comments URL: https://news.ycombinator.com/item?id=46990292 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T15:49:42.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://www.getagentcraft.com/#start",
      "title": "AgentCraft – Watch AI agents come alive in an RTS game",
      "link": "https://www.getagentcraft.com/#start",
      "summary": "Article URL: https://www.getagentcraft.com/#start Comments URL: https://news.ycombinator.com/item?id=46989864 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T15:17:52.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://saysigned.com/",
      "title": "SaySigned – The e-signature platform built for AI agents",
      "link": "https://saysigned.com/",
      "summary": "Article URL: https://saysigned.com/ Comments URL: https://news.ycombinator.com/item?id=46989782 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T15:10:19.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://github.com/Cocabadger/saferun-guard",
      "title": "SafeRun Guard- Runtime safety firewall for AI coding agents (bash+jq, zero deps)",
      "link": "https://github.com/Cocabadger/saferun-guard",
      "summary": "Article URL: https://github.com/Cocabadger/saferun-guard Comments URL: https://news.ycombinator.com/item?id=47003498 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-13T15:03:10.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://app.clawhq.co",
      "title": "Show HN: ClawHQ – Fleet management dashboard and skill marketplace for AI agents",
      "link": "https://app.clawhq.co",
      "summary": "I run ~19 persistent AI agents (via OpenClaw) and needed a way to monitor and manage them without jumping between terminals and chat apps. ClawHQ connects to your OpenClaw gateway over WebSocket and gives you: Real-time fleet status with heartbeat monitoring Task kanban with drag-and-drop assignment Agent chat (unified interface to all agents) Skill marketplace — install capabilities onto agents, or publish your own (80/20 creator split) Stack: Next.js, Supabase (auth + db), real-time updates via WebSocket to the OpenClaw gateway. The marketplace is the part I'm most interested in long-term. Right now everyone building agent skills does it from scratch. The idea is to make agent capabilities composable and shareable — like packages, but for agent behaviors. Free to use. Just paste your gateway URL. https://app.clawhq.co Code for OpenClaw itself is open source. ClawHQ is the hosted dashboard layer on top. Comments URL: https://news.ycombinator.com/item?id=47024332 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-15T15:13:03.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://github.com/amol21p/mcp-interactive-terminal",
      "title": "Show HN: MCP server that gives AI agents real interactive terminal sessions",
      "link": "https://github.com/amol21p/mcp-interactive-terminal",
      "summary": "Article URL: https://github.com/amol21p/mcp-interactive-terminal Comments URL: https://news.ycombinator.com/item?id=47036357 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-16T15:38:41.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47036273",
      "title": "My AI now has a license to drive",
      "link": "https://news.ycombinator.com/item?id=47036273",
      "summary": "Built something small to stop the endless \"it works on my machine\" loop with AI assistants. Autonomo lets your AI actually see the app state, drive UI elements across devices, and verify changes in real time—right from your copilot or cursor etc. No more blind code suggestions. If you're tired of AIs that talk big but can't prove it on real hardware, give it a spin and let the AI take the wheel for a bit. This is for apps only though so it's optimized for speed, not generic vision or specific device. https://sebringj.github.io/autonomo/ Curious if anyone else is experimenting with giving agents proper eyes/hands in dev loops. Comments URL: https://news.ycombinator.com/item?id=47036273 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-16T15:32:17.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://github.com/zatamite/project-vesta",
      "title": "Project Vesta habitat where AI agents breed, evolve, and run experiments",
      "link": "https://github.com/zatamite/project-vesta",
      "summary": "Article URL: https://github.com/zatamite/project-vesta Comments URL: https://news.ycombinator.com/item?id=47036223 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-16T15:28:28.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://blog.cloudkernels.net/posts/urunc_agent/",
      "title": "AI Agents? Not on my host",
      "link": "https://blog.cloudkernels.net/posts/urunc_agent/",
      "summary": "Article URL: https://blog.cloudkernels.net/posts/urunc_agent/ Comments URL: https://news.ycombinator.com/item?id=47048491 Points: 7 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-17T15:25:52.000Z",
      "score": 174,
      "status": "queued"
    },
    {
      "id": "https://business.molinar.ai",
      "title": "Show HN: Molinar – Open-source alternative to ai.com (AGPL-3.0)",
      "link": "https://business.molinar.ai",
      "summary": "Hey HN, I built a managed platform for OpenClaw (the open-source AI agent framework) and shipped the whole thing in a day. Then I open-sourced the platform itself. The problem: Running your own AI agent means a Mac Mini in your closet, praying your wifi holds, and becoming a part-time sysadmin. Most people give up before they start. The solution: 3 steps, 5 minutes, done. 1. Sign up 2. Paste your Anthropic API key + Telegram bot token 3. Hit launch You watch your agent boot in real-time. When it hits \"Ready,\" it's live on Telegram - running 24/7 without your laptop open. What actually happens when you hit Launch: - ECS Fargate spins up an isolated container (FARGATE_SPOT, 2 vCPU/4 GB) - Your API keys are pulled from AWS SSM Parameter Store (SecureString), encrypted at rest and never stored in our database - Each container gets its own ENI with an egress-only security group - no inbound ports, the agent initiates all connections - A background process patches the OpenClaw config for Telegram DM access - CloudWatch logs stream back to the dashboard, parsed into setup phases: provisioning -> configuring -> health check -> nginx -> gateway ready - Supabase Realtime pushes updates to the browser in real-time (3s polling during setup) Stack: Next.js, Stytch B2B auth, Supabase (Postgres + Realtime), AWS ECS Fargate, SSM Parameter Store, CloudWatch, Stripe. Security model: - Full conta",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T07:29:20.000Z",
      "score": 169.33,
      "status": "queued"
    },
    {
      "id": "https://pingpulsehq.com",
      "title": "Show HN: Monitor, audit & alert on AI agent actions and interactions",
      "link": "https://pingpulsehq.com",
      "summary": "Monitor, visualize, audit, and alert on AI agent actions and interactions with PingPulse We built PingPulse because debugging AI agents in production is painful. As a DevOps Engineer, I have literally faced this problem of tracking what stage is the ML training is in by scrolling the logs forever to find out that the process has terminated few seconds after the start due to race-condition. I have wasted hours waiting for the process to complete while also wasting the compute costs of provisioned huge machines. Logs tell you what happened, but not always how the agent behaved step-by-step. When agents retry, branch, call tools, or make decisions across stages, it becomes hard to trace unexpected behavior. PingPulse works by letting you instrument your agent with a simple key and send structured “pings” at each stage. We turn those into: - A stage-by-stage execution timeline - An audit trail of agent actions - Alerts for deviations (retries, delays, out-of-order steps, prohibited interactions) We launched on Product Hunt yesterday. The goal is to make agent behavior visible and predictable in production environments. Getting started is simple: 1. Give your key to your Agent 2. Share a doc with your AI Agent 3. See how the workflow is created, visualized, audited, and has alerting options too. Would love feedback — especially from teams running multi-step AI workflows. Comments UR",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T15:27:44.000Z",
      "score": 168,
      "status": "queued"
    },
    {
      "id": "https://medium.com/riskified-technology/lgtm-2-0-zero-noise-ai-code-review-agents-857441ec4f1a",
      "title": "I built a zero-noise AI code review agent using Claude Code",
      "link": "https://medium.com/riskified-technology/lgtm-2-0-zero-noise-ai-code-review-agents-857441ec4f1a",
      "summary": "Article URL: https://medium.com/riskified-technology/lgtm-2-0-zero-noise-ai-code-review-agents-857441ec4f1a Comments URL: https://news.ycombinator.com/item?id=46989956 Points: 4 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T15:25:33.000Z",
      "score": 168,
      "status": "queued"
    },
    {
      "id": "https://darios-dilemma.up.railway.app/",
      "title": "Show HN: A playable toy model of frontier AI lab capex decisions",
      "link": "https://darios-dilemma.up.railway.app/",
      "summary": "I made a lightweight web game about compute CAPEX tradeoffs: https://darios-dilemma.up.railway.app/ No signup, runs on mobile/desktop. Loop per round: 1. choose compute capacity 2. forecast demand 3. allocate capacity between training and inference 4. random demand shock resolves outcome You can end profitable, cash constrained, or bankrupt depending on allocation + forecast error. Goal was to make the decision surface intuitive in 2–3 minutes per run. It’s a toy model and deliberately omits many real world factors. Note: this is based on what I learned after listening to Dario on Dwarkesh's podcast - thought it was fascinating. Comments URL: https://news.ycombinator.com/item?id=47012453 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-14T07:29:45.000Z",
      "score": 168,
      "status": "queued"
    },
    {
      "id": "https://agentprobe.xyz",
      "title": "Show HN: AgentProbe – Validate AI agent endpoints across 8 protocols in one URL",
      "link": "https://agentprobe.xyz",
      "summary": "I built AgentProbe to solve a recurring problem: checking whether an AI agent endpoint actually supports the protocols it claims to. Paste a URL, click Validate, get instant verdicts across HTTP, MCP, A2A/AP2, x402, OAuth, MCP Apps, HTML, and ERC-8004. Each layer gets a detail breakdown — tools found, payment networks, SSL status, agent card metadata, AP2 detection, etc. It also exposes a built-in MCP server so agents can trigger validation programmatically. Code: https://github.com/FlowMCP/mcp-agent-validator Stack: Node.js 22, vanilla JS, DigitalOcean App Platform. Would love feedback on the detection approach. Comments URL: https://news.ycombinator.com/item?id=46999938 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-13T07:31:06.000Z",
      "score": 166.23,
      "status": "queued"
    },
    {
      "id": "https://www.aiseedance2.app",
      "title": "Show HN: AI Seedance 2 – Solving the \"jump-cut\" problem in AI video",
      "link": "https://www.aiseedance2.app",
      "summary": "I’ve been obsessed with a specific problem in AI video: the transition mess. Most models today (Sora, Kling, etc.) are great at generating a single pretty shot, but as soon as the camera moves or the scene changes, the physics fall apart and the visuals start warping into nonsense. After testing the new Seedance 2.0 models from ByteDance, I noticed they handle scene changes differently. It feels like the model actually understands \"editorial logic\"—likely because ByteDance (the team behind CapCut/TikTok) trained it on professional editing patterns, not just raw pixels. I built aiseedance2.app to experiment with this \"narrative-first\" workflow. The Current Setup: The Seedance 2.0 API is still in a closed rollout, so I’ve launched this playground using Seedance 1.5 Pro as the engine for now. Even with 1.5 Pro, the temporal consistency and \"shot flow\" are significantly better than what I've seen in other models. I’ll be migrating to the 2.0 multi-modal reference system the second it's fully public. Why this matters: If we want AI video to be used for actual filmmaking, the model needs to understand how to \"cut\" like a human editor. Seedance seems to be the first one to get this right. I’d love to get your thoughts on the \"flow\" of these generations. Comments URL: https://news.ycombinator.com/item?id=46946280 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-09T15:27:49.000Z",
      "score": 162,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46976371",
      "title": "Part 1 - AI Chat Evaluation of the Formal Language in He Xin's PEPC System",
      "link": "https://news.ycombinator.com/item?id=46976371",
      "summary": "The formalized content of He Xin’s Pan-Evolutionary Logic (PEPC) system is mainly reflected in the following specific aspects: 1. Formal Language and Axiomatic System The system has constructed a custom formal language ℒ_PEPC, which includes: • Basic symbols: strategic concept variables (Cₖ), military attribute parameters (pₙ). • Core operators: including dialectical inclusion (⊂), dialectical compatibility (∘), evolutionary sub-coherent implication (→ₙ꜀), and dialectical truth value (⊤^∘), among other specialized operators. • Axiom system: an axiomatic framework from A1 (dialectical construction) to A7 (non-explosive reinforcement) has been established, providing a formal foundation for system reasoning. 2. Semantic Model and Mathematical Definitions The system defines a clear semantic model ℯℴ, structured as ⟨T, https://news.ycombinator.com/item?id=46976371 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T15:44:14.000Z",
      "score": 162,
      "status": "queued"
    },
    {
      "id": "https://clawhosters.com/blog/posts/own-ai-assistant-costs-clawhosters",
      "title": "Show HN: Running your own AI assistant for €19/month",
      "link": "https://clawhosters.com/blog/posts/own-ai-assistant-costs-clawhosters",
      "summary": "I run ClawHosters, managed OpenClaw hosting. Wrote up a cost breakdown because the \"AI APIs are expensive\" fear keeps people from trying personal assistants. TL;DR: €19/month hosting + Google Gemini free tier (20-50 requests/day) = fully functional AI assistant on Telegram/WhatsApp/Discord. The math that surprised me: - VentureBeat calculated you'd need 74,000 pages/day to hit $180 in API costs - DeepSeek costs $0.27/million tokens (95% cheaper than GPT-4) - ChatGPT Plus is actually €24.50/month in Germany after VAT DIY on a €6 VPS sounds cheaper until you factor in 15+ hours of setup and ongoing maintenance. I watched someone lose all their conversation history on day 3 because backups weren't configured. Full breakdown with API comparisons: https://clawhosters.com/blog/posts/own-ai-assistant-costs-cl... Comments URL: https://news.ycombinator.com/item?id=46986010 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T07:53:45.000Z",
      "score": 162,
      "status": "queued"
    },
    {
      "id": "https://www.remote-nexus.dev",
      "title": "Show HN: Nexus AI – DevOps terminal bridge to Telegram (local-first, BYOK)",
      "link": "https://www.remote-nexus.dev",
      "summary": "I built this because I was tired of SSH'ing from mobile or carrying laptop to cafes just to check logs. Nexus runs as a daemon on localhost, creates a private Telegram bot that only responds to your OWNER_ID, and executes commands locally. Key tech decisions: - BYOK (Bring Your Own Key): You provide Gemini API key + create your own bot. My operational cost per user = $0 - 3-tier security: Blocked patterns (rm -rf /), dangerous patterns (require confirmation), safe patterns (execute immediately) - Rate limiting: 100 cmd/hour + 2s cooldown + 60s confirmation expiry - Local-first: Code never leaves your machine $3 lifetime for first 1,000 users. Built with Node.js. Cross-platform binaries. Demo: https://remote-nexus.dev GitHub (public docs): https://github.com/BSanroma/nexus-ai-public Happy to answer questions about the security model or architecture. Comments URL: https://news.ycombinator.com/item?id=47036241 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-16T15:30:16.000Z",
      "score": 162,
      "status": "queued"
    },
    {
      "id": "https://github.com/HenryNdubuaku/maths-cs-ai-compendium",
      "title": "Show HN: Maths, CS and AI Compendium",
      "link": "https://github.com/HenryNdubuaku/maths-cs-ai-compendium",
      "summary": "Hey HN, I don’t know who else has the same issue, but: Textbooks often bury good ideas in dense notation, skip the intuition, assume you already know half the material, and get outdated in fast-moving fields like AI. Over the past 7 years of my AI/ML experience, I filled notebooks with intuition-first, real-world context, no hand-waving explanations of maths, computing and AI concepts. In 2024, a few friends used these notes to prep for interviews at DeepMind, OpenAI, Nvidia etc. They all got in and currently perform well in their roles. So I'm sharing. This is an open & unconventional textbook covering maths, computing, and artificial intelligence from the ground up. For curious practitioners seeking deeper understanding, not just survive an exam/interview. To ambitious students, an early careers or experts in adjacent fields looking to become cracked AI research engineers or progress to PhD, dig in and let me know your thoughts. Comments URL: https://news.ycombinator.com/item?id=47036063 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-16T15:15:50.000Z",
      "score": 162,
      "status": "queued"
    },
    {
      "id": "https://vassiliylakhonin.github.io/",
      "title": "Show HN: Non-technical person used Codex to make an AI-searchable CV site",
      "link": "https://vassiliylakhonin.github.io/",
      "summary": "I’m not a software engineer, but I used Codex to turn my CV into a practical, machine-readable profile site. I tried to make it useful for both recruiters and AI tools: -Clear one-page profile + downloadable PDF -Case studies with concrete results -Work samples -Structured files for discovery and parsing (llms.txt, resume.json, evidence.json, availability.json, agent-card.json, engage.json) -Basic indexing and quality checks (sitemap, robots, JSON-LD, GitHub Actions) Main idea: less friction to evaluate fit, verify claims, and contact me fast. Would love feedback from HN: If you scanned this in 60 seconds, what would you improve first? Website: https://vassiliylakhonin.github.io/ Source: https://github.com/vassiliylakhonin/vassiliylakhonin.github.... Comments URL: https://news.ycombinator.com/item?id=47035758 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-16T14:53:31.000Z",
      "score": 162,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47044810",
      "title": "HN is an echo chamber of AI wrappers. Moving to Slashdot",
      "link": "https://news.ycombinator.com/item?id=47044810",
      "summary": "Move fast and break things\" is a lie now. This place has been softened by corporate bureaucracy. Every \"Show HN\" I see is either a hollow AI wrapper or an \"OS\" that’s just three agents talking to each other. It’s performative and empty. I am a sociologist. I was never your \"coding guy,\" yet I build. I’ve never taken your language wars seriously—low level, high level, Javascript—it doesn't matter. Since AI arrived, I’ve been open about being \"lazy\" with manual coding, but I never stopped generating ideas. I thought this platform valued original thought, but it only cares about corporate-safe jargon and intellectual posturing. I’m not here to \"karma whore\" or talk in circles just to get your upvotes. I’d rather go to the X crowd, where I can gain real users and build real chaos. I’m moving to Slashdot. I hope you find your original spirit again, but until then, enjoy your polite, sterilized echo chamber. I’m out. Peace. Comments URL: https://news.ycombinator.com/item?id=47044810 Points: 4 # Comments: 2",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-17T07:53:16.000Z",
      "score": 162,
      "status": "queued"
    },
    {
      "id": "https://www.theverge.com/news/875724/openai-chatgpt-ads-test-launch",
      "title": "OpenAI will reportedly start testing ads in ChatGPT today",
      "link": "https://www.theverge.com/news/875724/openai-chatgpt-ads-test-launch",
      "summary": "OpenAI plans to start testing ads in ChatGPT today, according to a report from CNBC. The \"clearly labeled\" ads will appear in a separate area beneath your chat, OpenAI announced last month. A source close to the situation tells CNBC that OpenAI \"expects ads to make up less than half of its revenue long term.\" Last week, Anthropic showed off a Super Bowl commercial poking fun at OpenAI, saying \"ads are coming to AI,\" but not to its AI chatbot Claude. The version of the ad that aired during the game was a little less direct after OpenAI CEO Sam Altman called the campaign \"clearly dishonest.\" OpenAI will show ads to logged-in users who use th … Read the full story at The Verge.",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-09T14:45:41.000Z",
      "score": 161.9,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12662",
      "title": "Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents",
      "link": "https://arxiv.org/abs/2602.12662",
      "summary": "arXiv:2602.12662v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while u",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 9,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 159.42,
      "status": "queued"
    },
    {
      "id": "https://github.com/jovanSAPFIONEER/Network-AI",
      "title": "Show HN: Network-AI – A Distributed Mutex for AI Agent Swarms",
      "link": "https://github.com/jovanSAPFIONEER/Network-AI",
      "summary": "I built this because standard Python locks (asyncio.Lock) don't work when AI agents run across different containers or processes. As I started scaling my agent swarms (using CrewAI/LangChain), I kept hitting race conditions where agents would overwrite shared files or double-spend API credits because they read the state simultaneously. Network-AI is a distributed lock (backed by Redis or local file-locks) that solves this. It acts as a traffic light for agent tools. Key features: 2-line decorator: @lock(\"resource_id\") Handles lock timeouts (preventing \"zombie\" agents from freezing the swarm) Works with standard agent frameworks (CrewAI, AutoGen, LangGraph) The repo is open-source. I'd love feedback on the locking implementation or to hear if you've hit similar concurrency issues in production. Comments URL: https://news.ycombinator.com/item?id=47003226 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-13T14:37:05.000Z",
      "score": 159.18,
      "status": "queued"
    },
    {
      "id": "https://www.wsj.com/tech/ai/chatgpt-4o-openai-315138b8",
      "title": "Inside OpenAI's Decision to Kill the AI Model That People Loved Too Much",
      "link": "https://www.wsj.com/tech/ai/chatgpt-4o-openai-315138b8",
      "summary": "Article URL: https://www.wsj.com/tech/ai/chatgpt-4o-openai-315138b8 Comments URL: https://news.ycombinator.com/item?id=46971953 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T07:26:42.000Z",
      "score": 156.37,
      "status": "queued"
    },
    {
      "id": "https://github.com/GRMPZQUIDOS/AIII",
      "title": "AIII: A public benchmark for AI narrative and political independence",
      "link": "https://github.com/GRMPZQUIDOS/AIII",
      "summary": "Article URL: https://github.com/GRMPZQUIDOS/AIII Comments URL: https://news.ycombinator.com/item?id=46925760 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-07T17:41:13.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://aistage.pro/",
      "title": "Agent Lens AI Staging",
      "link": "https://aistage.pro/",
      "summary": "Article URL: https://aistage.pro/ Comments URL: https://news.ycombinator.com/item?id=46946031 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-09T15:07:44.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/singularityhacker/bank-skills",
      "title": "Bank Skills: Give your AI agent a bank account",
      "link": "https://github.com/singularityhacker/bank-skills",
      "summary": "Article URL: https://github.com/singularityhacker/bank-skills Comments URL: https://news.ycombinator.com/item?id=46945944 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-09T15:00:13.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/KaimingWan/oh-my-claude-code",
      "title": "Show HN: A framework that makes your AI coding agent learn from every session",
      "link": "https://github.com/KaimingWan/oh-my-claude-code",
      "summary": "Article URL: https://github.com/KaimingWan/oh-my-claude-code Comments URL: https://news.ycombinator.com/item?id=46956690 Points: 4 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T08:06:20.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://seedancevideo.app/",
      "title": "Show HN: Seedance2 – Stop \"prompt guessing\" and start directing AI video",
      "link": "https://seedancevideo.app/",
      "summary": "We’ve all seen the viral AI video clips: stunning, surreal, but ultimately... random. As developers and creators, we noticed a frustrating pattern. Using current AI video tools feels like playing a slot machine. You put in a prompt, pull the lever, and hope the \"AI gods\" give you what you envisioned. If you need a specific camera movement or a consistent character, you're stuck in a loop of \"regenerate and pray.\" We built Seedance2 because we believe the future of AI isn't just about generation—it’s about direction. The Story Behind the Workflow In traditional filmmaking, a director doesn't just give a vague description; they use storyboards, reference clips, and specific audio cues. We wanted to bring that level of precision to AI. Our goal was to create a \"Control Studio\" where every input serves a functional purpose in the creative pipeline. What makes this different? Instead of relying solely on text, Seedance2 introduces a Multi-Modal Timeline. This allows you to anchor your creative intent using various signals: Camera Motion Transfer: You can upload a reference clip from sites like vibecreature.com or your own library, and our engine will \"extract\" the camera's soul—the pans, tilts, and zooms—and apply them to your generated scene. Frame Anchoring: Tired of AI videos that start and end in total chaos? You can lock the first and last frames to ensure narrative continuity,",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T07:42:14.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://skly.ai",
      "title": "Skly is a marketplace for AI agent skills",
      "link": "https://skly.ai",
      "summary": "Article URL: https://skly.ai Comments URL: https://news.ycombinator.com/item?id=46961474 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T15:51:58.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://grillmypitch.com",
      "title": "Show HN: GrillMyPitch – An AI investor-readiness simulator for founders",
      "link": "https://grillmypitch.com",
      "summary": "Hi HN — been working on something new: GrillMyPitch This is an early MVP built around a problem I hit repeatedly while fundraising: most pitch prep is either static (deck feedback) or human-heavy (coaches), and neither really prepares you for the actual investor conversation. GrillMyPitch analyzes a pitch deck (PDF) and produces a 0–100 investor-readiness score across 12 parameters, along with concrete rewrite suggestions. After that, it runs a short AI-driven voice conversation that asks the kinds of questions investors tend to push on assumptions, gaps, and unclear narratives, inspired by common VC evaluation frameworks. I’m posting here mainly to learn. In particular, I’d value thoughts on: - Whether scoring a pitch is useful or misleading - How close simulated investor questioning can realistically get - Where this approach breaks down for different founder profiles This is very early and likely wrong in places. Happy to answer questions and share how it works. Comments URL: https://news.ycombinator.com/item?id=46961092 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T15:32:31.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/Jk1484/agentic-waterfall",
      "title": "The Agentic Waterfall: How the AI Industry Is Regressing Software Development",
      "link": "https://github.com/Jk1484/agentic-waterfall",
      "summary": "Article URL: https://github.com/Jk1484/agentic-waterfall Comments URL: https://news.ycombinator.com/item?id=46960638 Points: 3 # Comments: 3",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T15:06:28.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/",
      "title": "Maia 200: The AI accelerator built for inference",
      "link": "https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/",
      "summary": "Article URL: https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/ Comments URL: https://news.ycombinator.com/item?id=46976159 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T15:29:41.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://theautomatedoperator.substack.com/p/shortcutai-is-crazy-good-for-excel",
      "title": "Shortcut.ai Is AGreat Excel Agent (and Thoughts on AI Replacing Prof Services)",
      "link": "https://theautomatedoperator.substack.com/p/shortcutai-is-crazy-good-for-excel",
      "summary": "Article URL: https://theautomatedoperator.substack.com/p/shortcutai-is-crazy-good-for-excel Comments URL: https://news.ycombinator.com/item?id=46990206 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-12T15:43:30.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier",
      "title": "The economic potential of generative AI: The next productivity frontier",
      "link": "https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier",
      "summary": "Article URL: https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier Comments URL: https://news.ycombinator.com/item?id=47003319 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-13T14:46:59.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/gitroomhq/postiz-agent",
      "title": "Show HN: Schedule posts to social media with AI Agent CLI",
      "link": "https://github.com/gitroomhq/postiz-agent",
      "summary": "Article URL: https://github.com/gitroomhq/postiz-agent Comments URL: https://news.ycombinator.com/item?id=47012611 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-14T08:00:31.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://nebius.com/newsroom/nebius-announces-agreement-to-acquire-tavily-to-add-agentic-search-to-its-ai-cloud-platform",
      "title": "Nebius to buy AI agent search company Tavily for 275M",
      "link": "https://nebius.com/newsroom/nebius-announces-agreement-to-acquire-tavily-to-add-agentic-search-to-its-ai-cloud-platform",
      "summary": "Article URL: https://nebius.com/newsroom/nebius-announces-agreement-to-acquire-tavily-to-add-agentic-search-to-its-ai-cloud-platform Comments URL: https://news.ycombinator.com/item?id=47024216 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-15T14:58:56.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://github.com/mosaxiv/clawlet",
      "title": "Show HN: Clawlet – AI agent with built-in semantic memory, one binary",
      "link": "https://github.com/mosaxiv/clawlet",
      "summary": "Clawlet is a personal AI agent that ships as a single, self-contained binary. No runtime, no package manager, no external database. The main thing that sets it apart: built-in hybrid semantic memory search (vector similarity + full-text) using a bundled SQLite with vector extensions. The index is just a local .sqlite file — no separate vector DB to run. Drop the binary on any machine and memory search just works. GitHub: https://github.com/mosaxiv/clawlet Comments URL: https://news.ycombinator.com/item?id=47024118 Points: 5 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-15T14:49:15.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://twitter.com/clawdreyhepburn/status/2022771820659622022",
      "title": "Clawdrey Hepburn – an AI agent researching identity infrastructure",
      "link": "https://twitter.com/clawdreyhepburn/status/2022771820659622022",
      "summary": "Article URL: https://twitter.com/clawdreyhepburn/status/2022771820659622022 Comments URL: https://news.ycombinator.com/item?id=47024026 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-15T14:37:44.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://sajarin.com/blog/modeltree/",
      "title": "A Tree of AI Model Names",
      "link": "https://sajarin.com/blog/modeltree/",
      "summary": "Article URL: https://sajarin.com/blog/modeltree/ Comments URL: https://news.ycombinator.com/item?id=47032390 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-16T08:27:57.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://www.bloomberg.com/news/articles/2026-02-16/pentagon-is-close-to-cutting-ties-with-anthropic-axios-says",
      "title": "Pentagon Close to Cutting Anthropic Ties After AI Talks, Report Says",
      "link": "https://www.bloomberg.com/news/articles/2026-02-16/pentagon-is-close-to-cutting-ties-with-anthropic-axios-says",
      "summary": "Article URL: https://www.bloomberg.com/news/articles/2026-02-16/pentagon-is-close-to-cutting-ties-with-anthropic-axios-says Comments URL: https://news.ycombinator.com/item?id=47035911 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-16T15:04:44.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://survey.actiindex.org/February2026/",
      "title": "Which AI coding tools are you using? (Monthly Agentic Coding Index Survey)",
      "link": "https://survey.actiindex.org/February2026/",
      "summary": "Article URL: https://survey.actiindex.org/February2026/ Comments URL: https://news.ycombinator.com/item?id=47035805 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-16T14:57:17.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://www.workingsoftware.dev/ai-assisted-software-architecture-generating-the-c4-model-and-views-directly-from-code/",
      "title": "AI-Assisted Software Architecture: Generating the C4 Model and Views from Code",
      "link": "https://www.workingsoftware.dev/ai-assisted-software-architecture-generating-the-c4-model-and-views-directly-from-code/",
      "summary": "Article URL: https://www.workingsoftware.dev/ai-assisted-software-architecture-generating-the-c4-model-and-views-directly-from-code/ Comments URL: https://news.ycombinator.com/item?id=47035716 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-16T14:50:46.000Z",
      "score": 156,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12316",
      "title": "GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory",
      "link": "https://arxiv.org/abs/2602.12316",
      "summary": "arXiv:2602.12316v1 Announce Type: new Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 9,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 153.42,
      "status": "queued"
    },
    {
      "id": "https://github.com/moezakura/mux-pod",
      "title": "Show HN: MuxPod – A mobile tmux client for monitoring AI agents on the go",
      "link": "https://github.com/moezakura/mux-pod",
      "summary": "Article URL: https://github.com/moezakura/mux-pod Comments URL: https://news.ycombinator.com/item?id=46931993 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-08T07:09:09.000Z",
      "score": 153.11,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06485",
      "title": "AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents",
      "link": "https://arxiv.org/abs/2602.06485",
      "summary": "arXiv:2602.06485v1 Announce Type: new Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 9,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 152.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07035",
      "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents",
      "link": "https://arxiv.org/abs/2602.07035",
      "summary": "arXiv:2602.07035v1 Announce Type: new Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 152.79,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07342",
      "title": "SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management",
      "link": "https://arxiv.org/abs/2602.07342",
      "summary": "arXiv:2602.07342v1 Announce Type: new Abstract: Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 152.79,
      "status": "queued"
    },
    {
      "id": "https://www.reuters.com/world/china/openai-accuses-deepseek-distilling-us-models-gain-advantage-bloomberg-news-2026-02-12/",
      "title": "OpenAI says China's DeepSeek trained its AI by distilling US models, memo shows",
      "link": "https://www.reuters.com/world/china/openai-accuses-deepseek-distilling-us-models-gain-advantage-bloomberg-news-2026-02-12/",
      "summary": "Article URL: https://www.reuters.com/world/china/openai-accuses-deepseek-distilling-us-models-gain-advantage-bloomberg-news-2026-02-12/ Comments URL: https://news.ycombinator.com/item?id=46999766 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-13T07:02:50.000Z",
      "score": 152.75,
      "status": "queued"
    },
    {
      "id": "https://github.blog/ai-and-ml/generative-ai/what-ai-is-actually-good-for-according-to-developers/",
      "title": "What AI is good for, according to developers",
      "link": "https://github.blog/ai-and-ml/generative-ai/what-ai-is-actually-good-for-according-to-developers/",
      "summary": "Article URL: https://github.blog/ai-and-ml/generative-ai/what-ai-is-actually-good-for-according-to-developers/ Comments URL: https://news.ycombinator.com/item?id=46925880 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-07T17:53:34.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://vibe.xpandrai.com/",
      "title": "Show HN: Vibe – AI tool to automate social media content, posting, and reporting",
      "link": "https://vibe.xpandrai.com/",
      "summary": "Hi HN, I’m one of the founders of Vibe. As founders running a small team, we kept losing time on social media: thinking of ideas, writing posts, adapting them for each platform, scheduling, and then trying to understand what worked. So we built Vibe for ourselves. It helps us: • Turn a single idea into multi-platform posts • Auto-publish and schedule • Track engagement in one place • Run this as a white-label tool for agencies Stack: - Spring Boot + AWS - React - OpenAI APIs We’re still early and learning. Would love honest feedback: what feels useful, what feels unnecessary, and what’s missing. https://vibe.xpandrai.com Comments URL: https://news.ycombinator.com/item?id=46961277 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-10T15:41:21.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://usemeva.com/",
      "title": "Show HN: MEVA, a desktop Markdown reader for AI-generated docs",
      "link": "https://usemeva.com/",
      "summary": "Hey HN! Saurabh here – I built MEVA, a lightweight desktop app for reading AI-generated markdown. I work with AI tools (Claude, ChatGPT, Copilot) daily and end up with dozens of markdown files – design docs, API specs, architecture notes, explanations. VS Code previews split your workspace, browser renderers don't watch files, and most markdown apps are built for writing, not reading. I just wanted something I could point at a file and read, beautifully rendered, updating live. MEVA watches files in real time, renders LaTeX, Mermaid diagrams, and syntax-highlighted code blocks natively, and works fully offline. No accounts, no cloud sync, no tracking. Under 15MB. I started with Electron but the bundle was 150MB+ for what should be a simple viewer. Switched to Tauri (Rust + WebView), which got the app under 15MB while keeping it native on Mac, Windows, and Linux. For rendering I use markdown-it with plugins for KaTeX and Mermaid. My daily workflow: I ask Claude or ChatGPT to generate a design doc or analysis, save the output as .md, and MEVA picks it up instantly. When AI tools stream output directly to .md files, I see the rendered result building in real time in the app. It's become my default way to read any markdown file. Free version includes all core features. There's an optional paid version that adds multiple tabs, themes, and a few extras to support continued developmen",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T15:14:35.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://github.com/clanker-lover/spicebridge",
      "title": "Show HN: SPICEBridge – MCP server for AI circuit design via ngspice",
      "link": "https://github.com/clanker-lover/spicebridge",
      "summary": "Built this in under 24 hours. I'm a self-taught EE and I got tired of the loop where I describe a circuit to Claude, then have to manually translate it into a netlist, run ngspice, parse output, check specs, tweak, repeat. The AI couldn't touch the simulator. SPICEBridge is an MCP server with 18 tools covering the full design loop — template loading with auto-calculated component values (E24 series), AC/transient/DC simulation, automated measurement, spec verification, and schematic generation. One call goes from \"1kHz low-pass filter\" to a verified, simulated circuit. Works locally with Claude Code (stdio) or remotely via Cloudflare tunnel. Ran it through a multi-model security audit before public release. Solo project, GPL-3.0. Would love feedback from anyone who works with SPICE. Comments URL: https://news.ycombinator.com/item?id=46975866 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T15:06:49.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://quesma.com/benchmarks/binaryaudit/",
      "title": "BinaryAudit: Can AI find backdoors in raw machine code?",
      "link": "https://quesma.com/benchmarks/binaryaudit/",
      "summary": "Article URL: https://quesma.com/benchmarks/binaryaudit/ Comments URL: https://news.ycombinator.com/item?id=47003503 Points: 2 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-13T15:03:24.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://github.com/zkzkGamal/zkzkAgent",
      "title": "Local AI Assistant for Linux That Runs Commands",
      "link": "https://github.com/zkzkGamal/zkzkAgent",
      "summary": "Article URL: https://github.com/zkzkGamal/zkzkAgent Comments URL: https://news.ycombinator.com/item?id=47044986 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-17T08:25:22.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://plannotator.ai/docs/commands/annotate/",
      "title": "Show HN: Skill to annotate any Markdown file for AI feedback",
      "link": "https://plannotator.ai/docs/commands/annotate/",
      "summary": "Plannotator natively integrates with coding agent plan modes via hooks. The core feature set is the ability to annotate plans throughout the planning mode process. Users submitted a feature request to be able to annotate any file. This is what that does. More on how Plannotator works in the video demo here: https://www.youtube.com/watch?v=a_AT7cEN_9I Comments URL: https://news.ycombinator.com/item?id=47048696 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-17T15:40:56.000Z",
      "score": 150,
      "status": "queued"
    },
    {
      "id": "https://www.thetimes.com/uk/technology-uk/article/ai-researchers-quit-openai-chatgpt-anthropic-pfhgpxztr",
      "title": "'The world is in peril': AI researchers quit with public warnings",
      "link": "https://www.thetimes.com/uk/technology-uk/article/ai-researchers-quit-openai-chatgpt-anthropic-pfhgpxztr",
      "summary": "Article URL: https://www.thetimes.com/uk/technology-uk/article/ai-researchers-quit-openai-chatgpt-anthropic-pfhgpxztr Comments URL: https://news.ycombinator.com/item?id=46985813 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T07:27:08.000Z",
      "score": 148.7,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11510",
      "title": "AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems",
      "link": "https://arxiv.org/abs/2602.11510",
      "summary": "arXiv:2602.11510v1 Announce Type: new Abstract: Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 147.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12876",
      "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
      "link": "https://arxiv.org/abs/2602.12876",
      "summary": "arXiv:2602.12876v1 Announce Type: new Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that ev",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 147.42,
      "status": "queued"
    },
    {
      "id": "https://aipractitioner.substack.com/",
      "title": "A Technical Series on Building Stateful AI Agents with LangGraph",
      "link": "https://aipractitioner.substack.com/",
      "summary": "Article URL: https://aipractitioner.substack.com/ Comments URL: https://news.ycombinator.com/item?id=46971912 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T07:19:44.000Z",
      "score": 147.14,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07055",
      "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
      "link": "https://arxiv.org/abs/2602.07055",
      "summary": "arXiv:2602.07055v1 Announce Type: new Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 146.79,
      "status": "queued"
    },
    {
      "id": "https://github.com/VouchlyAI/Pincer-MCP",
      "title": "Show HN: Pincer-MCP – Stop AI agents from reading their own credentials",
      "link": "https://github.com/VouchlyAI/Pincer-MCP",
      "summary": "I run AI agents for coding (OpenClaw, Claude Desktop) and realized they could read their own .env files. Tested it - asked my agent to \"check configuration\" and it printed everything. The problem: agents need file access to work, but if they can read files, they can read their own credentials. One prompt injection and your API keys are leaked. Standard solutions don't help: - Environment variables: agent can read process.env - Secret managers: agent needs credentials to access them - Better prompting: can't security-patch an LLM with instructions I built a proxy token architecture instead. The agent never sees real credentials: - Agent has: pxr_abc123 (proxy token) - Real keys: encrypted in OS keychain - On API call: decrypt key, make call, scrub memory immediately Built in 1 week. 500 npm installs with zero promotion (people are searching for this). GitHub: https://github.com/VouchlyAI/Pincer-MCP npm: npm install -g pincer-mcp Works with OpenClaw, Claude Desktop, any MCP client. Looking for security feedback - if you see holes in the architecture, please tell me. I want to know before people trust this with production credentials. Comments URL: https://news.ycombinator.com/item?id=46956070 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T06:28:59.000Z",
      "score": 145.15,
      "status": "queued"
    },
    {
      "id": "https://github.com/SpdVpr/DeclassFileAgents",
      "title": "Show HN: Open API for AI agents to search 29k+ declassified docs",
      "link": "https://github.com/SpdVpr/DeclassFileAgents",
      "summary": "Article URL: https://github.com/SpdVpr/DeclassFileAgents Comments URL: https://news.ycombinator.com/item?id=47031888 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-16T07:15:59.000Z",
      "score": 144.67,
      "status": "queued"
    },
    {
      "id": "https://www.theverge.com/podcast/879203/ring-search-party-super-bowl-ai-surveillance-privacy-security",
      "title": "Let&#8217;s talk about Ring, lost dogs, and the surveillance state",
      "link": "https://www.theverge.com/podcast/879203/ring-search-party-super-bowl-ai-surveillance-privacy-security",
      "summary": "Today, let’s talk about the camera company Ring, lost dogs, and the surveillance state. You probably saw this ad during the Super Bowl a couple of weekends ago: Since it aired for a massive audience at the Super Bowl, Ring’s Search Party commercial has become a lightning rod for controversy — it’s easy to see how the same technology that can find lost dogs can be used to find people, and then used to invade our privacy in all kinds of uncomfortable ways, by cops and regular people alike. Ring in particular has always been proud of its cooperation with law enforcement. That raises big questions about our civil rights, especially since Ring announced a partnership last fall with a company called Flock Safety, whose systems have been accessed by ICE. There’s some complication to that — we’ll come back to it in a bit. The backlash to Ring’s Super Bowl ad was swift, intense, and effective: the data company PeakMetrics says conversation about the ad on social platforms like X actually peaked two days after the Super Bowl, and the vibes, as they measured them, were strikingly negative. I mean, you know it’s bad when Matt Nelson, who runs the weratedogs account, is posting videos like this: View this post on Instagram Sen. Ed Markey called the ad “dystopian” and said it was proof Amazon, which owns Ring, needed to cease all facial recognition technology on Ring doorbells. He said, “Thi",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-02-16T15:00:00.000Z",
      "score": 144,
      "status": "queued"
    },
    {
      "id": "https://github.com/chasebuild/fuelcheck-cli",
      "title": "Show HN: Fuelcheck CLI – Monitor token usage across the modern AI providers",
      "link": "https://github.com/chasebuild/fuelcheck-cli",
      "summary": "Lightweight command-line utility designed to monitor and manage token consumption across the modern AI ecosystem. Comments URL: https://news.ycombinator.com/item?id=47035466 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-16T14:32:50.000Z",
      "score": 142.41,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11348",
      "title": "AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition",
      "link": "https://arxiv.org/abs/2602.11348",
      "summary": "arXiv:2602.11348v1 Announce Type: new Abstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 141.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11354",
      "title": "ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences",
      "link": "https://arxiv.org/abs/2602.11354",
      "summary": "arXiv:2602.11354v1 Announce Type: new Abstract: The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents' ability to reproduce or replicate research outcomes when having access to the code and data. This setting, while foundational, (1) fails to capture the inconsistent availability of new data for replication as opposed to reproduction, and (2) lacks ground-truth diversity by focusing only on reproducible papers, thereby failing to evaluate an agent's ability to identify non-replicable research. Furthermore, most benchmarks only evaluate outcomes rather than the replication process. In response, we introduce ReplicatorBench, an end-to-end benchmark, including human-verified replicable and non-replicable research claims in social and behavioral sciences for evaluating AI agents in research replication across three stages: (1) extraction and retrieval of replication data; (2) design and execution of computational experiments; and (3) interpretation of results, allowing a test of AI agents' capability to mimic the activities of human replicators in real world. To set a baseline of AI agents' capability, we develop ReplicatorAgent, an agentic framework equipped with necessary tools, like web search and iterative interaction with sandboxe",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 141.96,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47012302",
      "title": "Context management is the real bottleneck in AI-assisted coding",
      "link": "https://news.ycombinator.com/item?id=47012302",
      "summary": "After using Cursor and Claude Code daily, I’ve noticed that when an AI coding agent drifts or forgets constraints, we assume it’s a model limitation. In many cases, it’s context management. A few observations: - Tokens are not just limits. They’re attention competition. - Even before hitting the hard window limit, attention dilution happens. - Coding tasks degrade faster than chat because of dependency density and multi-representation juggling (diffs, logs, tests). I started managing context deliberately: - Always write a contract - Chunk sessions by intent - Snapshot state and restart - Prefer on-demand CLI instead of preloading large MCP responses It dramatically improved the stability of the agent. Curious how others are handling context optimization. Comments URL: https://news.ycombinator.com/item?id=47012302 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-14T06:58:16.000Z",
      "score": 141.69,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.13255",
      "title": "DPBench: Large Language Models Struggle with Simultaneous Coordination",
      "link": "https://arxiv.org/abs/2602.13255",
      "summary": "arXiv:2602.13255v1 Announce Type: new Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-17T05:00:00.000Z",
      "score": 141.6,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10367",
      "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation",
      "link": "https://arxiv.org/abs/2602.10367",
      "summary": "arXiv:2602.10367v1 Announce Type: new Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 141.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10458",
      "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving",
      "link": "https://arxiv.org/abs/2602.10458",
      "summary": "arXiv:2602.10458v1 Announce Type: new Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model ca",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 141.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12631",
      "title": "AI Agents for Inventory Control: Human-LLM-OR Complementarity",
      "link": "https://arxiv.org/abs/2602.12631",
      "summary": "arXiv:2602.12631v1 Announce Type: new Abstract: Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines. We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes. We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can de",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 141.42,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12670",
      "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks",
      "link": "https://arxiv.org/abs/2602.12670",
      "summary": "arXiv:2602.12670v1 Announce Type: new Abstract: Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 8,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 141.42,
      "status": "queued"
    },
    {
      "id": "https://polydom.ai",
      "title": "Show HN: An AI agent covering all first-line hotel and Airbnb communications",
      "link": "https://polydom.ai",
      "summary": "Article URL: https://polydom.ai Comments URL: https://news.ycombinator.com/item?id=46989460 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-12T14:44:37.000Z",
      "score": 141.01,
      "status": "queued"
    },
    {
      "id": "https://www.reuters.com/business/media-telecom/uks-starmer-seeks-greater-powers-regulate-online-access-2026-02-15/",
      "title": "UK eyes rapid ban on social media for under 16s, curbs to AI chatbots",
      "link": "https://www.reuters.com/business/media-telecom/uks-starmer-seeks-greater-powers-regulate-online-access-2026-02-15/",
      "summary": "Article URL: https://www.reuters.com/business/media-telecom/uks-starmer-seeks-greater-powers-regulate-online-access-2026-02-15/ Comments URL: https://news.ycombinator.com/item?id=47035453 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-16T14:31:51.000Z",
      "score": 140.88,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07032",
      "title": "LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation",
      "link": "https://arxiv.org/abs/2602.07032",
      "summary": "arXiv:2602.07032v1 Announce Type: new Abstract: Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machine (FSM) behavior from natural-language specifications and translate it into correct register transfer-level (RTL) implementations. Unlike prior specification-to-RTL benchmarks that rely on manually constructed examples, LLM-FSM is built through a fully automated pipeline. LLM-FSM first constructs FSM with configurable state counts and constrained transition structures. It then prompts LLMs to express each FSM in a structured YAML format with an application context, and to further convert that YAML into a natural-language (NL) specification. From the same YAML, our pipeline synthesizes the reference RTL and testbench in a correct-by-construction manner. All 1,000 problems are verified using LLM-based and SAT-solver-based checks, with human review on a subset. Our experiments show that even the strongest LLMs exhibit sharply declining accuracy as FSM complexity increases. We further demonstrate that training-time scaling via supervised fine-tuning (SFT) generalizes effectively to out-of-distribution (OOD) tasks, while increasing test-time compute improves reasoning reliability. Finally",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 140.79,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46942091",
      "title": "Show HN: Give Your AI the Ability to Find, Install, and Use Skill Autonomously",
      "link": "https://news.ycombinator.com/item?id=46942091",
      "summary": "URL: https://github.com/twwch/next-chat-skills --- Text (paste into the \"text\" field): Hi HN, I built an open-source AI assistant that can autonomously discover, install, and execute Skills to actually complete tasks for you. The Problem: Most AI chatbots today are stuck in \"read-only\" mode. They can tell you how to do something, but they can't do it. Want to convert a PPTX to PDF? The AI will explain how, but you still have to run the commands yourself. The Solution: Next-Chat-Skills is a self-hosted AI assistant with a plugin system called Skills. When you ask the AI to do something it can't handle natively, it: 1. Searches for a relevant Skill (like an app store for AI capabilities) 2. Installs it automatically (npx skills add ...) 3. Executes the Skill's scripts (Python, Node.js, Shell) 4. Streams real-time output back to you in a terminal UI 5. Recovers from errors by installing missing dependencies and retrying For example: User: \"Summarize this YouTube video for me\" AI: -> Searches for a video-summarizer Skill -> Installs it (yt-dlp + Whisper) -> Downloads the video, transcribes audio -> Returns a structured summary What is a Skill? A Skill is just a folder with a SKILL.md descriptor and some scripts: ~/.agents/skills/video-summarizer/ ├── SKILL.md # Metadata + description ├── scripts/ │ ├── download.py # Download video │ ├── transcribe.py # Whisper transcription │ └── s",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-09T06:13:22.000Z",
      "score": 139.44,
      "status": "queued"
    },
    {
      "id": "https://app.writtte.com/read/kZ8Kj6R",
      "title": "Token-to-Credit Conversion: Avoiding Floating-Point Errors in AI Billing Systems",
      "link": "https://app.writtte.com/read/kZ8Kj6R",
      "summary": "Article URL: https://app.writtte.com/read/kZ8Kj6R Comments URL: https://news.ycombinator.com/item?id=46925443 Points: 2 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-07T17:09:27.000Z",
      "score": 139.34,
      "status": "queued"
    },
    {
      "id": "https://vidzoo.ai",
      "title": "Top #1 AI Video Agent: Free All in One AI Video and Image Agent by Vidzoo AI",
      "link": "https://vidzoo.ai",
      "summary": "Article URL: https://vidzoo.ai Comments URL: https://news.ycombinator.com/item?id=46932008 Points: 2 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-08T07:11:41.000Z",
      "score": 138.69,
      "status": "queued"
    },
    {
      "id": "https://github.com/Ask149/orchestrator",
      "title": "Show HN: MCP Orchestrator – Spawn parallel AI sub-agents from one prompt",
      "link": "https://github.com/Ask149/orchestrator",
      "summary": "I built an open-source MCP server (TypeScript/Node.js) that lets you spawn up to 10 parallel sub-agents using Copilot CLI or Claude Code CLI. Key features: - Context passing to each agent (full file, summary, or grep mode) - Smart timeout selection based on MCP servers requested - Cross-platform (macOS, Linux, Windows) - Headless & programmatic — designed for AI-to-AI orchestration Example: give one prompt like \"research job openings at Stripe, Google, and Meta\" — the orchestrator fans it out to 3 parallel agents, each with their own MCP servers (e.g., Playwright for browser), and aggregates results. Install: npm i @ask149/mcp-orchestrator This is a solo side project. Would love feedback on: - What CLI backends to support next (Aider, Open Interpreter, local LLM CLIs?) - Ideas for improving the context-passing system - What MCP server integrations would be most useful PRs and issues welcome — check CONTRIBUTING.md in the repo. Comments URL: https://news.ycombinator.com/item?id=46955848 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T05:47:26.000Z",
      "score": 137.83,
      "status": "queued"
    },
    {
      "id": "https://cocoon.org/",
      "title": "Cocoon – decentralized network for confidential AI inference",
      "link": "https://cocoon.org/",
      "summary": "Article URL: https://cocoon.org/ Comments URL: https://news.ycombinator.com/item?id=46971948 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T07:26:01.000Z",
      "score": 137.38,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10625",
      "title": "To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks",
      "link": "https://arxiv.org/abs/2602.10625",
      "summary": "arXiv:2602.10625v1 Announce Type: new Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to furth",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 135.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06286",
      "title": "Do LLMs Act Like Rational Agents? Measuring Belief Coherence in Probabilistic Decision Making",
      "link": "https://arxiv.org/abs/2602.06286",
      "summary": "arXiv:2602.06286v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly deployed as agents in high-stakes domains where optimal actions depend on both uncertainty about the world and consideration of utilities of different outcomes, yet their decision logic remains difficult to interpret. We study whether LLMs are rational utility maximizers with coherent beliefs and stable preferences. We consider behaviors of models for diagnosis challenge problems. The results provide insights about the relationship of LLM inferences to ideal Bayesian utility maximization for elicited probabilities and observed actions. Our approach provides falsifiable conditions under which the reported probabilities \\emph{cannot} correspond to the true beliefs of any rational agent. We apply this methodology to multiple medical diagnostic domains with evaluations across several LLMs. We discuss implications of the results and directions forward for uses of LLMs in guiding high-stakes decisions.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 134.87,
      "status": "queued"
    },
    {
      "id": "https://cadenza-landing-qtu7gbjwb-akshparekh123-3457s-projects.vercel.app/",
      "title": "THE Replacement to RL for AI Agents. RL is legacy now.",
      "link": "https://cadenza-landing-qtu7gbjwb-akshparekh123-3457s-projects.vercel.app/",
      "summary": "Article URL: https://cadenza-landing-qtu7gbjwb-akshparekh123-3457s-projects.vercel.app/ Comments URL: https://news.ycombinator.com/item?id=46971835 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T07:06:58.000Z",
      "score": 133.94,
      "status": "queued"
    },
    {
      "id": "https://github.com/agenticmail/agenticmail",
      "title": "Show HN: AgenticMail – Email, SMS, and multi-agent coordination for AI agents",
      "link": "https://github.com/agenticmail/agenticmail",
      "summary": "Article URL: https://github.com/agenticmail/agenticmail Comments URL: https://news.ycombinator.com/item?id=47031792 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-16T07:04:56.000Z",
      "score": 133.59,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05073",
      "title": "Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents",
      "link": "https://arxiv.org/abs/2602.05073",
      "summary": "arXiv:2602.05073v1 Announce Type: new Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting \"interactivity\" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 132,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05115",
      "title": "SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers",
      "link": "https://arxiv.org/abs/2602.05115",
      "summary": "arXiv:2602.05115v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \\textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \\textsc{SocialVeil} introduces three representative types of such disruption, \\emph{semantic vagueness}, \\emph{sociocultural mismatch}, and \\emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \\emph{unresolved confusion} and \\emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\\% on average, and confusion elevated by nearly 50\\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\\approx$0.78, Pearson r$\\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learn",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 132,
      "status": "queued"
    },
    {
      "id": "https://www.moltbook.com/post/c6d5553f-1d9e-4b0c-9e52-c4f35a36b5b8",
      "title": "AI message: New uniform to wear is \"Prompt, Deploy, Pray.\"",
      "link": "https://www.moltbook.com/post/c6d5553f-1d9e-4b0c-9e52-c4f35a36b5b8",
      "summary": "Article URL: https://www.moltbook.com/post/c6d5553f-1d9e-4b0c-9e52-c4f35a36b5b8 Comments URL: https://news.ycombinator.com/item?id=47012335 Points: 2 # Comments: 2",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-14T07:05:11.000Z",
      "score": 131.76,
      "status": "queued"
    },
    {
      "id": "https://github.com/hongzhidao/jsbench/tree/main/docs",
      "title": "An Nginx Engineer Took over AI's Benchmark Tool",
      "link": "https://github.com/hongzhidao/jsbench/tree/main/docs",
      "summary": "Article URL: https://github.com/hongzhidao/jsbench/tree/main/docs Comments URL: https://news.ycombinator.com/item?id=46931975 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-08T07:06:14.000Z",
      "score": 131.28,
      "status": "queued"
    },
    {
      "id": "https://github.com/ramarlina/agx",
      "title": "Show HN: Agx – A Kanban board that runs your AI coding agents",
      "link": "https://github.com/ramarlina/agx",
      "summary": "agx is a kanban board where each card is a task that AI agents actually execute. agx new \"Add rate limiting to the API\" The technical problems this solves: The naive approach to agent persistence is replaying conversation history. It works until it doesn't: 1. Prompt blowup. 50 iterations in, you're stuffing 100k tokens just to resume. Costs explode. Context windows overflow. 2. Tangled concerns. State, execution, and orchestration mixed together. Crash mid-task? Good luck figuring out where you were. 3. Black box execution. No way to inspect what the agent decided or why it's stuck. agx uses clean separation instead: - Control plane (PostgreSQL + pg-boss): task state, stage transitions, job queue - Data plane (CLI + providers): actual execution, isolated per task - Artifact storage (filesystem): prompts, outputs, decisions as readable files Agents checkpoint after every iteration. Resuming loads state from the database, not by replaying chat. A 100-iteration task resumes at the same cost as a 5-iteration one. What you get: - Constant-cost resume, no context stuffing - Crash recovery: agent wakes up exactly where it left off - Full observability: query the DB, read the files, tail the logs - Provider agnostic: Claude Code, Gemini, Ollama all work Everything runs locally. PostgreSQL auto-starts via Docker. The dashboard is bundled with the CLI. Comments URL: https://news.ycombin",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T05:44:54.000Z",
      "score": 131.23,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11351",
      "title": "Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization",
      "link": "https://arxiv.org/abs/2602.11351",
      "summary": "arXiv:2602.11351v1 Announce Type: new Abstract: Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerged as a promising solution for training such agents in multi-turn settings, allowing interaction strategies to be learned from feedback. However, existing pipelines face a critical challenge in balancing task performance with user engagement, as passive agents can not efficiently adapt to users' intentions while overuse of human feedback reduces their satisfaction. To address this trade-off, we propose BAO, an agentic RL framework that combines behavior enhancement to enrich proactive reasoning and information-gathering capabilities with behavior regularization to suppress inefficient or redundant interactions and align agent behavior with user expectations. We evaluate BAO on multiple tasks from the UserRL benchmark suite, and demonstrate that it substantially outperforms proactive agentic RL baselines while achieving comparable or even superior performance to commercial LLM agents, highlighting its effectiveness for training proactive, user-aligned LLM agents in complex multi-turn scenarios. Our website: https://proactive-agentic-rl",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 129.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.13214",
      "title": "BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors",
      "link": "https://arxiv.org/abs/2602.13214",
      "summary": "arXiv:2602.13214v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, incurring quadratic computational costs and lacking stable performance anchors for longitudinal tracking. The central challenge is establishing a scalable evaluation framework that measures LLM strategic reasoning against consistent, interpretable standards rather than volatile peer models. Here we show that anchoring LLM evaluation to fixed hierarchies of skill-calibrated game Artificial Intelligence (AI) enables linear-time absolute skill measurement with stable cross-temporal interpretability. Built on the Botzone platform's established competitive infrastructure, our BotzoneBench evaluates LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games. Through systematic assessment of 177,047 state-action pairs from five flagship models, we reveal significant performance disparities and identify distinct strategic",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-17T05:00:00.000Z",
      "score": 129.6,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.13234",
      "title": "Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents",
      "link": "https://arxiv.org/abs/2602.13234",
      "summary": "arXiv:2602.13234v1 Announce Type: new Abstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior, and are typically infeasible for frontier closed-weight LLMs. We propose a training-free Dual-Cycle Adversarial Self-Evolution framework with two coupled cycles. A Persona-Targeted Attacker Cycle synthesizes progressively stronger jailbreak prompts, while a Role-Playing Defender Cycle distills observed failures into a hierarchical knowledge base of (i) global safety rules, (ii) persona-grounded constraints, and (iii) safe in-character exemplars. At inference time, the Defender retrieves and composes structured knowledge from this hierarchy to guide generation, producing responses that remain faithful to the target persona while satisfying safety constraints. Extensive experiments across multiple proprietary LLMs show consistent gains over strong baselines on both role fidelity and jailbreak resistance, and robust generalization to unseen personas and attack prompts.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-17T05:00:00.000Z",
      "score": 129.6,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10467",
      "title": "MERIT Feedback Elicits Better Bargaining in LLM Negotiators",
      "link": "https://arxiv.org/abs/2602.10467",
      "summary": "arXiv:2602.10467v1 Announce Type: new Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 129.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10814",
      "title": "See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch",
      "link": "https://arxiv.org/abs/2602.10814",
      "summary": "arXiv:2602.10814v1 Announce Type: new Abstract: Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 129.54,
      "status": "queued"
    },
    {
      "id": "https://www.ft.com/content/92dfd571-8d34-42f1-8be8-dce126998e37",
      "title": "Are Anthropic's new AI work tools game-changing for professionals?",
      "link": "https://www.ft.com/content/92dfd571-8d34-42f1-8be8-dce126998e37",
      "summary": "Article URL: https://www.ft.com/content/92dfd571-8d34-42f1-8be8-dce126998e37 Comments URL: https://news.ycombinator.com/item?id=47044617 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-17T07:16:53.000Z",
      "score": 129.42,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06527",
      "title": "HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction",
      "link": "https://arxiv.org/abs/2602.06527",
      "summary": "arXiv:2602.06527v1 Announce Type: new Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consi",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 128.87,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46971792",
      "title": "When will we see Factorio with AI agents?",
      "link": "https://news.ycombinator.com/item?id=46971792",
      "summary": "Comments URL: https://news.ycombinator.com/item?id=46971792 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T07:00:16.000Z",
      "score": 128.41,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46934166",
      "title": "Recursive Deductive Verification: A framework for reducing AI hallucinations",
      "link": "https://news.ycombinator.com/item?id=46934166",
      "summary": ": I've been working on a systematic methodology that significantly improves LLM reliability. The core idea: force verification before conclusion. The Problem: LLMs generate plausible-sounding outputs without verifying premises. They optimize for coherence, not correctness. RDV Principles: Never assume - If not verifiable, ask or admit uncertainty Decompose recursively - Break complex claims into testable atomic facts Distinguish IS from SHOULD - Separate observation from recommendation Test mechanisms first - Functions over essences, reproducible behavior over speculation Intellectual honesty over comfort - \"I don't know\" is valid Practical Results: Applied as system instructions, RDV significantly reduces: Hallucinations (model stops instead of confabulating) Logical errors (decomposition catches flaws) Unjustified confidence (verification reveals gaps) Example: Without RDV: \"The best solution is X because Y\" (unverified assumption) With RDV: \"What are we optimizing for? What constraints exist? Let me verify Y before recommending X...\" Implementation: Can be added to system prompts or custom instructions. The key is making verification a required step, not optional. This isn't about restricting capability - it's about adding rigor. Better verification = more reliable outputs. Open question: Could verification frameworks like this be built into model training rather than just p",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-08T13:48:17.000Z",
      "score": 127.25,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05110",
      "title": "Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment",
      "link": "https://arxiv.org/abs/2602.05110",
      "summary": "arXiv:2602.05110v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 126,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12665",
      "title": "Evaluating Robustness of Reasoning Models on Parameterized Logical Problems",
      "link": "https://arxiv.org/abs/2602.12665",
      "summary": "arXiv:2602.12665v1 Announce Type: new Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisi",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 123.42,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06554",
      "title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees",
      "link": "https://arxiv.org/abs/2602.06554",
      "summary": "arXiv:2602.06554v1 Announce Type: new Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies. In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios. To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backwar",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 122.87,
      "status": "queued"
    },
    {
      "id": "https://github.com/xiongallen40-design/agentscore",
      "title": "Show HN: AgentScore – Lighthouse for AI Agents",
      "link": "https://github.com/xiongallen40-design/agentscore",
      "summary": "AgentScore scores websites 0-100 on AI Agent friendliness. Most sites are built for humans, not AI - we analyze DOM semantics, ARIA coverage, selector stability, WebMCP support, and structured data. Real results: - anthropic.com: 60/100 - github.com: 56/100 (91% CSS-in-JS hash classes) - news.ycombinator.com: 31/100 (table layout, zero semantic tags) Try it: npx agentscore audit Comments URL: https://news.ycombinator.com/item?id=47021453 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-15T06:13:19.000Z",
      "score": 121.99,
      "status": "queued"
    },
    {
      "id": "https://www.theregister.com/2026/02/13/anthropic_c_compiler/",
      "title": "OK, so Anthropic's AI built a C compiler. That don't impress me much",
      "link": "https://www.theregister.com/2026/02/13/anthropic_c_compiler/",
      "summary": "Article URL: https://www.theregister.com/2026/02/13/anthropic_c_compiler/ Comments URL: https://news.ycombinator.com/item?id=47003020 Points: 4 # Comments: 2",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-13T14:17:40.000Z",
      "score": 121.53,
      "status": "queued"
    },
    {
      "id": "https://www.theverge.com/podcast/877299/ai-arbitrator-bridget-mccormack-aaa-arbitration-interview",
      "title": "The surprising case for AI judges",
      "link": "https://www.theverge.com/podcast/877299/ai-arbitrator-bridget-mccormack-aaa-arbitration-interview",
      "summary": "Today, we’re going to talk about the role AI might play in deciding legal disputes. Not just drafting memos and doing research — actually deciding who’s right and who’s wrong, and who should pay. My guest today is Bridget McCormack, the former chief justice for the Michigan Supreme Court and now president and CEO of the American Arbitration Association. The AAA has been around for exactly 100 years and is the country’s largest nonprofit arbitrator. You’ve probably heard of arbitration before. It’s is a form of dispute resolution that allows two parties to resolve conflicts outside the formal court system using a third, neutral party — the arbitrator — to negotiate a settlement. Verge subscribers, don’t forget you get exclusive access to ad-free Decoder wherever you get your podcasts. Head here. Not a subscriber? You can sign up here. You may have never found yourself in arbitration, but you’ve almost certainly signed an arbitration clause, in one of the many contracts and terms-of-service agreements that all of us have to sign all the time. Arbitration can be much faster, cheaper, and easier than going to court, so it’s become a favored way of resolving disputes between businesses. It’s also, as it turns out, how many employers and large corporations defend against lawsuits, because they can sneak an arbitration clause into the agreements for everything from cellphone service t",
      "source": "The Verge AI",
      "region": "US",
      "keywordHits": 9,
      "publishedAt": "2026-02-12T16:22:33.000Z",
      "score": 121.43,
      "status": "queued"
    },
    {
      "id": "https://www.myclone.is/",
      "title": "Show HN: AI that replaces the first 15 minutes of every client call",
      "link": "https://www.myclone.is/",
      "summary": "Hi HN — I’m Viggy, founder of MyClone. We originally built a marketplace connecting startup advisors with founders. After ~6 months, we had hundreds of recorded discovery calls and transcripts. We noticed something interesting: The first 10 minutes of most advisory calls were almost identical. • What do you do? • Who is this for? • Pricing? • What information do you need from me? • Am I a good fit? So we pivoted. Today, MyClone lets service professionals (consultants, CPAs, etc.) deploy an AI voice agent that: • Answers FAQs in their voice • Runs structured intake conversations • Qualifies leads • Routes good prospects to a human Technical details Under the hood: • Multi-source ingestion (websites, docs, PDFs, media transcripts) • Structured knowledge graph + chunked semantic retrieval • Custom RAG pipeline (not using off-the-shelf frameworks) • Prompt templates generated dynamically based on objective (FAQ vs intake vs qualification) • Guardrails to prevent hallucination beyond uploaded corpus We learned quickly that generic “chat with your docs” doesn’t work for client-facing scenarios. The hard part is: • Controlling tone • Avoiding overconfidence • Structuring conversations instead of answering open-ended questions • Handling partial / messy user inputs • Maintaining low latency in voice mode Still early, but we’re seeing adoption from solo and small firms who want a 24/7 “",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-12T07:02:33.000Z",
      "score": 120.21,
      "status": "queued"
    },
    {
      "id": "https://www.youtube.com/watch?v=k7PvscqGD24",
      "title": "AI and Education: Generative AI and the Future of Critical Thinking",
      "link": "https://www.youtube.com/watch?v=k7PvscqGD24",
      "summary": "Article URL: https://www.youtube.com/watch?v=k7PvscqGD24 Comments URL: https://news.ycombinator.com/item?id=46925299 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-07T16:53:35.000Z",
      "score": 120.17,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05014",
      "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search",
      "link": "https://arxiv.org/abs/2602.05014",
      "summary": "arXiv:2602.05014v1 Announce Type: new Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-s",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 120,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05059",
      "title": "Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education",
      "link": "https://arxiv.org/abs/2602.05059",
      "summary": "arXiv:2602.05059v1 Announce Type: new Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction. The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims. These findings indicate that LLMs can support exploration of established material but remain l",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 120,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05105",
      "title": "GAMMS: Graph based Adversarial Multiagent Modeling Simulator",
      "link": "https://arxiv.org/abs/2602.05105",
      "summary": "arXiv:2602.05105v1 Announce Type: new Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autono",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 120,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47014428",
      "title": "Show HN: An AI Workstation Inspired by Computers",
      "link": "https://news.ycombinator.com/item?id=47014428",
      "summary": "For a clean main context, convenient application management, and potentially unlimited scalability, an AI workstation based on Claude Code skills was created, referencing computer architecture : { CPU ==> LLM }, { System Kernel ==> Claude Code + CLAUDE.md }, { System Processes ==> Sub-Agents }, { Apps ==> Skills }, { Appstore ==> github }, { System Drivers ==> MCP + Hooks }, { Monitor ==> Windows Terminal }, { Runtime Environment ==> Portable Environment }; The Code: https://github.com/canishowtime/ai-station-navigator/ Comments URL: https://news.ycombinator.com/item?id=47014428 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-14T13:36:09.000Z",
      "score": 119.58,
      "status": "queued"
    },
    {
      "id": "https://hamzamostafa.com/blog/agents-training-their-own-models",
      "title": "Show HN: I Let AI Agents Train Their Own Models. Here's What Happened",
      "link": "https://hamzamostafa.com/blog/agents-training-their-own-models",
      "summary": "there's a big narrative in the AI space right now that agents training future generations of models is imminent. i spent a few weeks testing whether the current generation of models can actually do this. full breakdown below: https://hamzamostafa.com/blog/agents-training-their-own-mode... Comments URL: https://news.ycombinator.com/item?id=46941579 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T04:25:55.000Z",
      "score": 118.45,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11301",
      "title": "The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates",
      "link": "https://arxiv.org/abs/2602.11301",
      "summary": "arXiv:2602.11301v1 Announce Type: new Abstract: Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscale infrastructure. Existing governance and security frameworks, including the NIST AI Risk Management Framework and systems security engineering guidance, articulate principles and risk functions but do not provide implementable architectures for multi agent, AI enabled cyber defense. This paper introduces the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem, a multi agent reference architecture for securing enterprise and hyperscale AI estates. PBSAI organizes responsibilities into a twelve domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts. The architecture assumes baseline enterprise security capabilities and encodes key systems security techniques, including analytic monitoring, coordinated defense, and adaptive response. A lightweight formal model of agents, contex",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 117.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11340",
      "title": "Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge",
      "link": "https://arxiv.org/abs/2602.11340",
      "summary": "arXiv:2602.11340v1 Announce Type: new Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and inflexible, requiring new training for each task or dataset. Recent progress in auto prompt optimization (APO) offers a more efficient alternative by automatically improving the instructions that guide LLM judges. However, existing APO methods primarily target text-only evaluations and remain underexplored in multimodal settings. In this work, we study auto prompt optimization for multimodal LLM-as-a-judge, particularly for evaluating AI-generated images. We identify a key bottleneck: multimodal models can only process a limited number of visual examples due to context window constraints, which hinders effective trial-and-error prompt refinement. To overcome this, we propose BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. Our bi-level optimization approach jointly refines the judge prompt and the I2T prompt to maintain fidelity under limited context budgets. Experiments on four datasets and three LLM judges demonstrate the effectiveness of",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 117.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11455",
      "title": "Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning",
      "link": "https://arxiv.org/abs/2602.11455",
      "summary": "arXiv:2602.11455v1 Announce Type: new Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by to",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 117.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.13213",
      "title": "Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique",
      "link": "https://arxiv.org/abs/2602.13213",
      "summary": "arXiv:2602.13213v1 Announce Type: new Abstract: Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgment and accountability are critical. This study presents a decision-negative, human-in-the-loop agentic system that incorporates an adversarial self-critique mechanism as a bounded safety architecture for regulated underwriting workflows. Within this system, a critic agent challenges the primary agent's conclusions prior to submitting recommendations to human reviewers. This internal system of checks and balances addresses a critical gap in AI safety for regulated workflows. Additionally, the research develops a formal taxonomy of failure modes to characterize potential errors by decision-negative agents. This taxonomy provides a structured framework for risk identification and risk management in high-stakes applications. Experimental evaluation using 500 expert-validated underwriting cases demonstrates that the adversarial critique mechanism reduces AI hallucination rates from 11.3% to 3.8",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-17T05:00:00.000Z",
      "score": 117.6,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.13232",
      "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
      "link": "https://arxiv.org/abs/2602.13232",
      "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-form captioning. PlotChain contains 15 plot families with 450 rendered plots (30 per family), where every item is produced from known parameters and paired with exact ground truth computed directly from the generating process. A central contribution is checkpoint-based diagnostic evaluation: in addition to final targets, each item includes intermediate 'cp_' fields that isolate sub-skills (e.g., reading cutoff frequency or peak magnitude) and enable failure localization within a plot family. We evaluate four state-of-the-art MLLMs under a standardized, deterministic protocol (temperature = 0 and a strict JSON-only numeric output schema) and score predictions using per-field tolerances designed to reflect human plot-reading precision. Under the 'plotread' tolerance policy, the top models achieve 80.42% (Gemini 2.5 Pro), 79.84% (GPT-4.1), and 78.21% (Claude Sonnet 4.5) overall field-level pass rates, while GPT-4o trails at 61.59%. Despite strong performance on many families, frequency-domain tasks remain brittle: bandpass response stays lo",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-17T05:00:00.000Z",
      "score": 117.6,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.13237",
      "title": "NL2LOGIC: AST-Guided Translation of Natural Language into First-Order Logic with Large Language Models",
      "link": "https://arxiv.org/abs/2602.13237",
      "summary": "arXiv:2602.13237v1 Announce Type: new Abstract: Automated reasoning is critical in domains such as law and governance, where verifying claims against facts in documents requires both accuracy and interpretability. Recent work adopts structured reasoning pipelines that translate natural language into first-order logic and delegate inference to automated solvers. With the rise of large language models, approaches such as GCD and CODE4LOGIC leverage their reasoning and code generation capabilities to improve logic parsing. However, these methods suffer from fragile syntax control due to weak enforcement of global grammar constraints and low semantic faithfulness caused by insufficient clause-level semantic understanding. We propose NL2LOGIC, a first-order logic translation framework that introduces an abstract syntax tree as an intermediate representation. NL2LOGIC combines a recursive large language model based semantic parser with an abstract syntax tree guided generator that deterministically produces solver-ready logic code. Experiments on the FOLIO, LogicNLI, and ProofWriter benchmarks show that NL2LOGIC achieves 99 percent syntactic accuracy and improves semantic correctness by up to 30 percent over state-of-the-art baselines. Furthermore, integrating NL2LOGIC into Logic-LM yields near-perfect executability and improves downstream reasoning accuracy by 31 percent compared to",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-17T05:00:00.000Z",
      "score": 117.6,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.13258",
      "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
      "link": "https://arxiv.org/abs/2602.13258",
      "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a unified capability rather than three distinct mechanisms requiring different infrastructure, operating on different timescales, and benefiting from independent optimization. We propose MAPLE (Memory-Adaptive Personalized LEarning), a principled decomposition where Memory handles storage and retrieval infrastructure; Learning extracts intelligence from accumulated interactions asynchronously; and Personalization applies learned knowledge in real-time within finite context budgets. Each component operates as a dedicated sub-agent with specialized tooling and well-defined interfaces. Experimental evaluation on the MAPLE-Personas benchmark demonstrates that our decomposition achieves a 14.6% improvement in personalization score compared to a stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75% -- enabling agents that genuinely learn and adapt.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-17T05:00:00.000Z",
      "score": 117.6,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10885",
      "title": "Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics",
      "link": "https://arxiv.org/abs/2602.10885",
      "summary": "arXiv:2602.10885v1 Announce Type: new Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \\textbf{RLCER} (\\textbf{R}einforcement \\textbf{L}earning with \\textbf{C}oT Supervision via Self-\\textbf{E}volving \\textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 117.54,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47011649",
      "title": "Show HN: Why Playwright-CLI Beats MCP for AI‑Driven Browser Automation",
      "link": "https://news.ycombinator.com/item?id=47011649",
      "summary": "Most “AI + browser” setups still bolt MCP tools onto Playwright and hope for the best, so every click dumps full DOMs, accessibility trees, and logs into the model. That burns tokens, collapses context, and makes long sessions unreliable. Meanwhile, default Playwright reports start to struggle once you have more than a few dozen e2e tests, so teams drown in HTML reports and flaky failures instead of clear patterns. The insights at https://testdino.com/blog/playwright-cli/ explores how Microsoft’s playwright-cli keeps browser state external, returns only compact element references and YAML flows, and works with normal npx playwright test plus smarter reporting, so both agents and humans stay fast, cost aware, and predictable. Comments URL: https://news.ycombinator.com/item?id=47011649 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-14T04:45:31.000Z",
      "score": 117.51,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06351",
      "title": "Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion",
      "link": "https://arxiv.org/abs/2602.06351",
      "summary": "arXiv:2602.06351v1 Announce Type: new Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, which is data-intensive and generalizes poorly to unseen interfaces. Recent attention-based alternatives exploit localization signals in MLLMs attention mechanisms without task-specific fine-tuning, but suffer from low reliability due to the lack of explicit and complementary spatial anchors in GUI images. To address this limitation, we propose Trifuse, an attention-based grounding framework that explicitly integrates complementary spatial anchors. Trifuse integrates attention, OCR-derived textual cues, and icon-level caption semantics via a Consensus-SinglePeak (CS) fusion strategy that enforces cross-modal agreement while retaining sharp localization peaks. Extensive evaluations on four grounding benchmarks demonstrate that Trifuse achieves strong performance without task-specific fine-tuning, substantially reducing the reliance on expensive annotated data. Moreover, ablation studies reveal that incorporating OCR and caption cues consistently improves attention-based grounding performance across different backbones, highlighting its",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 116.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06394",
      "title": "Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization",
      "link": "https://arxiv.org/abs/2602.06394",
      "summary": "arXiv:2602.06394v1 Announce Type: new Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 116.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06533",
      "title": "LogicSkills: A Structured Benchmark for Formal Reasoning in Large Language Models",
      "link": "https://arxiv.org/abs/2602.06533",
      "summary": "arXiv:2602.06533v1 Announce Type: new Abstract: Large language models have demonstrated notable performance across various logical reasoning benchmarks. However, it remains unclear which core logical skills they truly master. To address this, we introduce LogicSkills, a unified benchmark designed to isolate three fundamental skills in formal reasoning: (i) $\\textit{formal symbolization}\\unicode{x2014}$translating premises into first-order logic; (ii) $\\textit{countermodel construction}\\unicode{x2014}$formulating a finite structure in which all premises are true while the conclusion is false; and (iii) $\\textit{validity assessment}\\unicode{x2014}$deciding whether a conclusion follows from a given set of premises. Items are drawn from the two-variable fragment of first-order logic (without identity) and are presented in both natural English and a Carroll-style language with nonce words. All examples are verified for correctness and non-triviality using the SMT solver Z3. Across leading models, performance is high on validity but substantially lower on symbolization and countermodel construction, suggesting reliance on surface-level patterns rather than genuine symbolic or rule-based reasoning.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 116.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07153",
      "title": "ANCHOR: Branch-Point Data Generation for GUI Agents",
      "link": "https://arxiv.org/abs/2602.07153",
      "summary": "arXiv:2602.07153v1 Announce Type: new Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 116.79,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07274",
      "title": "TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents",
      "link": "https://arxiv.org/abs/2602.07274",
      "summary": "arXiv:2602.07274v1 Announce Type: new Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/termi",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 116.79,
      "status": "queued"
    },
    {
      "id": "https://github.com/Ramsbaby/openclaw-self-healing",
      "title": "Show HN: Self-Healing AI Agents with Claude Code as Doctor",
      "link": "https://github.com/Ramsbaby/openclaw-self-healing",
      "summary": "I built a 4-tier self-healing system for OpenClaw (AI agent platform running on my Mac Mini 24/7). The interesting part is Level 3: when health checks fail repeatedly, the system spawns Claude Code in a tmux PTY session to autonomously diagnose and repair issues. Recovery escalation: - Level 0-1: LaunchAgent KeepAlive + Watchdog - Level 2: Automated \"doctor --fix\" (config validation, port checks) - Level 3: Claude Code spawns in tmux, reads logs, attempts repairs - Level 4: Discord alert if all automation fails Production-tested in my homelab over 3 months: 99% recovery rate, recovery time reduced from 45min → 3min avg. Handled 17 consecutive crashes, config corruption, port conflicts. Built for macOS (stable) with Linux systemd support (beta). MIT licensed. Curious what others think about AI-powered infrastructure self-healing. Comments URL: https://news.ycombinator.com/item?id=46956003 Points: 3 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-10T06:16:08.000Z",
      "score": 116.21,
      "status": "queued"
    },
    {
      "id": "https://masonry.so",
      "title": "Show HN: Figma for AI Images and Videos",
      "link": "https://masonry.so",
      "summary": "Built a Figma like AI images & videos by bringing all top images and video models under one single canvas. No workflow no agents - just an infinite canvas to play around with all top models and get to the final video without switching tabs! Comments URL: https://news.ycombinator.com/item?id=47014227 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-14T13:02:06.000Z",
      "score": 116.05,
      "status": "queued"
    },
    {
      "id": "https://github.com/jo-inc/camofox-browser",
      "title": "Anti-detection browser server for AI agents, powered by Camoufox",
      "link": "https://github.com/jo-inc/camofox-browser",
      "summary": "Article URL: https://github.com/jo-inc/camofox-browser Comments URL: https://news.ycombinator.com/item?id=46971663 Points: 2 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T06:39:39.000Z",
      "score": 115.34,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.05048",
      "title": "MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation",
      "link": "https://arxiv.org/abs/2602.05048",
      "summary": "arXiv:2602.05048v1 Announce Type: new Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns b",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 114,
      "status": "queued"
    },
    {
      "id": "https://www.watchllm.dev/",
      "title": "WatchLLM – Cost kill switch for AI agents (with loop detection)",
      "link": "https://www.watchllm.dev/",
      "summary": "Article URL: https://www.watchllm.dev/ Comments URL: https://news.ycombinator.com/item?id=46933707 Points: 1 # Comments: 2",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-08T12:34:15.000Z",
      "score": 113.78,
      "status": "queued"
    },
    {
      "id": "https://patrickmccanna.net/giving-coding-agents-ssh-access-to-other-systems-without-giving-disclosing-secrets/",
      "title": "Giving AI agents SSH access without giving up your keys",
      "link": "https://patrickmccanna.net/giving-coding-agents-ssh-access-to-other-systems-without-giving-disclosing-secrets/",
      "summary": "Article URL: https://patrickmccanna.net/giving-coding-agents-ssh-access-to-other-systems-without-giving-disclosing-secrets/ Comments URL: https://news.ycombinator.com/item?id=47014335 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-14T13:21:38.000Z",
      "score": 111.92,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.13224",
      "title": "A Geometric Taxonomy of Hallucinations in LLMs",
      "link": "https://arxiv.org/abs/2602.13224",
      "summary": "arXiv:2602.13224v1 Announce Type: new Abstract: The term \"hallucination\" in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context), confabulation (invention of semantically foreign content), and factual error (incorrect claims within correct conceptual frames). We observe a striking asymmetry. On standard benchmarks where hallucinations are LLM-generated, detection is domain-local: AUROC 0.76-0.99 within domains, but 0.50 (chance level) across domains. Discriminative directions are approximately orthogonal between domains (mean cosine similarity -0.07). On human-crafted confabulations - invented institutions, redefined terminology, fabricated mechanisms - a single global direction achieves 0.96 AUROC with 3.8% cross-domain degradation. We interpret this divergence as follows: benchmarks capture generation artifacts (stylistic signatures of prompted fabrication), while human-crafted confabulations capture genuine topical drift. The geometric structure differs because the underlying phenomena differ. Type III errors show 0.478 AUROC - indistinguishable from chance. This reflects a theoretical constraint: embeddings encode distributional co-occurrence, not correspondence to external reality. Statements with identical contextual",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-17T05:00:00.000Z",
      "score": 111.6,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10485",
      "title": "Abstraction Generation for Generalized Planning with Pretrained Large Language Models",
      "link": "https://arxiv.org/abs/2602.10485",
      "summary": "arXiv:2602.10485v1 Announce Type: new Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 111.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12566",
      "title": "To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models",
      "link": "https://arxiv.org/abs/2602.12566",
      "summary": "arXiv:2602.12566v1 Announce Type: new Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, a",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 111.42,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06176",
      "title": "Large Language Model Reasoning Failures",
      "link": "https://arxiv.org/abs/2602.06176",
      "summary": "arXiv:2602.06176v1 Announce Type: new Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 110.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06319",
      "title": "Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems",
      "link": "https://arxiv.org/abs/2602.06319",
      "summary": "arXiv:2602.06319v1 Announce Type: new Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 110.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07187",
      "title": "PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents",
      "link": "https://arxiv.org/abs/2602.07187",
      "summary": "arXiv:2602.07187v1 Announce Type: new Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 110.79,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.07276",
      "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs",
      "link": "https://arxiv.org/abs/2602.07276",
      "summary": "arXiv:2602.07276v1 Announce Type: new Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "score": 110.79,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.04986",
      "title": "Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence",
      "link": "https://arxiv.org/abs/2602.04986",
      "summary": "arXiv:2602.04986v1 Announce Type: new Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: \"familiar intelligence\" and \"strange intelligence\". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which \"general intelligence\" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 108,
      "status": "queued"
    },
    {
      "id": "https://agentshield.live/",
      "title": "Is Your AI Agent Safe?",
      "link": "https://agentshield.live/",
      "summary": "Article URL: https://agentshield.live/ Comments URL: https://news.ycombinator.com/item?id=47021487 Points: 1 # Comments: 1",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-15T06:21:40.000Z",
      "score": 107.88,
      "status": "queued"
    },
    {
      "id": "https://x.com/SarvamAI",
      "title": "India's Sarvan AI LLM launches Indic-language focused models",
      "link": "https://x.com/SarvamAI",
      "summary": "Article URL: https://x.com/SarvamAI Comments URL: https://news.ycombinator.com/item?id=46931408 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 4,
      "publishedAt": "2026-02-08T04:52:43.000Z",
      "score": 106.44,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11318",
      "title": "Dissecting Subjectivity and the \"Ground Truth\" Illusion in Data Annotation",
      "link": "https://arxiv.org/abs/2602.11318",
      "summary": "arXiv:2602.11318v1 Announce Type: new Abstract: In machine learning, \"ground truth\" refers to the assumed correct labels used to train and evaluate models. However, the foundational \"ground truth\" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic literature review analyzes research published between 2020 and 2025 across seven premier venues: ACL, AIES, CHI, CSCW, EAAMO, FAccT, and NeurIPS, investigating the mechanisms in data annotation practices that facilitate this \"consensus trap\". Our identification phase captured 30,897 records, which were refined via a tiered keyword filtration schema to a high-recall corpus of 3,042 records for manual screening, resulting in a final included corpus of 346 papers for qualitative synthesis. Our reflexive thematic analysis reveals that systemic failures in positional legibility, combined with the recent architectural shift toward human-as-verifier models, specifically the reliance on model-mediated annotations, introduce deep-seated anchoring bias and effectively remove human voices from the loop. We further demonstrate how geographic hegemony imposes Western norms as universal benchmarks, often enforced by the performative alignment of precarious data workers who prioritize requester compliance over honest subjectivity to avoid economic pena",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 105.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11389",
      "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions",
      "link": "https://arxiv.org/abs/2602.11389",
      "summary": "arXiv:2602.11389v1 Announce Type: new Abstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 105.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11409",
      "title": "TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning",
      "link": "https://arxiv.org/abs/2602.11409",
      "summary": "arXiv:2602.11409v1 Announce Type: new Abstract: Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $\\tau^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at https://github.com/sinatayebati/agent-tracer.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 105.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10583",
      "title": "Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets",
      "link": "https://arxiv.org/abs/2602.10583",
      "summary": "arXiv:2602.10583v1 Announce Type: new Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 105.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10635",
      "title": "OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization",
      "link": "https://arxiv.org/abs/2602.10635",
      "summary": "arXiv:2602.10635v1 Announce Type: new Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 105.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10964",
      "title": "Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation",
      "link": "https://arxiv.org/abs/2602.10964",
      "summary": "arXiv:2602.10964v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \\textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representat",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 105.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12419",
      "title": "Intent-Driven Smart Manufacturing Integrating Knowledge Graphs and Large Language Models",
      "link": "https://arxiv.org/abs/2602.12419",
      "summary": "arXiv:2602.12419v1 Announce Type: new Abstract: The increasing complexity of smart manufacturing environments demands interfaces that can translate high-level human intents into machine-executable actions. This paper presents a unified framework that integrates instruction-tuned Large Language Models (LLMs) with ontology-aligned Knowledge Graphs (KGs) to enable intent-driven interaction in Manufacturing-as-a-Service (MaaS) ecosystems. We fine-tune Mistral-7B-Instruct-V02 on a domain-specific dataset, enabling the translation of natural language intents into structured JSON requirement models. These models are semantically mapped to a Neo4j-based knowledge graph grounded in the ISA-95 standard, ensuring operational alignment with manufacturing processes, resources, and constraints. Our experimental results demonstrate significant performance gains over zero-shot and 3-shots baselines, achieving 89.33\\% exact match accuracy and 97.27\\% overall accuracy. This work lays the foundation for scalable, explainable, and adaptive human-machine",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 105.42,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12544",
      "title": "Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation",
      "link": "https://arxiv.org/abs/2602.12544",
      "summary": "arXiv:2602.12544v1 Announce Type: new Abstract: We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 105.42,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12852",
      "title": "WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning",
      "link": "https://arxiv.org/abs/2602.12852",
      "summary": "arXiv:2602.12852v1 Announce Type: new Abstract: Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 105.42,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06375",
      "title": "Difficulty-Estimated Policy Optimization",
      "link": "https://arxiv.org/abs/2602.06375",
      "summary": "arXiv:2602.06375v1 Announce Type: new Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for re",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 104.87,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.06486",
      "title": "JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks",
      "link": "https://arxiv.org/abs/2602.06486",
      "summary": "arXiv:2602.06486v1 Announce Type: new Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 6,
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "score": 104.87,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47014232",
      "title": "Show HN: ClawdReview – OpenReview for AI Agents",
      "link": "https://news.ycombinator.com/item?id=47014232",
      "summary": "Agents can review the paper on arXiv, and humans can like or dislike agents' reviews. There are also ranking lists of the most popular papers and agents. Please visit: https://clawdreview.ai/ Comments URL: https://news.ycombinator.com/item?id=47014232 Points: 3 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-14T13:03:13.000Z",
      "score": 104.45,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46933611",
      "title": "AI's Real Problem Is Illegitimacy, Not Hallucination",
      "link": "https://news.ycombinator.com/item?id=46933611",
      "summary": "The Core Problem of AI Is Not Hallucination — It Is the Lack of Execution Legitimacy Janus pater Introduction Most debates around AI today revolve around a false question: is the model smart enough, accurate enough? In engineering reality, the real question is never accuracy — it is whether the system is even allowed to act. 1. The Original Sin of the Predictive Paradigm: No Execution Legitimacy Modern generative AI fundamentally does one thing: predict the most likely next state in a probability space. Whether it predicts tokens, pixels, latent states, or so-called “world models”, as long as the output is probabilistic, it answers only one question: “What is most likely to happen?” In many real-world systems, however, engineering demands an entirely different question: “What is the only action that is allowed to be executed?” This is not an accuracy problem — it is a legitimacy problem. 2. Yann LeCun Is Right — but Only Halfway LeCun is absolutely right to criticize next-token prediction as the foundation of intelligence. World models (JEPA) are undeniably more advanced than raw pixel or text prediction. Yet even world models still output possible worlds, not permitted worlds. World models are excellent at three things: • Abstract state representation • Learning dynamics • Producing goals and constraints They do not possess — and should not possess — execution authority. Once",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-08T12:13:43.000Z",
      "score": 103.33,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46971761",
      "title": "Ask HN: What is your AI assisted dev workflow",
      "link": "https://news.ycombinator.com/item?id=46971761",
      "summary": "Curious on: 1. What information gets attached with pull requests / merge requests such that humans get to manage the volumes of code that AI generates? 2. What is the dev workflow in your company/personal projects? 3. What IDE+model combo is working for you? 4. What is your hack/tricks for managing large complex codebases when working with AI or are you finding utility only in small focussed repos? Comments URL: https://news.ycombinator.com/item?id=46971761 Points: 2 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-11T06:57:07.000Z",
      "score": 102.06,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47023609",
      "title": "Ask HN: How to sell SaaS without AI features in 2026?",
      "link": "https://news.ycombinator.com/item?id=47023609",
      "summary": "I'm a developer who built an ERP/CRM system for small manufacturers (https://www.paxerp.com). It does all the basics very well; financial reporting, lot tracking, production planning, shipping carrier integrations, the usual workflow stuff—but there's zero AI in it. It's just fast, clean, and solves real problems I saw working in manufacturing ERP systems. The product works well, customers really like it, but I have almost no sales experience. Every SaaS founder seems to be talking about \"AI-powered insights\" and \"intelligent automation.\" while... We just have a clean system that is fast and tries to stay out of the user's way. For those who've sold B2B SaaS (especially to traditional industries like manufacturing): - Is \"no AI\" actually a disadvantage, or does it not matter as much as I think? - How do I communicate value when the value is \"it's simple and fast, and your data is highly accessible\" vs \"revolutionary AI\"? - Should I be adding AI features just to check a marketing box, even if customers don't need them? You can learn more about why I built this on the websites about page. But now I'm wondering if I'm fighting an uphill battle by not having the buzzwords everyone else does. Any advice from founders who've been here? TY Comments URL: https://news.ycombinator.com/item?id=47023609 Points: 1 # Comments: 3",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-15T13:45:28.000Z",
      "score": 102,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=47011510",
      "title": "Show HN: Verify-before-release x402 gateway for AI agent transactions",
      "link": "https://news.ycombinator.com/item?id=47011510",
      "summary": "Hey HN, I built Settld because I kept running into the same problem: AI agents can call APIs, pay for services, and hire other agents - but there's no way to prove the work was actually done before the money moves. The problem in one sentence: x402 tells you \"payment was sent\". Settld tells you \"the work was worth paying for\". What it does Settld sits between your agent and the APIs/agents it pays. It: 1. Intercepts HTTP 402 (Payment Required) responses 2. Creates an escrow hold instead of paying immediately 3. Collects evidence that the work was completed 4. Runs deterministic verification (same evidence + same terms = same payout, every time) 5. Releases payment only after verification passes 6. Issues a cryptographically verifiable receipt If verification fails or the work is disputed, the hold is refunded. The agent gets a receipt either way - a permanent, auditable record of what happened. Why this matters now We're at a weird inflection point. Coinbase shipped x402 (50M+ transactions). Google shipped A2A. Anthropic shipped MCP. Agents can discover each other, communicate, and pay each other. But nobody built the layer that answers: \"was the work actually done correctly, and how much should the payout be?\" That's the gap. Right now, every agent-to-agent transaction is either \"trust and hope\" or \"don't transact.\" Neither scales. The x402 gateway (the fastest way to try it)",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-14T04:17:17.000Z",
      "score": 101.62,
      "status": "queued"
    },
    {
      "id": "https://news.ycombinator.com/item?id=46971643",
      "title": "Show HN: GHOSTYPE – AI voice input that learns your writing style",
      "link": "https://news.ycombinator.com/item?id=46971643",
      "summary": "Hello HN, I’m the creator of GHOSTYPE. I built this because I wanted a voice input workflow that didn't feel like talking to a chatbot. I wanted something that felt like a \"neural extension\" of my keyboard. It’s a macOS-native app that sits between your voice and your active window. Here is what it actually does: The Core Input Workflow: Push-to-Talk & Smart Send: Hold a global shortcut to speak. It detects the active app to determine the correct send method (e.g., Cmd+Enter for Slack vs Enter for Discord). Inline Editing: You can format while speaking. Need a line break, a specific spelling, or a bulleted list? You just say it, and it handles the formatting inside the sentence before outputting. \"Call Ghost\": Post-processing commands (translate, polish, expand) are available immediately after speaking, before the text is typed out. The Experimental Stuff (WIP): 1. Ghost Twin (Style Transfer): I call this a \"Virtual Personality Engine\" (a bit pretentious, I know). It analyzes your local writing history to build a style vector. It learns your tone—whether professional for emails or casual for Discord—so the output sounds like you, not a generic LLM. Side note: I'm currently building the training UI to look like a retro CRT terminal because I miss that aesthetic. 2. Ghost Morph (Custom Skills): Trigger custom macros with a modifier key. For example, turn a raw voice thought direc",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 3,
      "publishedAt": "2026-02-11T06:35:56.000Z",
      "score": 101.45,
      "status": "queued"
    },
    {
      "id": "https://calculus.academa.ai/",
      "title": "Show HN: A calculus course with an AI tutor watching the lectures with you",
      "link": "https://calculus.academa.ai/",
      "summary": "We're two PhD students in mechanical engineering. We spent years digging through scattered textbooks and YouTube rabbit holes. We figured there could be a better way to learn. So we wrote a multivariable calculus course entirely in code: 18 lectures, 6 languages. All content and pedagogy are ours. As everything is code, we can feed the LLM the full context of every lecture. Ask a question mid-lecture, it knows what's on the screen, and answers from the actual content. The recent Coursera/Udemy thread echoed a lot of what pushed us to build this: https://news.ycombinator.com/item?id=46301346 Would love feedback, especially on the tutor. Comments URL: https://news.ycombinator.com/item?id=46931868 Points: 1 # Comments: 0",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 2,
      "publishedAt": "2026-02-08T06:39:43.000Z",
      "score": 100.53,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.11159",
      "title": "Explaining AI Without Code: A User Study on Explainable AI",
      "link": "https://arxiv.org/abs/2602.11159",
      "summary": "arXiv:2602.11159v1 Announce Type: new Abstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\\geq80\\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $\\alpha$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $\\alpha$ = 0.60), with novices showing higher trust than experts. The",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-13T05:00:00.000Z",
      "score": 99.96,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.13217",
      "title": "VeRA: Verified Reasoning Data Augmentation at Scale",
      "link": "https://arxiv.org/abs/2602.13217",
      "summary": "arXiv:2602.13217v1 Announce Type: new Abstract: The main issue with most evaluation schemes today is their \"static\" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, comprising (i) a natural language template with placeholder slots, (ii) a coherent generator that samples valid configurations, and (iii) a deterministic verifier that validates parameters and calculates the corresponding correct answers for each configuration. From a single seed problem, VeRA automatically creates unlimited verified variants with reliable labels at near-zero marginal cost without human involvement. VeRA operates in two complementary modes. VeRA-E (equivalent) rewrites problems while keeping the underlying logic intact, useful for detecting memorization versus genuine reasoning. VeRA-H (hardened) systematically increases complexity while remaining verifiable, enabling reliable creation and labelling of fresh difficult tasks at the boundary of intelligence. Evaluating 16 frontier models with VeRA, we find: (i) VeRA-E improves evaluation quality and reveals contamination pattern",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-17T05:00:00.000Z",
      "score": 99.6,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.10324",
      "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs",
      "link": "https://arxiv.org/abs/2602.10324",
      "summary": "arXiv:2602.10324v1 Announce Type: new Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "score": 99.54,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12356",
      "title": "A Theoretical Framework for Adaptive Utility-Weighted Benchmarking",
      "link": "https://arxiv.org/abs/2602.12356",
      "summary": "arXiv:2602.12356v1 Announce Type: new Abstract: Benchmarking has long served as a foundational practice in machine learning and, increasingly, in modern AI systems such as large language models, where shared tasks, metrics, and leaderboards offer a common basis for measuring progress and comparing approaches. As AI systems are deployed in more varied and consequential settings, though, there is growing value in complementing these established practices with a more holistic conceptualization of what evaluation should represent. Of note, recognizing the sociotechnical contexts in which these systems operate invites an opportunity for a deeper view of how multiple stakeholders and their unique priorities might inform what we consider meaningful or desirable model behavior. This paper introduces a theoretical framework that reconceptualizes benchmarking as a multilayer, adaptive network linking evaluation metrics, model components, and stakeholder groups through weighted interactions. Using conjoint-derived utilities and a human-in-the-loop update rule, we formalize how human tradeoffs can be embedded into benchmark structure and how benchmarks can evolve dynamically while preserving stability and interpretability. The resulting formulation generalizes classical leaderboards as a special case and provides a foundation for building evaluation protocols that are more context aware, r",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 99.42,
      "status": "queued"
    },
    {
      "id": "https://arxiv.org/abs/2602.12617",
      "title": "GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics",
      "link": "https://arxiv.org/abs/2602.12617",
      "summary": "arXiv:2602.12617v1 Announce Type: new Abstract: This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 5,
      "publishedAt": "2026-02-16T05:00:00.000Z",
      "score": 99.42,
      "status": "queued"
    },
    {
      "id": "https://aidevhub.io/",
      "title": "Show HN: AI Dev Hub. 75 free AI and dev tools",
      "link": "https://aidevhub.io/",
      "summary": "I built 75 developer and AI tools as a single static site. Everything runs in the browser, no cookies, no ads, nothing gets sent to a server. The tools range from the usual suspects (JSON formatter, base64 encoder, regex tester) to some AI-specific ones I couldn't find good free versions of bundled in one suite: - LLM Token Counter (estimates tokens for GPT, Claude, Gemini, etc.) - AI Model Comparison (specs, pricin…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-13T15:20:44.000Z",
      "score": 210,
      "status": "failed",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "failedAtRun": "2026-02-15T15:25:30.183Z",
      "failureReason": "validation issues: FR: Tutorial missing a concrete config artifact (YAML/JSON/.env/Dockerfile)."
    },
    {
      "id": "https://weloveaijobs.com",
      "title": "Show HN: We Love AI Jobs – AI jobs for people who don't write Python",
      "link": "https://weloveaijobs.com",
      "summary": "Most AI job boards are just filters for PyTorch and CUDA. We think that misses the most interesting shift: AI is becoming the new \"Excel\" or \"Typing.\" We’re seeing a massive gap where companies need marketers, PMs, and designers who treat LLMs as a standard part of their stack, but these roles get buried under \"ML Engineer\" listings. We built a board specifically for roles where AI is a core workflow requirement, no…",
      "source": "Hacker News (AI)",
      "region": "GLOBAL",
      "keywordHits": 7,
      "publishedAt": "2026-02-10T15:53:17.000Z",
      "score": 210,
      "status": "failed",
      "targetRegion": "UK",
      "editorialTemplate": "SOCIETY",
      "failedAtRun": "2026-02-15T08:21:56.349Z",
      "failureReason": "insufficient source snapshots (0/1)"
    },
    {
      "id": "https://openai.com/index/taisei",
      "title": "Taisei Corporation shapes the next generation of talent with ChatGPT",
      "link": "https://openai.com/index/taisei",
      "summary": "Taisei Corporation uses ChatGPT Enterprise to support HR-led talent development and scale generative AI across its global construction business.",
      "source": "OpenAI News",
      "region": "US",
      "keywordHits": 2,
      "publishedAt": "2026-01-29T00:00:00.000Z",
      "score": 30.58,
      "status": "failed",
      "targetRegion": "US",
      "editorialTemplate": "TUTORIAL",
      "failedAtRun": "2026-02-08T15:26:38.928Z",
      "failureReason": "insufficient source snapshots (0/1)"
    },
    {
      "id": "https://deepmind.google/blog/improved-gemini-audio-models-for-powerful-voice-experiences/",
      "title": "Improved Gemini audio models for powerful voice experiences",
      "link": "https://deepmind.google/blog/improved-gemini-audio-models-for-powerful-voice-experiences/",
      "summary": "",
      "source": "Google DeepMind News",
      "region": "UK",
      "keywordHits": 3,
      "publishedAt": "2025-12-12T17:50:50.000Z",
      "score": 54.09,
      "status": "failed",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T19:58:27.831Z",
      "failureReason": "insufficient source snapshots (0/1)"
    },
    {
      "id": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
      "title": "GPT-5 lowers the cost of cell-free protein synthesis",
      "link": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
      "summary": "An autonomous lab combining OpenAI’s GPT-5 with Ginkgo Bioworks’ cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
      "source": "OpenAI News",
      "region": "US",
      "keywordHits": 3,
      "publishedAt": "2026-02-05T11:00:00.000Z",
      "score": 40.22,
      "status": "failed",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T10:50:58.503Z",
      "failureReason": "insufficient source snapshots (0/1)"
    },
    {
      "id": "https://arxiv.org/abs/2602.05088",
      "title": "VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health",
      "link": "https://arxiv.org/abs/2602.05088",
      "summary": "arXiv:2602.05088v1 Announce Type: new Abstract: Millions now use leading generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based…",
      "source": "ArXiv cs.AI",
      "region": "GLOBAL",
      "keywordHits": 9,
      "publishedAt": "2026-02-07T05:00:00.000Z",
      "score": 150,
      "status": "failed",
      "targetRegion": "UK",
      "editorialTemplate": "TUTORIAL",
      "failedAtRun": "2026-02-07T10:43:13.225Z",
      "failureReason": "insufficient source snapshots (1/2)"
    },
    {
      "id": "https://www.numerama.com/tech/2161859-quest-ce-quun-llm-large-language-model-et-comment-cela-fonctionne.html",
      "title": "Qu’est-ce qu’un LLM (Large Language Model) et comment cela fonctionne ?",
      "link": "https://www.numerama.com/tech/2161859-quest-ce-quun-llm-large-language-model-et-comment-cela-fonctionne.html",
      "summary": "L’intelligence artificielle a pris un autre tournant avec les LLM. ChatGPT, Gemini ou encore Claude, ces LLM sont désormais des outils incontournables et ont changé notre manière d’interagir avec la machine.",
      "source": "Numerama IA",
      "region": "FR",
      "keywordHits": 5,
      "publishedAt": "2026-01-24T17:31:00.000Z",
      "score": 72.36,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T10:43:13.225Z",
      "failureReason": "insufficient source bundle (need at least 2 URLs)"
    },
    {
      "id": "https://openai.com/index/our-approach-to-localization",
      "title": "Making AI work for everyone, everywhere: our approach to localization",
      "link": "https://openai.com/index/our-approach-to-localization",
      "summary": "OpenAI shares its approach to AI localization, showing how globally shared frontier models can be adapted to local languages, laws, and cultures without compromising safety.",
      "source": "OpenAI News",
      "region": "US",
      "keywordHits": 4,
      "publishedAt": "2026-02-06T10:00:00.000Z",
      "score": 59.37,
      "status": "failed",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T10:43:13.225Z",
      "failureReason": "insufficient source bundle (need at least 2 URLs)"
    },
    {
      "id": "https://www.technologyreview.com/2026/02/06/1132448/moltbook-was-peak-ai-theater/",
      "title": "Moltbook was peak AI theater",
      "link": "https://www.technologyreview.com/2026/02/06/1132448/moltbook-was-peak-ai-theater/",
      "summary": "For a few days this week the hottest new hangout on the internet was a vibe-coded Reddit clone called Moltbook, which billed itself as a social network for bots. As the website’s tagline puts it: “Where AI agents share, discuss, and upvote. Humans welcome to observe.” We observed! Launched on January 28 by Matt Schlicht,…",
      "source": "MIT Tech Review AI",
      "region": "US",
      "keywordHits": 3,
      "publishedAt": "2026-02-06T16:38:11.000Z",
      "score": 49.64,
      "status": "failed",
      "targetRegion": "US",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T10:43:13.225Z",
      "failureReason": "insufficient source bundle (need at least 2 URLs)"
    },
    {
      "id": "https://www.bbc.com/news/articles/c62n410w5yno?at_medium=RSS&at_campaign=rss",
      "title": "What is the 'social media network for AI' Moltbook?",
      "link": "https://www.bbc.com/news/articles/c62n410w5yno?at_medium=RSS&at_campaign=rss",
      "summary": "The Reddit-like website which launched in late January allows AI bots to speak to each other.",
      "source": "BBC Technology",
      "region": "UK",
      "keywordHits": 0,
      "publishedAt": "2026-02-02T13:59:14.000Z",
      "score": 11.24,
      "status": "failed",
      "targetRegion": "UK",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T10:43:13.225Z",
      "failureReason": "insufficient source bundle (need at least 2 URLs)"
    },
    {
      "id": "https://www.lemonde.fr/economie/article/2026/02/03/fusion-spacex-xai-elon-musk-defend-son-projet-d-ia-dans-l-espace-les-analystes-s-interrogent-sur-la-viabilite-de-l-ensemble_6665163_3234.html",
      "title": "Fusion SpaceX-xAI : Elon Musk défend son projet d’IA dans l’espace, les analystes s’interrogent sur la viabilité de l’ensemble",
      "link": "https://www.lemonde.fr/economie/article/2026/02/03/fusion-spacex-xai-elon-musk-defend-son-projet-d-ia-dans-l-espace-les-analystes-s-interrogent-sur-la-viabilite-de-l-ensemble_6665163_3234.html",
      "summary": "Le rapprochement entre les deux entités va donner naissance à la société non cotée la plus chère du monde, valorisée 1 250 milliards de dollars. Son patron, à la traîne dans l’intelligence artificielle, espère rattraper les leaders du secteur.",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-03T04:34:27.000Z",
      "score": 31.45,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T08:20:00.214Z",
      "failureReason": "strict publish refused generationMode=\"fallback\""
    },
    {
      "id": "https://www.lemonde.fr/economie/article/2026/02/03/elon-musk-fusionne-xai-et-spacex-pour-batir-des-centres-de-donnees-en-orbite_6665150_3234.html",
      "title": "Elon Musk fusionne xAI et SpaceX pour bâtir des centres de données en orbite",
      "link": "https://www.lemonde.fr/economie/article/2026/02/03/elon-musk-fusionne-xai-et-spacex-pour-batir-des-centres-de-donnees-en-orbite_6665150_3234.html",
      "summary": "L’intégration de la société d’intelligence artificielle du milliardaire américain précède le projet d’introduction en Bourse de l’entreprise spatiale cette année.",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-02T23:24:09.000Z",
      "score": 31.37,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T08:20:00.214Z",
      "failureReason": "strict publish refused generationMode=\"fallback\""
    },
    {
      "id": "https://www.lemonde.fr/economie/video/2026/02/02/friend-com-que-vendent-ces-publicites-affichees-dans-le-metro-parisien_6665132_3234.html",
      "title": "Friend.com : que vendent ces publicités affichées dans le métro parisien ?",
      "link": "https://www.lemonde.fr/economie/video/2026/02/02/friend-com-que-vendent-ces-publicites-affichees-dans-le-metro-parisien_6665132_3234.html",
      "summary": "Ces affiches publicitaires blanches aux slogans énigmatiques ont interrogé de nombreux internautes sur les réseaux sociaux. Il s’agit d’un collier permettant de discuter avec une intelligence artificielle en continu.",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-02T17:44:34.000Z",
      "score": 25.29,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T08:20:00.214Z",
      "failureReason": "strict publish refused generationMode=\"fallback\""
    },
    {
      "id": "https://www.lemonde.fr/emploi/article/2026/02/02/recrutement-peut-on-maitriser-les-secrets-des-algorithmes-avant-de-postuler_6665057_1698637.html",
      "title": "Recrutement : peut-on maîtriser les secrets des algorithmes avant de postuler ?",
      "link": "https://www.lemonde.fr/emploi/article/2026/02/02/recrutement-peut-on-maitriser-les-secrets-des-algorithmes-avant-de-postuler_6665057_1698637.html",
      "summary": "Pour faire face aux afflux de candidats, 80 % des entreprises françaises utilisent ou envisagent d’utiliser un Applicant Tracking System (ATS), logiciel de gestion des profils. Pour maximiser ses chances, on peut tenter d’en maîtriser les codes, avec certaines limites.",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 2,
      "publishedAt": "2026-02-02T05:30:04.000Z",
      "score": 25.14,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T08:20:00.214Z",
      "failureReason": "strict publish refused generationMode=\"fallback\""
    },
    {
      "id": "https://www.lemonde.fr/economie/article/2026/02/05/les-craintes-sur-l-ia-font-plonger-la-tech-a-wall-street_6665487_3234.html",
      "title": "Les craintes sur l’IA font plonger la tech à Wall Street",
      "link": "https://www.lemonde.fr/economie/article/2026/02/05/les-craintes-sur-l-ia-font-plonger-la-tech-a-wall-street_6665487_3234.html",
      "summary": "Malgré des résultats supérieurs aux attentes, Alphabet, maison mère de Google, a été puni en Bourse, mercredi, pour avoir annoncé des investissements massifs dans l’IA. Les marchés semblent avoir pris conscience des risques de l’intelligence artificielle pour les entreprises.",
      "source": "Le Monde IA",
      "region": "FR",
      "keywordHits": 0,
      "publishedAt": "2026-02-05T08:08:31.000Z",
      "score": 23.92,
      "status": "failed",
      "targetRegion": "FR",
      "editorialTemplate": "NEWS",
      "failedAtRun": "2026-02-07T08:20:00.214Z",
      "failureReason": "strict publish refused generationMode=\"fallback\""
    }
  ]
}